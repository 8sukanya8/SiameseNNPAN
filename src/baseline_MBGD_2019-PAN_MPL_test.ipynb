{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "from collections import Counter\n",
    "import csv\n",
    "import config\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "import subprocess\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from collections import OrderedDict, defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import fnmatch\n",
    "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from src.CustomTokenizer import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from data_processing.preprocessing import get_dataset_text, get_word_index_list, write_index_to_file, read_index_as_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvocab_base_folder_list = [\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000\\']\\n\\ntraining_files = [\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_0.csv\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_200.csv\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_400.csv\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_600.csv\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_800.csv\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1000.csv\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1200.csv\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1400.csv\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1600.csv\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1800.csv\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_2000.csv\\']\\n\\nresults =  [\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_0\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_200\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_400\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_600\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_800\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_1000\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_1200\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_1400\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_1600\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_1800\\',\\n\\'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_2000\\']\\nfor training_file_path,vocab_base_folder in zip(training_files, vocab_base_folder_list):\\n    print(training_file_path)\\n    vocab_path_list = [vocab_base_folder+\"/\"+opt for opt in [\\'opt1\\', \\'opt2\\', \\'opt3\\', \\'opt4\\']]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "vocab_base_folder_list = ['/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000']\n",
    "\n",
    "training_files = ['/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_0.csv',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_200.csv',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_400.csv',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_600.csv',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_800.csv',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1000.csv',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1200.csv',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1400.csv',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1600.csv',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1800.csv',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_2000.csv']\n",
    "\n",
    "results =  ['/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_0',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_200',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_400',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_600',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_800',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_1000',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_1200',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_1400',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_1600',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_1800',\n",
    "'/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_2000']\n",
    "for training_file_path,vocab_base_folder in zip(training_files, vocab_base_folder_list):\n",
    "    print(training_file_path)\n",
    "    vocab_path_list = [vocab_base_folder+\"/\"+opt for opt in ['opt1', 'opt2', 'opt3', 'opt4']]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MPL_list = [str(i) for i in range(0,2200,200)]\n",
    "vocab_base = '/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/'\n",
    "training_base = '/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/'\n",
    "results_base = '/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/'\n",
    "\n",
    "vocab_base_folder_list = [vocab_base + \"MPL_\" + i for i in MPL_list]\n",
    "training_files = [training_base+ \"training_\" + i + \".csv\" for i in MPL_list]\n",
    "results_folder_list = [results_base + \"MPL_\" + i for i in MPL_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_0.csv\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_200.csv\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_400.csv\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_600.csv\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_800.csv\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1000.csv\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1200.csv\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1400.csv\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1600.csv\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1800.csv\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_2000.csv\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_0\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_200\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_400\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_600\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_800\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_1000\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_1200\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_1400\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_1600\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_1800\n",
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_2000\n"
     ]
    }
   ],
   "source": [
    "for i in vocab_base_folder_list:\n",
    "    print(i)\n",
    "for i in training_files:\n",
    "    print(i)\n",
    "for i in results_folder_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(training_file_path, testing_file_path, vocab_folder, results_folder):\n",
    "    print(\"Loading training file from path: \", training_file_path)\n",
    "    training_data = pd.read_csv(training_file_path)\n",
    "    print(training_data.shape)\n",
    "    print(\"Loading testing file from path: \", config.config_io.get('pan_19_processed_test'))\n",
    "    testing_data = pd.read_csv(testing_file_path)\n",
    "    print(testing_data.shape)\n",
    "    feature_set_files = get_feature_set_files(vocab_folder, vocab_size = range(50, 550,50))\n",
    "    metrics = {}\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    for key in list(feature_set_files.keys()):\n",
    "        print(key)\n",
    "        temp_metrics_dict={}\n",
    "        vocab_files = []\n",
    "        training_acc = []\n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "        validation_acc = []\n",
    "        testing_acc= []\n",
    "        testing_loss = []\n",
    "        vocab_size_list = []\n",
    "        #print(key, feature_set_files.get(key))\n",
    "        for feature_file in feature_set_files.get(key):\n",
    "            print(\"Evaluating:\", feature_file)\n",
    "            word_tokenizer = Tokenizer(analyzer=custom_analyzer)\n",
    "            with open(feature_file, \"r\") as f:\n",
    "                word_index_str = f.read().replace('\\n', '')\n",
    "            print(\"WORD INDEX STR:\",word_index_str)\n",
    "            word_tokenizer.fit_on_texts([word_index_str])\n",
    "            len_train = len(training_data)\n",
    "            # define the generators\n",
    "            from src.DataGenerator import DataGenerator\n",
    "            training_generator = DataGenerator(training_data.iloc[0:int(0.8*len_train)], tokenizer=word_tokenizer, batch_size=16)\n",
    "            validation_generator = DataGenerator(training_data.iloc[int(0.8*len_train):], tokenizer=word_tokenizer, batch_size=16)\n",
    "            testing_generator = DataGenerator(testing_data, tokenizer=word_tokenizer, batch_size=16)\n",
    "\n",
    "            # this is a hack for \"'DataGenerator' object has no attribute 'index'\". It turns out that on_epoch_end creates the index that is used\n",
    "            training_generator.on_epoch_end()\n",
    "            validation_generator.on_epoch_end()\n",
    "            testing_generator.on_epoch_end()\n",
    "            # parameters\n",
    "            num_classes =2\n",
    "            num_features = 1\n",
    "\n",
    "            # define model\n",
    "            model = Sequential()\n",
    "            model.add(Dense(50, input_dim=num_features, activation='relu', kernel_initializer='he_uniform'))\n",
    "            model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "            # compile model\n",
    "            opt = SGD(lr=0.01, momentum=0.9)\n",
    "            #model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "            model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "            # fit model\n",
    "            history = model.fit(training_generator, validation_data=validation_generator, verbose=1, batch_size=16, \n",
    "                                 epochs=30, steps_per_epoch=100, callbacks=[callback]) #validation_steps=100,\n",
    "\n",
    "            history_dict = history.history\n",
    "            vocab_files.append(feature_file.split('/')[-1])\n",
    "            json.dump(history_dict, open(results_folder+\"/history_\" + key + \"_\" +feature_file.split('/')[-1], 'w'))\n",
    "            loss = history_dict['loss'][-1]\n",
    "            training_loss.append(loss)\n",
    "            acc = history_dict['accuracy'][-1]\n",
    "            training_acc.append(acc)\n",
    "            val_loss = history_dict['val_loss'][-1]\n",
    "            validation_loss.append(val_loss)\n",
    "            val_acc = history_dict['val_accuracy'][-1]\n",
    "            validation_acc.append(val_acc)\n",
    "            test_loss, test_acc = model.evaluate(testing_generator)\n",
    "            testing_acc.append(test_acc)\n",
    "            testing_loss.append(test_loss)\n",
    "            vocab_size_list.append(feature_file.split('/')[-1])\n",
    "            print(\"training acc: \", acc, \", training loss: \", loss, \", val acc: \", val_acc, \", val loss: \", val_loss,\", test acc: \", test_acc, \", test loss: \", test_loss)\n",
    "            print()\n",
    "        temp_metrics_dict={'vocab_size':vocab_size_list,\n",
    "                           'training_acc': training_acc,\n",
    "                           'validation_acc': validation_acc,\n",
    "                           'testing_acc':testing_acc,\n",
    "                           'training_loss' : training_loss,\n",
    "                           'validation_loss': validation_loss,\n",
    "                           'testing_loss':testing_loss}\n",
    "        json.dump(temp_metrics_dict, open(results_folder+\"/total_history_\" + key, 'w'))\n",
    "        metrics[key] = temp_metrics_dict\n",
    "    json.dump(metrics, open(results_folder+\"/total_history_combined\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 7\n",
      "2 5 8\n",
      "3 6 9\n"
     ]
    }
   ],
   "source": [
    "for i,j,k in zip([1,2,3],[4,5,6],[7,8,9]):\n",
    "    print(i,j,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_file_path = '/home/sukanya/PhD/Datasets/PAN SCD/pan19-style-change-detection/processed/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started evaluation for: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_0.csv\n",
      "Loading training file from path:  /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_0.csv\n",
      "(18961, 6)\n",
      "Loading testing file from path:  /home/sukanya/PhD/Datasets/PAN SCD/pan19-style-change-detection/processed/test.csv\n",
      "(9538, 5)\n",
      "opt1\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt1/50word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.8958 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0054s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.7042 - accuracy: 0.5581 - val_loss: 0.6925 - val_accuracy: 0.5680\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6968 - accuracy: 0.5050 - val_loss: 0.6903 - val_accuracy: 0.5614\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6962 - accuracy: 0.5206 - val_loss: 0.6918 - val_accuracy: 0.5570\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6914 - accuracy: 0.5794 - val_loss: 0.6872 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6937 - accuracy: 0.5288 - val_loss: 0.6873 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6845 - accuracy: 0.5919 - val_loss: 0.6842 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6906 - accuracy: 0.5569 - val_loss: 0.6837 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6804 - accuracy: 0.5975 - val_loss: 0.6821 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6852 - accuracy: 0.5700 - val_loss: 0.6809 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6913 - accuracy: 0.5406 - val_loss: 0.6800 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6887 - accuracy: 0.5362 - val_loss: 0.6795 - val_accuracy: 0.5794\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6787 - accuracy: 0.5981 - val_loss: 0.6786 - val_accuracy: 0.5794\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6733 - accuracy: 0.6075 - val_loss: 0.6796 - val_accuracy: 0.5794\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6846 - accuracy: 0.5656 - val_loss: 0.6772 - val_accuracy: 0.5741\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6813 - accuracy: 0.5725 - val_loss: 0.6771 - val_accuracy: 0.5794\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6797 - accuracy: 0.5763 - val_loss: 0.6773 - val_accuracy: 0.5794\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6753 - accuracy: 0.5913 - val_loss: 0.6752 - val_accuracy: 0.5794\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6718 - accuracy: 0.5788 - val_loss: 0.6810 - val_accuracy: 0.5794\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6792 - accuracy: 0.5800 - val_loss: 0.6738 - val_accuracy: 0.5794\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6832 - accuracy: 0.5638 - val_loss: 0.6735 - val_accuracy: 0.5762\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6658 - accuracy: 0.6137 - val_loss: 0.6775 - val_accuracy: 0.5794\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6758 - accuracy: 0.5763 - val_loss: 0.6725 - val_accuracy: 0.5654\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6816 - accuracy: 0.5456 - val_loss: 0.6803 - val_accuracy: 0.5712\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6885 - accuracy: 0.5250 - val_loss: 0.6749 - val_accuracy: 0.5886\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6745 - accuracy: 0.5788 - val_loss: 0.6715 - val_accuracy: 0.5625\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6688 - accuracy: 0.5663 - val_loss: 0.6710 - val_accuracy: 0.5672\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6745 - accuracy: 0.5663 - val_loss: 0.6707 - val_accuracy: 0.5659\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6675 - accuracy: 0.5863 - val_loss: 0.6706 - val_accuracy: 0.5773\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6667 - accuracy: 0.5994 - val_loss: 0.6706 - val_accuracy: 0.5815\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6755 - accuracy: 0.5688 - val_loss: 0.6706 - val_accuracy: 0.5783\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.6471 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0193s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6673 - accuracy: 0.5863\n",
      "training acc:  0.5687500238418579 , training loss:  0.6754927635192871 , val acc:  0.5783227682113647 , val loss:  0.6705930829048157 , test acc:  0.5863045454025269 , test loss:  0.6672631502151489\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt1/100word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7155 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0153s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6823 - accuracy: 0.5950 - val_loss: 0.6889 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6912 - accuracy: 0.5525 - val_loss: 0.6837 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6730 - accuracy: 0.6131 - val_loss: 0.6936 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6871 - accuracy: 0.5831 - val_loss: 0.6816 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6729 - accuracy: 0.6062 - val_loss: 0.6865 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6827 - accuracy: 0.5775 - val_loss: 0.6781 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6739 - accuracy: 0.5994 - val_loss: 0.6794 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6712 - accuracy: 0.6100 - val_loss: 0.6785 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6835 - accuracy: 0.5756 - val_loss: 0.6748 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6729 - accuracy: 0.5950 - val_loss: 0.6765 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6699 - accuracy: 0.6044 - val_loss: 0.6736 - val_accuracy: 0.5794\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6798 - accuracy: 0.5781 - val_loss: 0.6730 - val_accuracy: 0.5783\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6724 - accuracy: 0.5794 - val_loss: 0.6723 - val_accuracy: 0.5791\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6865 - accuracy: 0.5494 - val_loss: 0.6742 - val_accuracy: 0.5794\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6852 - accuracy: 0.5425 - val_loss: 0.6717 - val_accuracy: 0.5725\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6762 - accuracy: 0.5656 - val_loss: 0.6712 - val_accuracy: 0.5614\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6600 - accuracy: 0.6187 - val_loss: 0.6729 - val_accuracy: 0.5794\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6716 - accuracy: 0.5969 - val_loss: 0.6708 - val_accuracy: 0.5725\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6654 - accuracy: 0.5856 - val_loss: 0.6732 - val_accuracy: 0.5794\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6841 - accuracy: 0.5512 - val_loss: 0.6727 - val_accuracy: 0.5820\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6692 - accuracy: 0.5975 - val_loss: 0.6705 - val_accuracy: 0.5754\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6538 - accuracy: 0.6263 - val_loss: 0.6762 - val_accuracy: 0.5794\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6803 - accuracy: 0.5663 - val_loss: 0.6705 - val_accuracy: 0.5868\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6748 - accuracy: 0.5537 - val_loss: 0.6692 - val_accuracy: 0.5860\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6753 - accuracy: 0.5594 - val_loss: 0.6684 - val_accuracy: 0.5728\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6728 - accuracy: 0.5650 - val_loss: 0.6694 - val_accuracy: 0.5657\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6756 - accuracy: 0.5763 - val_loss: 0.6682 - val_accuracy: 0.5802\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6733 - accuracy: 0.5612 - val_loss: 0.6685 - val_accuracy: 0.5862\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6696 - accuracy: 0.5763 - val_loss: 0.6753 - val_accuracy: 0.5794\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6661 - accuracy: 0.5938 - val_loss: 0.6684 - val_accuracy: 0.5667\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6643 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0156s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6647 - accuracy: 0.5767\n",
      "training acc:  0.59375 , training loss:  0.6661174297332764 , val acc:  0.5667194128036499 , val loss:  0.6684046983718872 , test acc:  0.5766568779945374 , test loss:  0.6646770238876343\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt1/150word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each people = different most used might much something now question 've he such am after well case its 're set good however number 2 been find try new problem say being since really many take both still possible data did answer those right another code who too between why were\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.4920 - accuracy: 0.8750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0027s vs `on_train_batch_end` time: 0.0231s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.7039 - accuracy: 0.5587 - val_loss: 0.6907 - val_accuracy: 0.5628\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6798 - accuracy: 0.6175 - val_loss: 0.6960 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6900 - accuracy: 0.5856 - val_loss: 0.6865 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6864 - accuracy: 0.5844 - val_loss: 0.6850 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6902 - accuracy: 0.5381 - val_loss: 0.6880 - val_accuracy: 0.5860\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6786 - accuracy: 0.6062 - val_loss: 0.6819 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6810 - accuracy: 0.5900 - val_loss: 0.6845 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6818 - accuracy: 0.5788 - val_loss: 0.6788 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6730 - accuracy: 0.5994 - val_loss: 0.6822 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6730 - accuracy: 0.6050 - val_loss: 0.6793 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6835 - accuracy: 0.5487 - val_loss: 0.6756 - val_accuracy: 0.5720\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6758 - accuracy: 0.5831 - val_loss: 0.6763 - val_accuracy: 0.5794\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6688 - accuracy: 0.5987 - val_loss: 0.6753 - val_accuracy: 0.5794\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6824 - accuracy: 0.5500 - val_loss: 0.6762 - val_accuracy: 0.5839\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6784 - accuracy: 0.5450 - val_loss: 0.6723 - val_accuracy: 0.5643\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6689 - accuracy: 0.5956 - val_loss: 0.6716 - val_accuracy: 0.5657\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6617 - accuracy: 0.6150 - val_loss: 0.6718 - val_accuracy: 0.5794\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6856 - accuracy: 0.5238 - val_loss: 0.6711 - val_accuracy: 0.5796\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6809 - accuracy: 0.5600 - val_loss: 0.6722 - val_accuracy: 0.5931\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6740 - accuracy: 0.5606 - val_loss: 0.6701 - val_accuracy: 0.5752\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6674 - accuracy: 0.5825 - val_loss: 0.6702 - val_accuracy: 0.5733\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6736 - accuracy: 0.5663 - val_loss: 0.6736 - val_accuracy: 0.5794\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6645 - accuracy: 0.6006 - val_loss: 0.6699 - val_accuracy: 0.5709\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6801 - accuracy: 0.5512 - val_loss: 0.6685 - val_accuracy: 0.5688\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6701 - accuracy: 0.5681 - val_loss: 0.6684 - val_accuracy: 0.5691\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6663 - accuracy: 0.5844 - val_loss: 0.6680 - val_accuracy: 0.5730\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6761 - accuracy: 0.5537 - val_loss: 0.6693 - val_accuracy: 0.5865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6671 - accuracy: 0.5750 - val_loss: 0.6690 - val_accuracy: 0.5625\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6839 - accuracy: 0.5500 - val_loss: 0.6679 - val_accuracy: 0.5804\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6685 - accuracy: 0.5788 - val_loss: 0.6714 - val_accuracy: 0.5791\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.7369 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0134s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6696 - accuracy: 0.5924\n",
      "training acc:  0.5787500143051147 , training loss:  0.6685338020324707 , val acc:  0.5791139006614685 , val loss:  0.6713536977767944 , test acc:  0.5923867225646973 , test loss:  0.669553816318512\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt1/200word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each people = different most used might much something now question 've he such am after well case its 're set good however number 2 been find try new problem say being since really many take both still possible data did answer those right another code who too between why were while over go had point change without value probably things page + better before image let ca sure create add back url$ enough actually 'd look 'll through system our his 3 function able down file every going long following given    own seems second off thing part though help\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7118 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0027s vs `on_train_batch_end` time: 0.0220s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6910 - accuracy: 0.5575 - val_loss: 0.6867 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6889 - accuracy: 0.5769 - val_loss: 0.6827 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6717 - accuracy: 0.6250 - val_loss: 0.6822 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6838 - accuracy: 0.5681 - val_loss: 0.6782 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6755 - accuracy: 0.5944 - val_loss: 0.6760 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6609 - accuracy: 0.6338 - val_loss: 0.6921 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6627 - accuracy: 0.6331 - val_loss: 0.6810 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6853 - accuracy: 0.5406 - val_loss: 0.6729 - val_accuracy: 0.5670\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6748 - accuracy: 0.5713 - val_loss: 0.6778 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6854 - accuracy: 0.5519 - val_loss: 0.6739 - val_accuracy: 0.5910\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6820 - accuracy: 0.5481 - val_loss: 0.6716 - val_accuracy: 0.5783\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6693 - accuracy: 0.5894 - val_loss: 0.6705 - val_accuracy: 0.5646\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6655 - accuracy: 0.5962 - val_loss: 0.6710 - val_accuracy: 0.5789\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6813 - accuracy: 0.5487 - val_loss: 0.6700 - val_accuracy: 0.5899\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6750 - accuracy: 0.5744 - val_loss: 0.6743 - val_accuracy: 0.5818\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6857 - accuracy: 0.5600 - val_loss: 0.6704 - val_accuracy: 0.5889\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6791 - accuracy: 0.5694 - val_loss: 0.6745 - val_accuracy: 0.5839\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7472 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0126s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6732 - accuracy: 0.5887\n",
      "training acc:  0.5693749785423279 , training loss:  0.6791483163833618 , val acc:  0.5838607549667358 , val loss:  0.6744591593742371 , test acc:  0.588716447353363 , test loss:  0.6732144951820374\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt1/250word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each people = different most used might much something now question 've he such am after well case its 're set good however number 2 been find try new problem say being since really many take both still possible data did answer those right another code who too between why were while over go had point change without value probably things page + better before image let ca sure create add back url$ enough actually 'd look 'll through system our his 3 function able down file every going long following given    own seems second off thing part though help around either best lot give etc always x form high note > means power above order less check list edit found must few bit solution having based start made trying % done end option doing least likely run field & key size \\mathcal word server keep small type 0 already\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.3739 - accuracy: 0.8750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0253s). Check your callbacks.\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.7470 - accuracy: 0.5316WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0141s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.7462 - accuracy: 0.5331 - val_loss: 0.6810 - val_accuracy: 0.5765\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6836 - accuracy: 0.5750 - val_loss: 0.6777 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6855 - accuracy: 0.5325 - val_loss: 0.6781 - val_accuracy: 0.5804\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6858 - accuracy: 0.5369 - val_loss: 0.6803 - val_accuracy: 0.5796\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6840 - accuracy: 0.5675 - val_loss: 0.6748 - val_accuracy: 0.5720\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6721 - accuracy: 0.5975 - val_loss: 0.6760 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6808 - accuracy: 0.5506 - val_loss: 0.6750 - val_accuracy: 0.5902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6711 - accuracy: 0.5944 - val_loss: 0.6722 - val_accuracy: 0.5678\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6629 - accuracy: 0.5925 - val_loss: 0.6876 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6787 - accuracy: 0.5744 - val_loss: 0.6710 - val_accuracy: 0.5696\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6818 - accuracy: 0.5425 - val_loss: 0.6705 - val_accuracy: 0.5630\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6696 - accuracy: 0.6056 - val_loss: 0.6702 - val_accuracy: 0.5643\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6768 - accuracy: 0.5738 - val_loss: 0.6713 - val_accuracy: 0.5902\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6647 - accuracy: 0.5931 - val_loss: 0.6712 - val_accuracy: 0.5770\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6772 - accuracy: 0.5644 - val_loss: 0.6694 - val_accuracy: 0.5667\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6863 - accuracy: 0.5412 - val_loss: 0.6710 - val_accuracy: 0.5899\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6663 - accuracy: 0.5819 - val_loss: 0.6705 - val_accuracy: 0.5730\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6671 - accuracy: 0.5850 - val_loss: 0.6705 - val_accuracy: 0.5720\n",
      "  1/596 [..............................] - ETA: 0s - loss: 0.6520 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0141s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6654 - accuracy: 0.5785\n",
      "training acc:  0.5849999785423279 , training loss:  0.6670557260513306 , val acc:  0.5719936490058899 , val loss:  0.6705494523048401 , test acc:  0.5785444378852844 , test loss:  0.6654130816459656\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt1/300word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each people = different most used might much something now question 've he such am after well case its 're set good however number 2 been find try new problem say being since really many take both still possible data did answer those right another code who too between why were while over go had point change without value probably things page + better before image let ca sure create add back url$ enough actually 'd look 'll through system our his 3 function able down file every going long following given    own seems second off thing part though help around either best lot give etc always x form high note > means power above order less check list edit found must few bit solution having based start made trying % done end option doing least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct once day again similar looking light rather issue non us information person someone specific output reason single mean result space she name de times said simple open quite simply never level show process put years place real often\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6955 - accuracy: 0.5525 - val_loss: 0.6815 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6764 - accuracy: 0.6006 - val_loss: 0.6826 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6829 - accuracy: 0.5731 - val_loss: 0.6776 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6814 - accuracy: 0.5831 - val_loss: 0.6773 - val_accuracy: 0.5736\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6842 - accuracy: 0.5475 - val_loss: 0.6744 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6760 - accuracy: 0.5813 - val_loss: 0.6756 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6687 - accuracy: 0.6087 - val_loss: 0.6723 - val_accuracy: 0.5728\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6827 - accuracy: 0.5531 - val_loss: 0.6728 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6688 - accuracy: 0.5781 - val_loss: 0.6741 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6762 - accuracy: 0.5794 - val_loss: 0.6706 - val_accuracy: 0.5778\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6733 - accuracy: 0.5875 - val_loss: 0.6734 - val_accuracy: 0.5849\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6609 - accuracy: 0.6162 - val_loss: 0.6717 - val_accuracy: 0.5791\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6675 - accuracy: 0.5825 - val_loss: 0.6691 - val_accuracy: 0.5609\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6649 - accuracy: 0.5863 - val_loss: 0.6695 - val_accuracy: 0.5643\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6828 - accuracy: 0.5100 - val_loss: 0.6689 - val_accuracy: 0.5907\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6716 - accuracy: 0.5550 - val_loss: 0.6690 - val_accuracy: 0.5902\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6677 - accuracy: 0.5763 - val_loss: 0.6676 - val_accuracy: 0.5680\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6696 - accuracy: 0.5725 - val_loss: 0.6673 - val_accuracy: 0.5876\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6784 - accuracy: 0.5600 - val_loss: 0.6674 - val_accuracy: 0.5873\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6726 - accuracy: 0.5675 - val_loss: 0.6672 - val_accuracy: 0.5749\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6683 - accuracy: 0.5800 - val_loss: 0.6671 - val_accuracy: 0.5775\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6804 - accuracy: 0.5519 - val_loss: 0.6686 - val_accuracy: 0.5910\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6624 - accuracy: 0.5894 - val_loss: 0.6674 - val_accuracy: 0.5675\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6767 - accuracy: 0.5544 - val_loss: 0.6674 - val_accuracy: 0.5638\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6624 - accuracy: 0.5818\n",
      "training acc:  0.5543749928474426 , training loss:  0.67670077085495 , val acc:  0.5638185739517212 , val loss:  0.6674444675445557 , test acc:  0.5817952752113342 , test loss:  0.6624096632003784\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt1/350word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each people = different most used might much something now question 've he such am after well case its 're set good however number 2 been find try new problem say being since really many take both still possible data did answer those right another code who too between why were while over go had point change without value probably things page + better before image let ca sure create add back url$ enough actually 'd look 'll through system our his 3 function able down file every going long following given    own seems second off thing part though help around either best lot give etc always x form high note > means power above order less check list edit found must few bit solution having based start made trying % done end option doing least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct once day again similar looking light rather issue non us information person someone specific output reason single mean result space she name de times said simple open quite simply never level show process put years place real often usually input b understand site error below far last c world come group large under online working object fact que side course hard \\to makes method values layer important until him whether 5 next el original idea pretty got water called state amount current her color test tried y getting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6936 - accuracy: 0.5437 - val_loss: 0.6858 - val_accuracy: 0.5870\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6874 - accuracy: 0.5688 - val_loss: 0.6812 - val_accuracy: 0.5630\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6824 - accuracy: 0.5806 - val_loss: 0.6822 - val_accuracy: 0.5860\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6771 - accuracy: 0.5850 - val_loss: 0.6753 - val_accuracy: 0.5701\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6693 - accuracy: 0.6012 - val_loss: 0.6803 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6841 - accuracy: 0.5706 - val_loss: 0.6756 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6608 - accuracy: 0.6306 - val_loss: 0.6730 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6847 - accuracy: 0.5462 - val_loss: 0.6713 - val_accuracy: 0.5643\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6743 - accuracy: 0.5688 - val_loss: 0.6712 - val_accuracy: 0.5736\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6745 - accuracy: 0.5688 - val_loss: 0.6702 - val_accuracy: 0.5659\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6795 - accuracy: 0.5412 - val_loss: 0.6715 - val_accuracy: 0.5786\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6768 - accuracy: 0.5788 - val_loss: 0.6754 - val_accuracy: 0.5854\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6757 - accuracy: 0.5594 - val_loss: 0.6693 - val_accuracy: 0.5889\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6712 - accuracy: 0.5556 - val_loss: 0.6688 - val_accuracy: 0.5646\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6704 - accuracy: 0.5850 - val_loss: 0.6690 - val_accuracy: 0.5628\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6671 - accuracy: 0.5844 - val_loss: 0.6684 - val_accuracy: 0.5899\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6713 - accuracy: 0.5731 - val_loss: 0.6677 - val_accuracy: 0.5839\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6760 - accuracy: 0.5456 - val_loss: 0.6706 - val_accuracy: 0.5881\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6828 - accuracy: 0.5669 - val_loss: 0.6691 - val_accuracy: 0.5926\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6721 - accuracy: 0.5894 - val_loss: 0.6699 - val_accuracy: 0.5657\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6530 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_test_batch_end` time: 0.0183s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6644 - accuracy: 0.5762\n",
      "training acc:  0.5893750190734863 , training loss:  0.6721302270889282 , val acc:  0.565664529800415 , val loss:  0.6699225306510925 , test acc:  0.5762374401092529 , test loss:  0.6644439697265625\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt1/400word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each people = different most used might much something now question 've he such am after well case its 're set good however number 2 been find try new problem say being since really many take both still possible data did answer those right another code who too between why were while over go had point change without value probably things page + better before image let ca sure create add back url$ enough actually 'd look 'll through system our his 3 function able down file every going long following given    own seems second off thing part though help around either best lot give etc always x form high note > means power above order less check list edit found must few bit solution having based start made trying % done end option doing least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct once day again similar looking light rather issue non us information person someone specific output reason single mean result space she name de times said simple open quite simply never level show process put years place real often usually input b understand site error below far last c world come group large under online working object fact que side course hard \\to makes method values layer important until him whether 5 next el original idea pretty got water called state amount current her color test tried y getting 10 assume    free kind common    bytes left wo else words call character general sense seem block difference past consider post top matter results created available source full itself old term 4 great running life range control class address particular god numbers camera three paper human within i.e. e.g.\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.7500 - accuracy: 0.5531WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_test_batch_end` time: 0.0064s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.7500 - accuracy: 0.5531 - val_loss: 0.6983 - val_accuracy: 0.5200\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6948 - accuracy: 0.5575 - val_loss: 0.6952 - val_accuracy: 0.4950\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6935 - accuracy: 0.5113 - val_loss: 0.6920 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6839 - accuracy: 0.6012 - val_loss: 0.6902 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6783 - accuracy: 0.6087 - val_loss: 0.6850 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6913 - accuracy: 0.5381 - val_loss: 0.6960 - val_accuracy: 0.5071\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6844 - accuracy: 0.5756 - val_loss: 0.6785 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6839 - accuracy: 0.5800 - val_loss: 0.6773 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6837 - accuracy: 0.5669 - val_loss: 0.6820 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6565 - accuracy: 0.6406 - val_loss: 0.6847 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6838 - accuracy: 0.5506 - val_loss: 0.6751 - val_accuracy: 0.5918\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6681 - accuracy: 0.6144 - val_loss: 0.6789 - val_accuracy: 0.5794\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6750 - accuracy: 0.5869 - val_loss: 0.6714 - val_accuracy: 0.5662\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6654 - accuracy: 0.6219 - val_loss: 0.6755 - val_accuracy: 0.5794\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6729 - accuracy: 0.5931 - val_loss: 0.6761 - val_accuracy: 0.5841\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6744 - accuracy: 0.5706 - val_loss: 0.6722 - val_accuracy: 0.5897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/596 [..............................] - ETA: 7s - loss: 0.7212 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0198s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6703 - accuracy: 0.6034\n",
      "training acc:  0.5706250071525574 , training loss:  0.6743893623352051 , val acc:  0.5896624326705933 , val loss:  0.6722056269645691 , test acc:  0.6033976674079895 , test loss:  0.670284628868103\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt1/450word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each people = different most used might much something now question 've he such am after well case its 're set good however number 2 been find try new problem say being since really many take both still possible data did answer those right another code who too between why were while over go had point change without value probably things page + better before image let ca sure create add back url$ enough actually 'd look 'll through system our his 3 function able down file every going long following given    own seems second off thing part though help around either best lot give etc always x form high note > means power above order less check list edit found must few bit solution having based start made trying % done end option doing least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct once day again similar looking light rather issue non us information person someone specific output reason single mean result space she name de times said simple open quite simply never level show process put years place real often usually input b understand site error below far last c world come group large under online working object fact que side course hard \\to makes method values layer important until him whether 5 next el original idea pretty got water called state amount current her color test tried y getting 10 assume    free kind common    bytes left wo else words call character general sense seem block difference past consider post top matter results created available source full itself old term 4 great running life range control class address particular god numbers camera three paper human within i.e. e.g. account select access due short # needs area away energy google comes book search changes write low anyone everything provide exactly yes ai easy experience making < thought wrong fine multiple product language hand standard n thanks several game perhaps ask tell design ' year nothing lower although uses feel\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7130 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0137s). Check your callbacks.\n",
      " 98/100 [============================>.] - ETA: 0s - loss: 0.6958 - accuracy: 0.5102WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_test_batch_end` time: 0.0138s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6955 - accuracy: 0.5125 - val_loss: 0.6845 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6888 - accuracy: 0.5663 - val_loss: 0.6858 - val_accuracy: 0.5712\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6797 - accuracy: 0.5838 - val_loss: 0.6851 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6788 - accuracy: 0.5962 - val_loss: 0.6799 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6648 - accuracy: 0.6319 - val_loss: 0.6819 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6752 - accuracy: 0.5938 - val_loss: 0.6762 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6891 - accuracy: 0.5300 - val_loss: 0.6838 - val_accuracy: 0.5773\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6827 - accuracy: 0.5675 - val_loss: 0.6748 - val_accuracy: 0.5770\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6717 - accuracy: 0.5969 - val_loss: 0.6737 - val_accuracy: 0.5770\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6721 - accuracy: 0.5819 - val_loss: 0.6772 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6624 - accuracy: 0.6137 - val_loss: 0.6718 - val_accuracy: 0.5643\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6720 - accuracy: 0.5669 - val_loss: 0.6721 - val_accuracy: 0.5791\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6731 - accuracy: 0.5775 - val_loss: 0.6706 - val_accuracy: 0.5638\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6748 - accuracy: 0.5813 - val_loss: 0.6721 - val_accuracy: 0.5794\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6628 - accuracy: 0.5881 - val_loss: 0.6701 - val_accuracy: 0.5649\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6766 - accuracy: 0.5663 - val_loss: 0.6708 - val_accuracy: 0.5733\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6714 - accuracy: 0.5725 - val_loss: 0.6715 - val_accuracy: 0.5786\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6755 - accuracy: 0.5650 - val_loss: 0.6701 - val_accuracy: 0.5683\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6745 - accuracy: 0.5650 - val_loss: 0.6782 - val_accuracy: 0.5791\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6672 - accuracy: 0.5819 - val_loss: 0.6682 - val_accuracy: 0.5738\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6791 - accuracy: 0.5512 - val_loss: 0.6686 - val_accuracy: 0.5659\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6869 - accuracy: 0.5356 - val_loss: 0.6684 - val_accuracy: 0.5825\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6643 - accuracy: 0.5894 - val_loss: 0.6713 - val_accuracy: 0.5741\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6450 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0140s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6659 - accuracy: 0.5822\n",
      "training acc:  0.5893750190734863 , training loss:  0.6642787456512451 , val acc:  0.5741033554077148 , val loss:  0.6713194251060486 , test acc:  0.5822147727012634 , test loss:  0.6659209728240967\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt1/500word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each people = different most used might much something now question 've he such am after well case its 're set good however number 2 been find try new problem say being since really many take both still possible data did answer those right another code who too between why were while over go had point change without value probably things page + better before image let ca sure create add back url$ enough actually 'd look 'll through system our his 3 function able down file every going long following given    own seems second off thing part though help around either best lot give etc always x form high note > means power above order less check list edit found must few bit solution having based start made trying % done end option doing least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct once day again similar looking light rather issue non us information person someone specific output reason single mean result space she name de times said simple open quite simply never level show process put years place real often usually input b understand site error below far last c world come group large under online working object fact que side course hard \\to makes method values layer important until him whether 5 next el original idea pretty got water called state amount current her color test tried y getting 10 assume    free kind common    bytes left wo else words call character general sense seem block difference past consider post top matter results created available source full itself old term 4 great running life range control class address particular god numbers camera three paper human within i.e. e.g. account select access due short # needs area away energy google comes book search changes write low anyone everything provide exactly yes ai easy experience making < thought wrong fine multiple product language hand standard n thanks several game perhaps ask tell design ' year nothing lower although uses feel thus points update takes looks position useful added cases unless d true clear return lens _ content higher effect price believe en further almost view la model certain days string length files click context sentence others big against s normal gives step options turn english sort map terms close actual\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 98/100 [============================>.] - ETA: 0s - loss: 0.7141 - accuracy: 0.4930WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0131s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.7136 - accuracy: 0.4950 - val_loss: 0.6863 - val_accuracy: 0.5847\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6884 - accuracy: 0.5506 - val_loss: 0.6793 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6810 - accuracy: 0.5850 - val_loss: 0.6776 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6732 - accuracy: 0.6056 - val_loss: 0.6759 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6751 - accuracy: 0.5725 - val_loss: 0.6775 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6835 - accuracy: 0.5612 - val_loss: 0.6724 - val_accuracy: 0.5701\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6794 - accuracy: 0.5656 - val_loss: 0.6758 - val_accuracy: 0.5857\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6760 - accuracy: 0.5694 - val_loss: 0.6714 - val_accuracy: 0.5778\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6683 - accuracy: 0.6056 - val_loss: 0.6763 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6780 - accuracy: 0.5550 - val_loss: 0.6707 - val_accuracy: 0.5733\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6747 - accuracy: 0.5681 - val_loss: 0.6688 - val_accuracy: 0.5678\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6760 - accuracy: 0.5644 - val_loss: 0.6719 - val_accuracy: 0.5786\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6770 - accuracy: 0.5731 - val_loss: 0.6683 - val_accuracy: 0.5704\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6757 - accuracy: 0.5763 - val_loss: 0.6688 - val_accuracy: 0.5633\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6685 - accuracy: 0.5706 - val_loss: 0.6768 - val_accuracy: 0.5794\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6698 - accuracy: 0.5806 - val_loss: 0.6679 - val_accuracy: 0.5738\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6634 - accuracy: 0.5769 - val_loss: 0.6688 - val_accuracy: 0.5628\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6646 - accuracy: 0.5844 - val_loss: 0.6673 - val_accuracy: 0.5717\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6775 - accuracy: 0.5431 - val_loss: 0.6676 - val_accuracy: 0.5643\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6765 - accuracy: 0.5606 - val_loss: 0.6683 - val_accuracy: 0.5891\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6801 - accuracy: 0.5700 - val_loss: 0.6773 - val_accuracy: 0.5825\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6770 - accuracy: 0.5834\n",
      "training acc:  0.5699999928474426 , training loss:  0.6801482439041138 , val acc:  0.5825421810150146 , val loss:  0.6773322224617004 , test acc:  0.5833683013916016 , test loss:  0.6770035624504089\n",
      "\n",
      "opt2\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt2/50word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7555 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0079s vs `on_train_batch_end` time: 0.0151s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.7124 - accuracy: 0.4894 - val_loss: 0.6936 - val_accuracy: 0.4813\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6941 - accuracy: 0.5294 - val_loss: 0.6918 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.7010 - accuracy: 0.5100 - val_loss: 0.7082 - val_accuracy: 0.4246\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6962 - accuracy: 0.5019 - val_loss: 0.6904 - val_accuracy: 0.5701\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6917 - accuracy: 0.5544 - val_loss: 0.6835 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6924 - accuracy: 0.5256 - val_loss: 0.6812 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6797 - accuracy: 0.5962 - val_loss: 0.6844 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6879 - accuracy: 0.5631 - val_loss: 0.6843 - val_accuracy: 0.5765\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6923 - accuracy: 0.5156 - val_loss: 0.6780 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6824 - accuracy: 0.5838 - val_loss: 0.6779 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6856 - accuracy: 0.5569 - val_loss: 0.6847 - val_accuracy: 0.5643\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6822 - accuracy: 0.5644 - val_loss: 0.6757 - val_accuracy: 0.5791\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6730 - accuracy: 0.5944 - val_loss: 0.6817 - val_accuracy: 0.5672\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6845 - accuracy: 0.5587 - val_loss: 0.6764 - val_accuracy: 0.5796\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6830 - accuracy: 0.5612 - val_loss: 0.6754 - val_accuracy: 0.5744\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6809 - accuracy: 0.5587 - val_loss: 0.6732 - val_accuracy: 0.5794\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6698 - accuracy: 0.5981 - val_loss: 0.6740 - val_accuracy: 0.5794\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6802 - accuracy: 0.5738 - val_loss: 0.6721 - val_accuracy: 0.5759\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6782 - accuracy: 0.5863 - val_loss: 0.6718 - val_accuracy: 0.5636\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6727 - accuracy: 0.5781 - val_loss: 0.6746 - val_accuracy: 0.5794\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6678 - accuracy: 0.6056 - val_loss: 0.6718 - val_accuracy: 0.5694\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6757 - accuracy: 0.5719 - val_loss: 0.6781 - val_accuracy: 0.5691\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6884 - accuracy: 0.5256 - val_loss: 0.6739 - val_accuracy: 0.5773\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6803 - accuracy: 0.5519 - val_loss: 0.6732 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6289 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0132s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6699 - accuracy: 0.5877\n",
      "training acc:  0.5518749952316284 , training loss:  0.6802618503570557 , val acc:  0.5793776512145996 , val loss:  0.6731613278388977 , test acc:  0.5876677632331848 , test loss:  0.6699070334434509\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt2/100word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/100 [..............................] - ETA: 0s - loss: 0.7580 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0028s vs `on_train_batch_end` time: 0.0104s). Check your callbacks.\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.8305 - accuracy: 0.5827WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0146s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.8291 - accuracy: 0.5813 - val_loss: 0.6989 - val_accuracy: 0.4618\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6986 - accuracy: 0.5038 - val_loss: 0.7001 - val_accuracy: 0.4201\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6897 - accuracy: 0.5469 - val_loss: 0.6925 - val_accuracy: 0.5483\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6825 - accuracy: 0.6006 - val_loss: 0.6966 - val_accuracy: 0.5741\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6831 - accuracy: 0.6019 - val_loss: 0.6894 - val_accuracy: 0.5717\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6912 - accuracy: 0.5625 - val_loss: 0.6870 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6972 - accuracy: 0.5256 - val_loss: 0.6885 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6884 - accuracy: 0.5788 - val_loss: 0.6837 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6915 - accuracy: 0.5419 - val_loss: 0.6843 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6655 - accuracy: 0.6313 - val_loss: 0.6937 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6837 - accuracy: 0.5900 - val_loss: 0.6867 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6004 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0154s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6830 - accuracy: 0.5877\n",
      "training acc:  0.5899999737739563 , training loss:  0.6836929321289062 , val acc:  0.5793776512145996 , val loss:  0.6867071986198425 , test acc:  0.5876677632331848 , test loss:  0.6829521656036377\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt2/150word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each people = different most used might much something now question 've he such am after well case its 're set good however number 2 been find try new problem say being since really many take both still possible data did answer those right another code who too between\n",
      "Epoch 1/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6953 - accuracy: 0.5259WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_test_batch_end` time: 0.0132s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6954 - accuracy: 0.5238 - val_loss: 0.6875 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6886 - accuracy: 0.5738 - val_loss: 0.6843 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6796 - accuracy: 0.6037 - val_loss: 0.6862 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6865 - accuracy: 0.5519 - val_loss: 0.6809 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6874 - accuracy: 0.5512 - val_loss: 0.6816 - val_accuracy: 0.5775\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6697 - accuracy: 0.6219 - val_loss: 0.6784 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6788 - accuracy: 0.5944 - val_loss: 0.6783 - val_accuracy: 0.5625\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6720 - accuracy: 0.6044 - val_loss: 0.6779 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6813 - accuracy: 0.5669 - val_loss: 0.6754 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6781 - accuracy: 0.5819 - val_loss: 0.6967 - val_accuracy: 0.5251\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6860 - accuracy: 0.5531 - val_loss: 0.6730 - val_accuracy: 0.5794\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6720 - accuracy: 0.5906 - val_loss: 0.6722 - val_accuracy: 0.5794\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6649 - accuracy: 0.6106 - val_loss: 0.6745 - val_accuracy: 0.5794\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6769 - accuracy: 0.5881 - val_loss: 0.6709 - val_accuracy: 0.5767\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6645 - accuracy: 0.6062 - val_loss: 0.6744 - val_accuracy: 0.5794\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6642 - accuracy: 0.6181 - val_loss: 0.6739 - val_accuracy: 0.5794\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6688 - accuracy: 0.5994 - val_loss: 0.6773 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6161 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_test_batch_end` time: 0.0132s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6726 - accuracy: 0.5877\n",
      "training acc:  0.5993750095367432 , training loss:  0.6687567234039307 , val acc:  0.5793776512145996 , val loss:  0.6772509813308716 , test acc:  0.5876677632331848 , test loss:  0.6726293563842773\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt2/200word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each people = different most used might much something now question 've he such am after well case its 're set good however number 2 been find try new problem say being since really many take both still possible data did answer those right another code who too between why were while over go had point change without value probably things page + better before image let ca sure create add back url$ enough actually 'd look 'll through system our his 3 function able down file every going long following given    own seems second off thing part\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.9488 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0079s vs `on_train_batch_end` time: 0.0197s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6959 - accuracy: 0.5350 - val_loss: 0.6884 - val_accuracy: 0.5791\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6818 - accuracy: 0.5881 - val_loss: 0.6812 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6871 - accuracy: 0.5644 - val_loss: 0.6797 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6818 - accuracy: 0.5800 - val_loss: 0.6783 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6814 - accuracy: 0.5800 - val_loss: 0.6788 - val_accuracy: 0.5862\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6726 - accuracy: 0.5806 - val_loss: 0.6772 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6653 - accuracy: 0.6119 - val_loss: 0.6748 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6876 - accuracy: 0.5337 - val_loss: 0.6721 - val_accuracy: 0.5575\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6653 - accuracy: 0.6056 - val_loss: 0.6720 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6700 - accuracy: 0.6031 - val_loss: 0.6738 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6718 - accuracy: 0.5950 - val_loss: 0.6749 - val_accuracy: 0.5818\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6849 - accuracy: 0.5331 - val_loss: 0.6732 - val_accuracy: 0.5783\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6726 - accuracy: 0.5942\n",
      "training acc:  0.5331249833106995 , training loss:  0.6849209666252136 , val acc:  0.5783227682113647 , val loss:  0.6732357144355774 , test acc:  0.5941694378852844 , test loss:  0.6726324558258057\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt2/250word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each people = different most used might much something now question 've he such am after well case its 're set good however number 2 been find try new problem say being since really many take both still possible data did answer those right another code who too between why were while over go had point change without value probably things page + better before image let ca sure create add back url$ enough actually 'd look 'll through system our his 3 function able down file every going long following given    own seems second off thing part though help around either best lot give etc always x form high note > means power above order less check list edit found must few bit solution having based start made trying % done end option doing least likely run field & key size \\mathcal word server keep small type\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.8969 - accuracy: 0.4769 - val_loss: 0.6866 - val_accuracy: 0.5651\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6719 - accuracy: 0.6056 - val_loss: 0.6769 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6701 - accuracy: 0.6019 - val_loss: 0.6740 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6752 - accuracy: 0.5788 - val_loss: 0.6723 - val_accuracy: 0.5725\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6755 - accuracy: 0.5806 - val_loss: 0.6732 - val_accuracy: 0.5857\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6825 - accuracy: 0.5587 - val_loss: 0.6761 - val_accuracy: 0.5839\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6791 - accuracy: 0.5719 - val_loss: 0.6709 - val_accuracy: 0.5791\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6614 - accuracy: 0.6069 - val_loss: 0.6813 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6841 - accuracy: 0.5562 - val_loss: 0.6696 - val_accuracy: 0.5583\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6704 - accuracy: 0.5969 - val_loss: 0.6708 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6727 - accuracy: 0.5869 - val_loss: 0.6704 - val_accuracy: 0.5870\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6810 - accuracy: 0.5462 - val_loss: 0.6688 - val_accuracy: 0.5607\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6658 - accuracy: 0.6094 - val_loss: 0.6736 - val_accuracy: 0.5794\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6769 - accuracy: 0.5594 - val_loss: 0.6681 - val_accuracy: 0.5622\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6630 - accuracy: 0.6006 - val_loss: 0.6779 - val_accuracy: 0.5794\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6738 - accuracy: 0.5631 - val_loss: 0.6792 - val_accuracy: 0.5794\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6876 - accuracy: 0.5450 - val_loss: 0.6727 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6324 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0144s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6680 - accuracy: 0.5877\n",
      "training acc:  0.5450000166893005 , training loss:  0.687577486038208 , val acc:  0.5793776512145996 , val loss:  0.6727373003959656 , test acc:  0.5876677632331848 , test loss:  0.6679751873016357\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt2/300word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each people = different most used might much something now question 've he such am after well case its 're set good however number 2 been find try new problem say being since really many take both still possible data did answer those right another code who too between why were while over go had point change without value probably things page + better before image let ca sure create add back url$ enough actually 'd look 'll through system our his 3 function able down file every going long following given    own seems second off thing part though help around either best lot give etc always x form high note > means power above order less check list edit found must few bit solution having based start made trying % done end option doing least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct once day again similar looking light rather issue non us information person someone specific output reason single mean result space she name de times said simple open quite simply never level show process put years place\n",
      "Epoch 1/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.8960 - accuracy: 0.4817WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_test_batch_end` time: 0.0134s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.8940 - accuracy: 0.4812 - val_loss: 0.6840 - val_accuracy: 0.5849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6762 - accuracy: 0.6012 - val_loss: 0.6766 - val_accuracy: 0.5783\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6780 - accuracy: 0.5931 - val_loss: 0.6794 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6834 - accuracy: 0.5706 - val_loss: 0.6744 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6819 - accuracy: 0.5800 - val_loss: 0.6740 - val_accuracy: 0.5789\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6753 - accuracy: 0.5975 - val_loss: 0.6733 - val_accuracy: 0.5773\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6776 - accuracy: 0.5444 - val_loss: 0.6721 - val_accuracy: 0.5578\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6762 - accuracy: 0.5938 - val_loss: 0.6830 - val_accuracy: 0.5712\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6770 - accuracy: 0.5694 - val_loss: 0.6740 - val_accuracy: 0.5862\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6829 - accuracy: 0.5612 - val_loss: 0.6708 - val_accuracy: 0.5578\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6702 - accuracy: 0.5938 - val_loss: 0.6707 - val_accuracy: 0.5789\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6788 - accuracy: 0.5713 - val_loss: 0.6699 - val_accuracy: 0.5749\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6771 - accuracy: 0.5656 - val_loss: 0.6692 - val_accuracy: 0.5570\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6689 - accuracy: 0.5975 - val_loss: 0.6687 - val_accuracy: 0.5572\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6705 - accuracy: 0.5900 - val_loss: 0.6792 - val_accuracy: 0.5778\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6818 - accuracy: 0.5462 - val_loss: 0.6683 - val_accuracy: 0.5570\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6786 - accuracy: 0.5575 - val_loss: 0.6675 - val_accuracy: 0.5665\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6769 - accuracy: 0.5763 - val_loss: 0.6677 - val_accuracy: 0.5585\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6717 - accuracy: 0.5831 - val_loss: 0.6755 - val_accuracy: 0.5841\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6735 - accuracy: 0.5606 - val_loss: 0.6674 - val_accuracy: 0.5881\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6600 - accuracy: 0.6012 - val_loss: 0.6743 - val_accuracy: 0.5794\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6588 - accuracy: 0.6012 - val_loss: 0.6730 - val_accuracy: 0.5794\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6692 - accuracy: 0.5844 - val_loss: 0.6671 - val_accuracy: 0.5593\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6641 - accuracy: 0.5863 - val_loss: 0.6694 - val_accuracy: 0.5688\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6755 - accuracy: 0.5769 - val_loss: 0.6680 - val_accuracy: 0.5580\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6721 - accuracy: 0.5694 - val_loss: 0.6658 - val_accuracy: 0.5831\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6751 - accuracy: 0.5744 - val_loss: 0.6662 - val_accuracy: 0.5622\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6733 - accuracy: 0.5519 - val_loss: 0.6659 - val_accuracy: 0.5860\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6829 - accuracy: 0.5450 - val_loss: 0.6657 - val_accuracy: 0.5770\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6648 - accuracy: 0.5925 - val_loss: 0.6662 - val_accuracy: 0.5881\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.7096 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_test_batch_end` time: 0.0126s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6636 - accuracy: 0.6005\n",
      "training acc:  0.5924999713897705 , training loss:  0.6648221015930176 , val acc:  0.5880801677703857 , val loss:  0.6662068963050842 , test acc:  0.6004614233970642 , test loss:  0.6636497378349304\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt2/350word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each people = different most used might much something now question 've he such am after well case its 're set good however number 2 been find try new problem say being since really many take both still possible data did answer those right another code who too between why were while over go had point change without value probably things page + better before image let ca sure create add back url$ enough actually 'd look 'll through system our his 3 function able down file every going long following given    own seems second off thing part though help around either best lot give etc always x form high note > means power above order less check list edit found must few bit solution having based start made trying % done end option doing least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct once day again similar looking light rather issue non us information person someone specific output reason single mean result space she name de times said simple open quite simply never level show process put years place real often usually input b understand site error below far last c world come group large under online working object fact que side course hard \\to makes method values layer important until him whether 5 next el original idea pretty got water called state amount current her color test tried\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 1.1868 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0166s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.7776 - accuracy: 0.5113 - val_loss: 0.6787 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6810 - accuracy: 0.5800 - val_loss: 0.6800 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6892 - accuracy: 0.5337 - val_loss: 0.6771 - val_accuracy: 0.5630\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6810 - accuracy: 0.5638 - val_loss: 0.6755 - val_accuracy: 0.5649\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6761 - accuracy: 0.5987 - val_loss: 0.6746 - val_accuracy: 0.5593\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6864 - accuracy: 0.5694 - val_loss: 0.6779 - val_accuracy: 0.5854\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6792 - accuracy: 0.5631 - val_loss: 0.6725 - val_accuracy: 0.5715\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6777 - accuracy: 0.5806 - val_loss: 0.6749 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6700 - accuracy: 0.6000 - val_loss: 0.6911 - val_accuracy: 0.5794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6787 - accuracy: 0.5819 - val_loss: 0.6740 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6273 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0133s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6702 - accuracy: 0.5877\n",
      "training acc:  0.5818750262260437 , training loss:  0.678682267665863 , val acc:  0.5793776512145996 , val loss:  0.6740440130233765 , test acc:  0.5876677632331848 , test loss:  0.6702364087104797\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt2/400word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each people = different most used might much something now question 've he such am after well case its 're set good however number 2 been find try new problem say being since really many take both still possible data did answer those right another code who too between why were while over go had point change without value probably things page + better before image let ca sure create add back url$ enough actually 'd look 'll through system our his 3 function able down file every going long following given    own seems second off thing part though help around either best lot give etc always x form high note > means power above order less check list edit found must few bit solution having based start made trying % done end option doing least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct once day again similar looking light rather issue non us information person someone specific output reason single mean result space she name de times said simple open quite simply never level show process put years place real often usually input b understand site error below far last c world come group large under online working object fact que side course hard \\to makes method values layer important until him whether 5 next el original idea pretty got water called state amount current her color test tried y getting 10 assume    free kind common    bytes left wo else words call character general sense seem block difference past consider post top matter results created available source full itself old term 4 great running life range control class address particular god numbers camera three paper human within\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.5069 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_train_batch_end` time: 0.0123s). Check your callbacks.\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6966 - accuracy: 0.5925WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0087s vs `on_test_batch_end` time: 0.0134s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6966 - accuracy: 0.5925 - val_loss: 0.6922 - val_accuracy: 0.5728\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 0.6879 - accuracy: 0.5781 - val_loss: 0.6907 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6912 - accuracy: 0.5725 - val_loss: 0.6876 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6931 - accuracy: 0.5350 - val_loss: 0.6900 - val_accuracy: 0.5852\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6795 - accuracy: 0.6119 - val_loss: 0.6856 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6917 - accuracy: 0.5325 - val_loss: 0.6834 - val_accuracy: 0.5588\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6754 - accuracy: 0.6006 - val_loss: 0.6894 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6706 - accuracy: 0.6137 - val_loss: 0.6830 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6790 - accuracy: 0.5806 - val_loss: 0.6763 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6803 - accuracy: 0.5756 - val_loss: 0.6764 - val_accuracy: 0.5617\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6862 - accuracy: 0.5406 - val_loss: 0.6756 - val_accuracy: 0.5794\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6783 - accuracy: 0.5800 - val_loss: 0.6738 - val_accuracy: 0.5794\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6653 - accuracy: 0.6069 - val_loss: 0.6724 - val_accuracy: 0.5744\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6836 - accuracy: 0.5350 - val_loss: 0.6734 - val_accuracy: 0.5847\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6844 - accuracy: 0.5544 - val_loss: 0.6732 - val_accuracy: 0.5870\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6797 - accuracy: 0.5462 - val_loss: 0.6709 - val_accuracy: 0.5622\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6709 - accuracy: 0.5925 - val_loss: 0.6706 - val_accuracy: 0.5614\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6738 - accuracy: 0.5738 - val_loss: 0.6705 - val_accuracy: 0.5759\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6678 - accuracy: 0.6031 - val_loss: 0.6695 - val_accuracy: 0.5604\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6665 - accuracy: 0.5794 - val_loss: 0.6687 - val_accuracy: 0.5641\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6704 - accuracy: 0.5725 - val_loss: 0.6686 - val_accuracy: 0.5585\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6708 - accuracy: 0.5769 - val_loss: 0.6699 - val_accuracy: 0.5770\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6647 - accuracy: 0.6031 - val_loss: 0.6699 - val_accuracy: 0.5759\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6625 - accuracy: 0.5944 - val_loss: 0.6996 - val_accuracy: 0.5794\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6920 - accuracy: 0.5877\n",
      "training acc:  0.5943750143051147 , training loss:  0.6624752283096313 , val acc:  0.5793776512145996 , val loss:  0.6995978355407715 , test acc:  0.5876677632331848 , test loss:  0.6919728517532349\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt2/450word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each people = different most used might much something now question 've he such am after well case its 're set good however number 2 been find try new problem say being since really many take both still possible data did answer those right another code who too between why were while over go had point change without value probably things page + better before image let ca sure create add back url$ enough actually 'd look 'll through system our his 3 function able down file every going long following given    own seems second off thing part though help around either best lot give etc always x form high note > means power above order less check list edit found must few bit solution having based start made trying % done end option doing least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct once day again similar looking light rather issue non us information person someone specific output reason single mean result space she name de times said simple open quite simply never level show process put years place real often usually input b understand site error below far last c world come group large under online working object fact que side course hard \\to makes method values layer important until him whether 5 next el original idea pretty got water called state amount current her color test tried y getting 10 assume    free kind common    bytes left wo else words call character general sense seem block difference past consider post top matter results created available source full itself old term 4 great running life range control class address particular god numbers camera three paper human within i.e. e.g. account select access due short # needs area away energy google comes book search changes write low anyone everything provide exactly yes ai easy experience making < thought wrong fine multiple product language hand standard n thanks several game perhaps ask tell design ' year nothing lower although\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6901 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0147s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6770 - accuracy: 0.6206 - val_loss: 0.6909 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6716 - accuracy: 0.6256 - val_loss: 0.6894 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6876 - accuracy: 0.5694 - val_loss: 0.6818 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6853 - accuracy: 0.5706 - val_loss: 0.6802 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6834 - accuracy: 0.5788 - val_loss: 0.6778 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6856 - accuracy: 0.5650 - val_loss: 0.6773 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6642 - accuracy: 0.6250 - val_loss: 0.6865 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6594 - accuracy: 0.6325 - val_loss: 0.6834 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6734 - accuracy: 0.5938 - val_loss: 0.6740 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6890 - accuracy: 0.5331 - val_loss: 0.6792 - val_accuracy: 0.5839\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6742 - accuracy: 0.5906 - val_loss: 0.6746 - val_accuracy: 0.5794\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6790 - accuracy: 0.5644 - val_loss: 0.6719 - val_accuracy: 0.5773\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6802 - accuracy: 0.5612 - val_loss: 0.6719 - val_accuracy: 0.5778\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6728 - accuracy: 0.5738 - val_loss: 0.6928 - val_accuracy: 0.5794\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6932 - accuracy: 0.5312 - val_loss: 0.6710 - val_accuracy: 0.5738\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6815 - accuracy: 0.5475 - val_loss: 0.6707 - val_accuracy: 0.5738\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6730 - accuracy: 0.5750 - val_loss: 0.6696 - val_accuracy: 0.5672\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6734 - accuracy: 0.5663 - val_loss: 0.6691 - val_accuracy: 0.5570\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6728 - accuracy: 0.5612 - val_loss: 0.6700 - val_accuracy: 0.5905\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6684 - accuracy: 0.5731 - val_loss: 0.6698 - val_accuracy: 0.5767\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6727 - accuracy: 0.5775 - val_loss: 0.6755 - val_accuracy: 0.5833\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.7470 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0135s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6757 - accuracy: 0.5915\n",
      "training acc:  0.5774999856948853 , training loss:  0.6726965308189392 , val acc:  0.5833333134651184 , val loss:  0.675456166267395 , test acc:  0.5915478467941284 , test loss:  0.6756576299667358\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt2/500word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in you that it $ for   this be with as on are have if not can or but your 's n't would an do so from there will at by they   one my which what all more we some when then like was use / has just any also about get no how only could other time out does them than up need should way using same make want 'm their where work - because here see first two very into think even know these me 1 example may each people = different most used might much something now question 've he such am after well case its 're set good however number 2 been find try new problem say being since really many take both still possible data did answer those right another code who too between why were while over go had point change without value probably things page + better before image let ca sure create add back url$ enough actually 'd look 'll through system our his 3 function able down file every going long following given    own seems second off thing part though help around either best lot give etc always x form high note > means power above order less check list edit found must few bit solution having based start made trying % done end option doing least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct once day again similar looking light rather issue non us information person someone specific output reason single mean result space she name de times said simple open quite simply never level show process put years place real often usually input b understand site error below far last c world come group large under online working object fact que side course hard \\to makes method values layer important until him whether 5 next el original idea pretty got water called state amount current her color test tried y getting 10 assume    free kind common    bytes left wo else words call character general sense seem block difference past consider post top matter results created available source full itself old term 4 great running life range control class address particular god numbers camera three paper human within i.e. e.g. account select access due short # needs area away energy google comes book search changes write low anyone everything provide exactly yes ai easy experience making < thought wrong fine multiple product language hand standard n thanks several game perhaps ask tell design ' year nothing lower although uses feel thus points update takes looks position useful added cases unless d true clear return lens _ content higher effect price believe en further almost view la model certain days string length files click context sentence others big against s normal gives step options turn english sort map terms\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.8573 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_train_batch_end` time: 0.0122s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.9878 - accuracy: 0.5625 - val_loss: 0.7088 - val_accuracy: 0.4945\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6927 - accuracy: 0.5500 - val_loss: 0.7117 - val_accuracy: 0.5396\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 0.6959 - accuracy: 0.5644 - val_loss: 0.7043 - val_accuracy: 0.5451\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.7005 - accuracy: 0.5288 - val_loss: 0.6966 - val_accuracy: 0.5264\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6950 - accuracy: 0.5519 - val_loss: 0.6933 - val_accuracy: 0.5393\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6984 - accuracy: 0.4950 - val_loss: 0.6908 - val_accuracy: 0.5654\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6862 - accuracy: 0.6150 - val_loss: 0.6899 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6810 - accuracy: 0.6044 - val_loss: 0.6849 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 8s 84ms/step - loss: 0.6899 - accuracy: 0.5419 - val_loss: 0.6833 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6899 - accuracy: 0.5444 - val_loss: 0.6805 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6700 - accuracy: 0.6294 - val_loss: 0.6803 - val_accuracy: 0.5794\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6670 - accuracy: 0.6237 - val_loss: 0.6904 - val_accuracy: 0.5794\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6941 - accuracy: 0.5469 - val_loss: 0.6785 - val_accuracy: 0.5794\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6750 - accuracy: 0.6094 - val_loss: 0.6753 - val_accuracy: 0.5794\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 0.6828 - accuracy: 0.5700 - val_loss: 0.6742 - val_accuracy: 0.5775\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6800 - accuracy: 0.5888 - val_loss: 0.6757 - val_accuracy: 0.5794\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6741 - accuracy: 0.5900 - val_loss: 0.6759 - val_accuracy: 0.5794\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6727 - accuracy: 0.5913 - val_loss: 0.6730 - val_accuracy: 0.5794\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6769 - accuracy: 0.5713 - val_loss: 0.6726 - val_accuracy: 0.5794\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6756 - accuracy: 0.5800 - val_loss: 0.6720 - val_accuracy: 0.5881\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6670 - accuracy: 0.5888 - val_loss: 0.6757 - val_accuracy: 0.5794\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6771 - accuracy: 0.5713 - val_loss: 0.6776 - val_accuracy: 0.5794\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6733 - accuracy: 0.5813 - val_loss: 0.6711 - val_accuracy: 0.5878\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6742 - accuracy: 0.5806 - val_loss: 0.6780 - val_accuracy: 0.5794\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6668 - accuracy: 0.5962 - val_loss: 0.6686 - val_accuracy: 0.5578\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6775 - accuracy: 0.5581 - val_loss: 0.6681 - val_accuracy: 0.5749\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6769 - accuracy: 0.5644 - val_loss: 0.6704 - val_accuracy: 0.5791\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.6791 - accuracy: 0.5650 - val_loss: 0.6686 - val_accuracy: 0.5636\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6799 - accuracy: 0.5581 - val_loss: 0.6679 - val_accuracy: 0.5575\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6834 - accuracy: 0.5331 - val_loss: 0.6675 - val_accuracy: 0.5665\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.6768 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0142s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6649 - accuracy: 0.5780\n",
      "training acc:  0.5331249833106995 , training loss:  0.6834065318107605 , val acc:  0.5664557218551636 , val loss:  0.667526125907898 , test acc:  0.5780201554298401 , test loss:  0.6649134159088135\n",
      "\n",
      "opt3\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt3/50word_list.txt\n",
      "WORD INDEX STR:  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6801 - accuracy: 0.5913 - val_loss: 0.6907 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6901 - accuracy: 0.5838 - val_loss: 0.6825 - val_accuracy: 0.6218\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6913 - accuracy: 0.5956 - val_loss: 0.6870 - val_accuracy: 0.6121\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6806 - accuracy: 0.5856 - val_loss: 0.6776 - val_accuracy: 0.6123\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6795 - accuracy: 0.5844 - val_loss: 0.6764 - val_accuracy: 0.6187\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6779 - accuracy: 0.6350 - val_loss: 0.6750 - val_accuracy: 0.6063\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6707 - accuracy: 0.5981 - val_loss: 0.6734 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6761 - accuracy: 0.5850 - val_loss: 0.6726 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6631 - accuracy: 0.6087 - val_loss: 0.6732 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6737 - accuracy: 0.5700 - val_loss: 0.6706 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6802 - accuracy: 0.5562 - val_loss: 0.6705 - val_accuracy: 0.5794\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6624 - accuracy: 0.5894 - val_loss: 0.6738 - val_accuracy: 0.5794\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6726 - accuracy: 0.5688 - val_loss: 0.6692 - val_accuracy: 0.5886\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6804 - accuracy: 0.5738 - val_loss: 0.6693 - val_accuracy: 0.5941\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6619 - accuracy: 0.6062 - val_loss: 0.6685 - val_accuracy: 0.5912\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6748 - accuracy: 0.5725 - val_loss: 0.6690 - val_accuracy: 0.5902\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6802 - accuracy: 0.5456 - val_loss: 0.6781 - val_accuracy: 0.5717\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6691 - accuracy: 0.5975 - val_loss: 0.6676 - val_accuracy: 0.5794\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6711 - accuracy: 0.5706 - val_loss: 0.6687 - val_accuracy: 0.5883\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6612 - accuracy: 0.5994 - val_loss: 0.6678 - val_accuracy: 0.5794\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6610 - accuracy: 0.5950 - val_loss: 0.6670 - val_accuracy: 0.5794\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6786 - accuracy: 0.5469 - val_loss: 0.6706 - val_accuracy: 0.5915\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6718 - accuracy: 0.5813 - val_loss: 0.6664 - val_accuracy: 0.5823\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6665 - accuracy: 0.5987 - val_loss: 0.6666 - val_accuracy: 0.5807\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6714 - accuracy: 0.5806 - val_loss: 0.6662 - val_accuracy: 0.5878\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6667 - accuracy: 0.5962 - val_loss: 0.6663 - val_accuracy: 0.5847\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6680 - accuracy: 0.5944 - val_loss: 0.6689 - val_accuracy: 0.5883\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6680 - accuracy: 0.5800 - val_loss: 0.6654 - val_accuracy: 0.5833\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6777 - accuracy: 0.5738 - val_loss: 0.6659 - val_accuracy: 0.5857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6617 - accuracy: 0.6019 - val_loss: 0.6650 - val_accuracy: 0.5849\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.6657 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_test_batch_end` time: 0.0056s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6621 - accuracy: 0.5940\n",
      "training acc:  0.6018750071525574 , training loss:  0.6616564989089966 , val acc:  0.5849156379699707 , val loss:  0.6649806499481201 , test acc:  0.5939597487449646 , test loss:  0.6621388792991638\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt3/100word_list.txt\n",
      "WORD INDEX STR:  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.4032 - accuracy: 1.0000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0117s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6852 - accuracy: 0.5706 - val_loss: 0.6830 - val_accuracy: 0.5949\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6803 - accuracy: 0.6056 - val_loss: 0.6811 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6907 - accuracy: 0.5462 - val_loss: 0.6836 - val_accuracy: 0.6073\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6883 - accuracy: 0.5700 - val_loss: 0.6824 - val_accuracy: 0.6050\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6842 - accuracy: 0.5675 - val_loss: 0.6820 - val_accuracy: 0.6028\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6810 - accuracy: 0.6040\n",
      "training acc:  0.5674999952316284 , training loss:  0.6841825246810913 , val acc:  0.6028481125831604 , val loss:  0.6820335984230042 , test acc:  0.6040268540382385 , test loss:  0.6809528470039368\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt3/150word_list.txt\n",
      "WORD INDEX STR:  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems second thing part though help around either best lot give etc always x form high note > means power order less check list edit found must bit solution based start made trying % done end option least likely run field & key size \\mathcal word server keep small type 0\n",
      "Epoch 1/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6901 - accuracy: 0.5726WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0067s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6902 - accuracy: 0.5719 - val_loss: 0.6852 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6846 - accuracy: 0.5825 - val_loss: 0.6812 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6887 - accuracy: 0.5631 - val_loss: 0.6807 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6688 - accuracy: 0.6187 - val_loss: 0.6847 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6683 - accuracy: 0.6187 - val_loss: 0.6821 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6861 - accuracy: 0.5725 - val_loss: 0.6810 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 0s - loss: 0.6399 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0136s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6788 - accuracy: 0.5877\n",
      "training acc:  0.5724999904632568 , training loss:  0.6861496567726135 , val acc:  0.5793776512145996 , val loss:  0.6809569001197815 , test acc:  0.5876677632331848 , test loss:  0.6788496971130371\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt3/200word_list.txt\n",
      "WORD INDEX STR:  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems second thing part though help around either best lot give etc always x form high note > means power order less check list edit found must bit solution based start made trying % done end option least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct day similar looking light rather issue non us information person someone specific output reason single mean result space name de times said simple open quite simply never level show process put years place real often usually input\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.7205 - accuracy: 0.5181 - val_loss: 0.6836 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6758 - accuracy: 0.6106 - val_loss: 0.6813 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6817 - accuracy: 0.5800 - val_loss: 0.6802 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6852 - accuracy: 0.5663 - val_loss: 0.6796 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6826 - accuracy: 0.5831 - val_loss: 0.6803 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6848 - accuracy: 0.5681 - val_loss: 0.6797 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6864 - accuracy: 0.5600 - val_loss: 0.6797 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.6407 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0129s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6773 - accuracy: 0.5877\n",
      "training acc:  0.5600000023841858 , training loss:  0.6864256858825684 , val acc:  0.5793776512145996 , val loss:  0.6797479391098022 , test acc:  0.5876677632331848 , test loss:  0.6773392558097839\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt3/250word_list.txt\n",
      "WORD INDEX STR:  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems second thing part though help around either best lot give etc always x form high note > means power order less check list edit found must bit solution based start made trying % done end option least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct day similar looking light rather issue non us information person someone specific output reason single mean result space name de times said simple open quite simply never level show process put years place real often usually input b understand site error far last c world come group large online working object fact que side course hard \\to makes method values layer important whether 5 next el original idea pretty got water called state amount current color test tried getting 10 assume    free kind common    bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6925 - accuracy: 0.4994WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0027s vs `on_test_batch_end` time: 0.0139s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6925 - accuracy: 0.4994 - val_loss: 0.6824 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6733 - accuracy: 0.6225 - val_loss: 0.6811 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6883 - accuracy: 0.5694 - val_loss: 0.6804 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6785 - accuracy: 0.5831 - val_loss: 0.6805 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6656 - accuracy: 0.6206 - val_loss: 0.6821 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6817 - accuracy: 0.5700 - val_loss: 0.6803 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6795 - accuracy: 0.6000 - val_loss: 0.6803 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6856 - accuracy: 0.5756 - val_loss: 0.6805 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6799 - accuracy: 0.5744 - val_loss: 0.6797 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6865 - accuracy: 0.5631 - val_loss: 0.6815 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6817 - accuracy: 0.5850 - val_loss: 0.6796 - val_accuracy: 0.5794\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6819 - accuracy: 0.5744 - val_loss: 0.6796 - val_accuracy: 0.5794\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6687 - accuracy: 0.6050 - val_loss: 0.6811 - val_accuracy: 0.5794\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6866 - accuracy: 0.5581 - val_loss: 0.6798 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 0s - loss: 0.6408 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0140s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6773 - accuracy: 0.5877\n",
      "training acc:  0.5581250190734863 , training loss:  0.6866413950920105 , val acc:  0.5793776512145996 , val loss:  0.6798445582389832 , test acc:  0.5876677632331848 , test loss:  0.6772600412368774\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt3/300word_list.txt\n",
      "WORD INDEX STR:  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems second thing part though help around either best lot give etc always x form high note > means power order less check list edit found must bit solution based start made trying % done end option least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct day similar looking light rather issue non us information person someone specific output reason single mean result space name de times said simple open quite simply never level show process put years place real often usually input b understand site error far last c world come group large online working object fact que side course hard \\to makes method values layer important whether 5 next el original idea pretty got water called state amount current color test tried getting 10 assume    free kind common    bytes left wo else words call character general sense seem block difference past consider post top matter results created available source full old term 4 great running life range control class address particular god numbers camera three paper human within i.e. e.g. account select access due short # needs area away\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6795 - accuracy: 0.6119WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0128s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6795 - accuracy: 0.6119 - val_loss: 0.6821 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6893 - accuracy: 0.5569 - val_loss: 0.6828 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6927 - accuracy: 0.5063 - val_loss: 0.6809 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6851 - accuracy: 0.5650 - val_loss: 0.6810 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6851 - accuracy: 0.5875 - val_loss: 0.6812 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6778 - accuracy: 0.5975 - val_loss: 0.6805 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6814 - accuracy: 0.5888 - val_loss: 0.6799 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6629 - accuracy: 0.6237 - val_loss: 0.6806 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6902 - accuracy: 0.5456 - val_loss: 0.6825 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6761 - accuracy: 0.6012 - val_loss: 0.6806 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6182 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0128s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6770 - accuracy: 0.5877\n",
      "training acc:  0.6012499928474426 , training loss:  0.6761119961738586 , val acc:  0.5793776512145996 , val loss:  0.680622935295105 , test acc:  0.5876677632331848 , test loss:  0.6769819855690002\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt3/350word_list.txt\n",
      "WORD INDEX STR:  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems second thing part though help around either best lot give etc always x form high note > means power order less check list edit found must bit solution based start made trying % done end option least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct day similar looking light rather issue non us information person someone specific output reason single mean result space name de times said simple open quite simply never level show process put years place real often usually input b understand site error far last c world come group large online working object fact que side course hard \\to makes method values layer important whether 5 next el original idea pretty got water called state amount current color test tried getting 10 assume    free kind common    bytes left wo else words call character general sense seem block difference past consider post top matter results created available source full old term 4 great running life range control class address particular god numbers camera three paper human within i.e. e.g. account select access due short # needs area away energy google comes book search changes write low anyone everything provide exactly yes ai easy experience making < thought wrong fine multiple product language hand standard n thanks several game perhaps ask tell design ' year nothing lower although uses feel thus points update takes looks position useful added cases\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/100 [..............................] - ETA: 0s - loss: 0.9565 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0108s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.7917 - accuracy: 0.5025 - val_loss: 0.6925 - val_accuracy: 0.5345\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6807 - accuracy: 0.5881 - val_loss: 0.6844 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6874 - accuracy: 0.5656 - val_loss: 0.6819 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6883 - accuracy: 0.5606 - val_loss: 0.6807 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6844 - accuracy: 0.5769 - val_loss: 0.6804 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6769 - accuracy: 0.5913 - val_loss: 0.6795 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6874 - accuracy: 0.5587 - val_loss: 0.6823 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6786 - accuracy: 0.5875 - val_loss: 0.6787 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6750 - accuracy: 0.5850 - val_loss: 0.6786 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6780 - accuracy: 0.5788 - val_loss: 0.6784 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6873 - accuracy: 0.5506 - val_loss: 0.6795 - val_accuracy: 0.5794\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6804 - accuracy: 0.5775 - val_loss: 0.6783 - val_accuracy: 0.5794\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6845 - accuracy: 0.5694 - val_loss: 0.6783 - val_accuracy: 0.5794\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6713 - accuracy: 0.6075 - val_loss: 0.6781 - val_accuracy: 0.5794\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6840 - accuracy: 0.5519 - val_loss: 0.6779 - val_accuracy: 0.5794\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6668 - accuracy: 0.6112 - val_loss: 0.6789 - val_accuracy: 0.5794\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6679 - accuracy: 0.6150 - val_loss: 0.6788 - val_accuracy: 0.5794\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6894 - accuracy: 0.5294 - val_loss: 0.6819 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6689 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0154s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6804 - accuracy: 0.5877\n",
      "training acc:  0.5293750166893005 , training loss:  0.6894212365150452 , val acc:  0.5793776512145996 , val loss:  0.6818708181381226 , test acc:  0.5876677632331848 , test loss:  0.6804059743881226\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt3/400word_list.txt\n",
      "WORD INDEX STR:  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems second thing part though help around either best lot give etc always x form high note > means power order less check list edit found must bit solution based start made trying % done end option least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct day similar looking light rather issue non us information person someone specific output reason single mean result space name de times said simple open quite simply never level show process put years place real often usually input b understand site error far last c world come group large online working object fact que side course hard \\to makes method values layer important whether 5 next el original idea pretty got water called state amount current color test tried getting 10 assume    free kind common    bytes left wo else words call character general sense seem block difference past consider post top matter results created available source full old term 4 great running life range control class address particular god numbers camera three paper human within i.e. e.g. account select access due short # needs area away energy google comes book search changes write low anyone everything provide exactly yes ai easy experience making < thought wrong fine multiple product language hand standard n thanks several game perhaps ask tell design ' year nothing lower although uses feel thus points update takes looks position useful added cases unless true clear return lens _ content higher effect price believe en almost view la model certain days string length files click context sentence others big normal gives step options turn english sort map terms close actual future seen link avoid allow network per problems easily \\\\ 100 meaning main\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7004 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0182s). Check your callbacks.\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.7118 - accuracy: 0.5113 ETA: 0s - loss: 0.7WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0117s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.7118 - accuracy: 0.5113 - val_loss: 0.6861 - val_accuracy: 0.5071\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6846 - accuracy: 0.5163 - val_loss: 0.6852 - val_accuracy: 0.5032\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6814 - accuracy: 0.5369 - val_loss: 0.6816 - val_accuracy: 0.6057\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6927 - accuracy: 0.5512 - val_loss: 0.6817 - val_accuracy: 0.6044\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6819 - accuracy: 0.5819 - val_loss: 0.6799 - val_accuracy: 0.5989\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6868 - accuracy: 0.5706 - val_loss: 0.6842 - val_accuracy: 0.5947\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6837 - accuracy: 0.5744 - val_loss: 0.6788 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6809 - accuracy: 0.5856 - val_loss: 0.6779 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6812 - accuracy: 0.5625 - val_loss: 0.6807 - val_accuracy: 0.5928\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6842 - accuracy: 0.5550 - val_loss: 0.6770 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6715 - accuracy: 0.5713 - val_loss: 0.6767 - val_accuracy: 0.5833\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6758 - accuracy: 0.5956 - val_loss: 0.6755 - val_accuracy: 0.5794\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6618 - accuracy: 0.6094 - val_loss: 0.6781 - val_accuracy: 0.5794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6620 - accuracy: 0.6087 - val_loss: 0.6754 - val_accuracy: 0.5794\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6769 - accuracy: 0.5906 - val_loss: 0.6746 - val_accuracy: 0.5794\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6820 - accuracy: 0.5594 - val_loss: 0.6742 - val_accuracy: 0.5794\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6470 - accuracy: 0.6612 - val_loss: 0.6776 - val_accuracy: 0.5794\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6687 - accuracy: 0.5975 - val_loss: 0.6736 - val_accuracy: 0.5794\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6699 - accuracy: 0.5919 - val_loss: 0.6733 - val_accuracy: 0.5794\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6602 - accuracy: 0.6112 - val_loss: 0.6740 - val_accuracy: 0.5794\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6642 - accuracy: 0.5894 - val_loss: 0.6728 - val_accuracy: 0.5794\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6794 - accuracy: 0.5569 - val_loss: 0.6731 - val_accuracy: 0.5794\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6765 - accuracy: 0.5831 - val_loss: 0.6729 - val_accuracy: 0.5794\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6710 - accuracy: 0.5931 - val_loss: 0.6740 - val_accuracy: 0.5794\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6720 - accuracy: 0.5877\n",
      "training acc:  0.5931249856948853 , training loss:  0.6709696054458618 , val acc:  0.5793776512145996 , val loss:  0.6739864349365234 , test acc:  0.5876677632331848 , test loss:  0.671986997127533\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt3/450word_list.txt\n",
      "WORD INDEX STR:  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems second thing part though help around either best lot give etc always x form high note > means power order less check list edit found must bit solution based start made trying % done end option least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct day similar looking light rather issue non us information person someone specific output reason single mean result space name de times said simple open quite simply never level show process put years place real often usually input b understand site error far last c world come group large online working object fact que side course hard \\to makes method values layer important whether 5 next el original idea pretty got water called state amount current color test tried getting 10 assume    free kind common    bytes left wo else words call character general sense seem block difference past consider post top matter results created available source full old term 4 great running life range control class address particular god numbers camera three paper human within i.e. e.g. account select access due short # needs area away energy google comes book search changes write low anyone everything provide exactly yes ai easy experience making < thought wrong fine multiple product language hand standard n thanks several game perhaps ask tell design ' year nothing lower although uses feel thus points update takes looks position useful added cases unless true clear return lens _ content higher effect price believe en almost view la model certain days string length files click context sentence others big normal gives step options turn english sort map terms close actual future seen link avoid allow network per problems easily \\\\ 100 meaning main client computer known smaller tool difficult please research save whole generally setting bad mind transaction functions choose sometimes therefore sound ones related instance   characters along play images mode move database cause rate gets support yet table e says background examples web money head reading random inside n$ rules happen\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6020 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0129s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6939 - accuracy: 0.5688 - val_loss: 0.6834 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6834 - accuracy: 0.5775 - val_loss: 0.6812 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6805 - accuracy: 0.5781 - val_loss: 0.6807 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6896 - accuracy: 0.5625 - val_loss: 0.6836 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6837 - accuracy: 0.5781 - val_loss: 0.6803 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6860 - accuracy: 0.5606 - val_loss: 0.6804 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6797 - accuracy: 0.5800 - val_loss: 0.6800 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6818 - accuracy: 0.5769 - val_loss: 0.6799 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6803 - accuracy: 0.5788 - val_loss: 0.6801 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6655 - accuracy: 0.6212 - val_loss: 0.6808 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6795 - accuracy: 0.5700 - val_loss: 0.6800 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6416 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0127s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6771 - accuracy: 0.5877\n",
      "training acc:  0.5699999928474426 , training loss:  0.679523229598999 , val acc:  0.5793776512145996 , val loss:  0.6800028085708618 , test acc:  0.5876677632331848 , test loss:  0.6770873069763184\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt3/500word_list.txt\n",
      "WORD INDEX STR:  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems second thing part though help around either best lot give etc always x form high note > means power order less check list edit found must bit solution based start made trying % done end option least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct day similar looking light rather issue non us information person someone specific output reason single mean result space name de times said simple open quite simply never level show process put years place real often usually input b understand site error far last c world come group large online working object fact que side course hard \\to makes method values layer important whether 5 next el original idea pretty got water called state amount current color test tried getting 10 assume    free kind common    bytes left wo else words call character general sense seem block difference past consider post top matter results created available source full old term 4 great running life range control class address particular god numbers camera three paper human within i.e. e.g. account select access due short # needs area away energy google comes book search changes write low anyone everything provide exactly yes ai easy experience making < thought wrong fine multiple product language hand standard n thanks several game perhaps ask tell design ' year nothing lower although uses feel thus points update takes looks position useful added cases unless true clear return lens _ content higher effect price believe en almost view la model certain days string length files click context sentence others big normal gives step options turn english sort map terms close actual future seen link avoid allow network per problems easily \\\\ 100 meaning main client computer known smaller tool difficult please research save whole generally setting bad mind transaction functions choose sometimes therefore sound ones related instance   characters along play images mode move database cause rate gets support yet table e says background examples web money head reading random inside n$ rules happen taking guess approach mm pages hope via 6 depends explanation photoshop assuming build default become company application air users windows body apply bitcoin later directly writing custom target program rule situation story structure exact parts local stop longer remove algorithm sequence limit service history job larger whatever half theory base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 1s - loss: 0.6936 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0230s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6781 - accuracy: 0.5531 - val_loss: 0.6838 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6803 - accuracy: 0.5856 - val_loss: 0.6812 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6831 - accuracy: 0.5888 - val_loss: 0.6803 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6779 - accuracy: 0.5931 - val_loss: 0.6800 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6759 - accuracy: 0.5919 - val_loss: 0.6802 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6641 - accuracy: 0.6194 - val_loss: 0.6830 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6652 - accuracy: 0.6125 - val_loss: 0.6820 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6169 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0133s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6781 - accuracy: 0.5877\n",
      "training acc:  0.612500011920929 , training loss:  0.6651812791824341 , val acc:  0.5793776512145996 , val loss:  0.6819612979888916 , test acc:  0.5876677632331848 , test loss:  0.6780919432640076\n",
      "\n",
      "opt4\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt4/50word_list.txt\n",
      "WORD INDEX STR: . ,  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.7163 - accuracy: 0.5400 - val_loss: 0.6830 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6915 - accuracy: 0.5487 - val_loss: 0.6825 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6900 - accuracy: 0.5400 - val_loss: 0.6815 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6667 - accuracy: 0.6288 - val_loss: 0.6836 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6888 - accuracy: 0.5456 - val_loss: 0.6845 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6814 - accuracy: 0.5913 - val_loss: 0.6804 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6897 - accuracy: 0.5506 - val_loss: 0.6799 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6874 - accuracy: 0.5713 - val_loss: 0.6806 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6797 - accuracy: 0.5938 - val_loss: 0.6824 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6934 - accuracy: 0.5312 - val_loss: 0.6795 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6890 - accuracy: 0.5512 - val_loss: 0.6816 - val_accuracy: 0.5794\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6851 - accuracy: 0.5831 - val_loss: 0.6814 - val_accuracy: 0.5794\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6887 - accuracy: 0.5487 - val_loss: 0.6806 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.6650 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0141s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6793 - accuracy: 0.5877\n",
      "training acc:  0.5487499833106995 , training loss:  0.6886729598045349 , val acc:  0.5793776512145996 , val loss:  0.6805925369262695 , test acc:  0.5876677632331848 , test loss:  0.6792863011360168\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt4/100word_list.txt\n",
      "WORD INDEX STR: . ,  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.8424 - accuracy: 0.4944 - val_loss: 0.6808 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6827 - accuracy: 0.5788 - val_loss: 0.6809 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6867 - accuracy: 0.5625 - val_loss: 0.6859 - val_accuracy: 0.5443\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6915 - accuracy: 0.5344 - val_loss: 0.6798 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6678 - accuracy: 0.6306 - val_loss: 0.6874 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6887 - accuracy: 0.5606 - val_loss: 0.6801 - val_accuracy: 0.5791\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6892 - accuracy: 0.5288 - val_loss: 0.6786 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6890 - accuracy: 0.5487 - val_loss: 0.6786 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6899 - accuracy: 0.5437 - val_loss: 0.6783 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6779 - accuracy: 0.5919 - val_loss: 0.6783 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6736 - accuracy: 0.5931 - val_loss: 0.6895 - val_accuracy: 0.5794\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6881 - accuracy: 0.5569 - val_loss: 0.6792 - val_accuracy: 0.5530\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6870 - accuracy: 0.5650 - val_loss: 0.6898 - val_accuracy: 0.5382\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.7188 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_test_batch_end` time: 0.0150s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6899 - accuracy: 0.5285\n",
      "training acc:  0.5649999976158142 , training loss:  0.6869877576828003 , val acc:  0.5382384061813354 , val loss:  0.6898196339607239 , test acc:  0.5285235047340393 , test loss:  0.6898917555809021\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt4/150word_list.txt\n",
      "WORD INDEX STR: . ,  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems second thing part though help around either best lot give etc always x form high note > means power order less check list edit found must bit solution based start made trying % done end option least likely run field & key size \\mathcal word server keep small\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6952 - accuracy: 0.5300 - val_loss: 0.6907 - val_accuracy: 0.5406\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6913 - accuracy: 0.5531 - val_loss: 0.6869 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6998 - accuracy: 0.4900 - val_loss: 0.6862 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6909 - accuracy: 0.5437 - val_loss: 0.6841 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6857 - accuracy: 0.5825 - val_loss: 0.6835 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6961 - accuracy: 0.5219 - val_loss: 0.6983 - val_accuracy: 0.4963\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6945 - accuracy: 0.5206 - val_loss: 0.6888 - val_accuracy: 0.5498\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6836 - accuracy: 0.5806 - val_loss: 0.6837 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.6118 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0122s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6806 - accuracy: 0.5877\n",
      "training acc:  0.5806249976158142 , training loss:  0.6835996508598328 , val acc:  0.5793776512145996 , val loss:  0.6837478876113892 , test acc:  0.5876677632331848 , test loss:  0.6805817484855652\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt4/200word_list.txt\n",
      "WORD INDEX STR: . ,  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems second thing part though help around either best lot give etc always x form high note > means power order less check list edit found must bit solution based start made trying % done end option least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct day similar looking light rather issue non us information person someone specific output reason single mean result space name de times said simple open quite simply never level show process put years place real often\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6809 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_train_batch_end` time: 0.0152s). Check your callbacks.\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.5612WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0069s vs `on_test_batch_end` time: 0.0144s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6952 - accuracy: 0.5612 - val_loss: 0.6856 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6931 - accuracy: 0.5431 - val_loss: 0.6845 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6906 - accuracy: 0.5437 - val_loss: 0.6821 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6813 - accuracy: 0.5906 - val_loss: 0.6811 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6799 - accuracy: 0.5956 - val_loss: 0.6801 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6794 - accuracy: 0.5913 - val_loss: 0.7033 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6954 - accuracy: 0.5456 - val_loss: 0.6790 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6764 - accuracy: 0.6000 - val_loss: 0.6783 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6874 - accuracy: 0.5688 - val_loss: 0.6781 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6721 - accuracy: 0.6100 - val_loss: 0.6770 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6927 - accuracy: 0.5312 - val_loss: 0.6788 - val_accuracy: 0.5794\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6826 - accuracy: 0.5850 - val_loss: 0.6775 - val_accuracy: 0.5794\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6663 - accuracy: 0.6187 - val_loss: 0.6916 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.5863 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_test_batch_end` time: 0.0131s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6867 - accuracy: 0.5877\n",
      "training acc:  0.6187499761581421 , training loss:  0.6663429737091064 , val acc:  0.5793776512145996 , val loss:  0.6916385293006897 , test acc:  0.5876677632331848 , test loss:  0.6866658926010132\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt4/250word_list.txt\n",
      "WORD INDEX STR: . ,  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems second thing part though help around either best lot give etc always x form high note > means power order less check list edit found must bit solution based start made trying % done end option least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct day similar looking light rather issue non us information person someone specific output reason single mean result space name de times said simple open quite simply never level show process put years place real often usually input b understand site error far last c world come group large online working object fact que side course hard \\to makes method values layer important whether 5 next el original idea pretty got water called state amount current color test tried getting 10 assume    free kind common\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6854 - accuracy: 0.5800 - val_loss: 0.6897 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6911 - accuracy: 0.5781 - val_loss: 0.6966 - val_accuracy: 0.4581\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6828 - accuracy: 0.5825 - val_loss: 0.6841 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6882 - accuracy: 0.5819 - val_loss: 0.6836 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6788 - accuracy: 0.6037 - val_loss: 0.6818 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6890 - accuracy: 0.5525 - val_loss: 0.6835 - val_accuracy: 0.5794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6838 - accuracy: 0.5819 - val_loss: 0.6833 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6861 - accuracy: 0.5375 - val_loss: 0.6820 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6126 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_test_batch_end` time: 0.0203s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6787 - accuracy: 0.5877\n",
      "training acc:  0.5375000238418579 , training loss:  0.6861129999160767 , val acc:  0.5793776512145996 , val loss:  0.6819950342178345 , test acc:  0.5876677632331848 , test loss:  0.6786601543426514\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt4/300word_list.txt\n",
      "WORD INDEX STR: . ,  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems second thing part though help around either best lot give etc always x form high note > means power order less check list edit found must bit solution based start made trying % done end option least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct day similar looking light rather issue non us information person someone specific output reason single mean result space name de times said simple open quite simply never level show process put years place real often usually input b understand site error far last c world come group large online working object fact que side course hard \\to makes method values layer important whether 5 next el original idea pretty got water called state amount current color test tried getting 10 assume    free kind common    bytes left wo else words call character general sense seem block difference past consider post top matter results created available source full old term 4 great running life range control class address particular god numbers camera three paper human within i.e. e.g. account select access due short # needs\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6588 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0137s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.7013 - accuracy: 0.5337 - val_loss: 0.6906 - val_accuracy: 0.5580\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6897 - accuracy: 0.5387 - val_loss: 0.6838 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6570 - accuracy: 0.6400 - val_loss: 0.7077 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6734 - accuracy: 0.6175 - val_loss: 0.6833 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6941 - accuracy: 0.5362 - val_loss: 0.6802 - val_accuracy: 0.5583\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6808 - accuracy: 0.5688 - val_loss: 0.6781 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6702 - accuracy: 0.6112 - val_loss: 0.6781 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6761 - accuracy: 0.5888 - val_loss: 0.6805 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6865 - accuracy: 0.5325 - val_loss: 0.6765 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6789 - accuracy: 0.5944 - val_loss: 0.6763 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6871 - accuracy: 0.5450 - val_loss: 0.6761 - val_accuracy: 0.5794\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6806 - accuracy: 0.5731 - val_loss: 0.6775 - val_accuracy: 0.5564\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6796 - accuracy: 0.5769 - val_loss: 0.6764 - val_accuracy: 0.5794\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6814 - accuracy: 0.5806 - val_loss: 0.6761 - val_accuracy: 0.5599\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6561 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_test_batch_end` time: 0.0180s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6746 - accuracy: 0.5673\n",
      "training acc:  0.5806249976158142 , training loss:  0.6814298033714294 , val acc:  0.5598628520965576 , val loss:  0.676139235496521 , test acc:  0.5673238039016724 , test loss:  0.6746079325675964\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt4/350word_list.txt\n",
      "WORD INDEX STR: . ,  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems second thing part though help around either best lot give etc always x form high note > means power order less check list edit found must bit solution based start made trying % done end option least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct day similar looking light rather issue non us information person someone specific output reason single mean result space name de times said simple open quite simply never level show process put years place real often usually input b understand site error far last c world come group large online working object fact que side course hard \\to makes method values layer important whether 5 next el original idea pretty got water called state amount current color test tried getting 10 assume    free kind common    bytes left wo else words call character general sense seem block difference past consider post top matter results created available source full old term 4 great running life range control class address particular god numbers camera three paper human within i.e. e.g. account select access due short # needs area away energy google comes book search changes write low anyone everything provide exactly yes ai easy experience making < thought wrong fine multiple product language hand standard n thanks several game perhaps ask tell design ' year nothing lower although uses feel thus points update takes looks position useful\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.5969 - accuracy: 0.9375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0079s vs `on_train_batch_end` time: 0.0185s). Check your callbacks.\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6970 - accuracy: 0.5398WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0062s vs `on_test_batch_end` time: 0.0118s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6970 - accuracy: 0.5394 - val_loss: 0.6899 - val_accuracy: 0.5794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6827 - accuracy: 0.6094 - val_loss: 0.6853 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6875 - accuracy: 0.5675 - val_loss: 0.6850 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6907 - accuracy: 0.5231 - val_loss: 0.6834 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6768 - accuracy: 0.6081 - val_loss: 0.6848 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6813 - accuracy: 0.5956 - val_loss: 0.6827 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6831 - accuracy: 0.5663 - val_loss: 0.6806 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6818 - accuracy: 0.5675 - val_loss: 0.6787 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6832 - accuracy: 0.5738 - val_loss: 0.6792 - val_accuracy: 0.5759\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6684 - accuracy: 0.6162 - val_loss: 0.6832 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6775 - accuracy: 0.5888 - val_loss: 0.6801 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6146 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0129s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6768 - accuracy: 0.5877\n",
      "training acc:  0.5887500047683716 , training loss:  0.677542507648468 , val acc:  0.5793776512145996 , val loss:  0.6801103949546814 , test acc:  0.5876677632331848 , test loss:  0.6768086552619934\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt4/400word_list.txt\n",
      "WORD INDEX STR: . ,  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems second thing part though help around either best lot give etc always x form high note > means power order less check list edit found must bit solution based start made trying % done end option least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct day similar looking light rather issue non us information person someone specific output reason single mean result space name de times said simple open quite simply never level show process put years place real often usually input b understand site error far last c world come group large online working object fact que side course hard \\to makes method values layer important whether 5 next el original idea pretty got water called state amount current color test tried getting 10 assume    free kind common    bytes left wo else words call character general sense seem block difference past consider post top matter results created available source full old term 4 great running life range control class address particular god numbers camera three paper human within i.e. e.g. account select access due short # needs area away energy google comes book search changes write low anyone everything provide exactly yes ai easy experience making < thought wrong fine multiple product language hand standard n thanks several game perhaps ask tell design ' year nothing lower although uses feel thus points update takes looks position useful added cases unless true clear return lens _ content higher effect price believe en almost view la model certain days string length files click context sentence others big normal gives step options turn english sort map terms close actual future seen link avoid allow network per problems easily \\\\ 100\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6958 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0163s). Check your callbacks.\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5720WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_test_batch_end` time: 0.0146s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6908 - accuracy: 0.5763 - val_loss: 0.6863 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6922 - accuracy: 0.5362 - val_loss: 0.6842 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6826 - accuracy: 0.5994 - val_loss: 0.6824 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6847 - accuracy: 0.5831 - val_loss: 0.6819 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6878 - accuracy: 0.5381 - val_loss: 0.6871 - val_accuracy: 0.5546\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6873 - accuracy: 0.5525 - val_loss: 0.6852 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6645 - accuracy: 0.6263 - val_loss: 0.6814 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6726 - accuracy: 0.6137 - val_loss: 0.6858 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6743 - accuracy: 0.6025 - val_loss: 0.6775 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6729 - accuracy: 0.6019 - val_loss: 0.6817 - val_accuracy: 0.5794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6814 - accuracy: 0.5756 - val_loss: 0.6772 - val_accuracy: 0.5794\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6699 - accuracy: 0.5962 - val_loss: 0.6778 - val_accuracy: 0.5601\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6660 - accuracy: 0.6137 - val_loss: 0.6786 - val_accuracy: 0.5794\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6743 - accuracy: 0.5831 - val_loss: 0.6755 - val_accuracy: 0.5794\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6844 - accuracy: 0.5650 - val_loss: 0.6753 - val_accuracy: 0.5789\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6832 - accuracy: 0.5481 - val_loss: 0.6760 - val_accuracy: 0.5794\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6787 - accuracy: 0.5788 - val_loss: 0.6751 - val_accuracy: 0.5775\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6739 - accuracy: 0.5819 - val_loss: 0.6753 - val_accuracy: 0.5578\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6833 - accuracy: 0.5556 - val_loss: 0.6791 - val_accuracy: 0.5591\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6766 - accuracy: 0.5612 - val_loss: 0.6747 - val_accuracy: 0.5662\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6851 - accuracy: 0.5394 - val_loss: 0.6788 - val_accuracy: 0.5578\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6767 - accuracy: 0.5631 - val_loss: 0.6755 - val_accuracy: 0.5794\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6748 - accuracy: 0.5700 - val_loss: 0.6750 - val_accuracy: 0.5583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/596 [..............................] - ETA: 3s - loss: 0.6577 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0145s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6736 - accuracy: 0.5642\n",
      "training acc:  0.5699999928474426 , training loss:  0.6748363971710205 , val acc:  0.5582805871963501 , val loss:  0.6750228404998779 , test acc:  0.5641778707504272 , test loss:  0.6735616326332092\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt4/450word_list.txt\n",
      "WORD INDEX STR: . ,  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems second thing part though help around either best lot give etc always x form high note > means power order less check list edit found must bit solution based start made trying % done end option least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct day similar looking light rather issue non us information person someone specific output reason single mean result space name de times said simple open quite simply never level show process put years place real often usually input b understand site error far last c world come group large online working object fact que side course hard \\to makes method values layer important whether 5 next el original idea pretty got water called state amount current color test tried getting 10 assume    free kind common    bytes left wo else words call character general sense seem block difference past consider post top matter results created available source full old term 4 great running life range control class address particular god numbers camera three paper human within i.e. e.g. account select access due short # needs area away energy google comes book search changes write low anyone everything provide exactly yes ai easy experience making < thought wrong fine multiple product language hand standard n thanks several game perhaps ask tell design ' year nothing lower although uses feel thus points update takes looks position useful added cases unless true clear return lens _ content higher effect price believe en almost view la model certain days string length files click context sentence others big normal gives step options turn english sort map terms close actual future seen link avoid allow network per problems easily \\\\ 100 meaning main client computer known smaller tool difficult please research save whole generally setting bad mind transaction functions choose sometimes therefore sound ones related instance   characters along play images mode move database cause rate gets support yet table e says background examples web money head reading random inside n$\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6935 - accuracy: 0.5425 - val_loss: 0.6857 - val_accuracy: 0.5794\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6883 - accuracy: 0.5644 - val_loss: 0.6842 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6879 - accuracy: 0.5675 - val_loss: 0.6830 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6884 - accuracy: 0.5606 - val_loss: 0.6820 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6841 - accuracy: 0.5775 - val_loss: 0.6813 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6821 - accuracy: 0.5781 - val_loss: 0.6857 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6827 - accuracy: 0.5806 - val_loss: 0.6798 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6800 - accuracy: 0.5856 - val_loss: 0.6797 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6707 - accuracy: 0.6137 - val_loss: 0.6808 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6791 - accuracy: 0.5806 - val_loss: 0.6784 - val_accuracy: 0.5789\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6825 - accuracy: 0.5781 - val_loss: 0.6778 - val_accuracy: 0.5794\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6758 - accuracy: 0.6000 - val_loss: 0.6790 - val_accuracy: 0.5794\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6806 - accuracy: 0.5781 - val_loss: 0.6769 - val_accuracy: 0.5794\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6776 - accuracy: 0.5719 - val_loss: 0.6772 - val_accuracy: 0.5794\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6901 - accuracy: 0.5263 - val_loss: 0.6765 - val_accuracy: 0.5794\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6774 - accuracy: 0.5781 - val_loss: 0.6762 - val_accuracy: 0.5789\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6748 - accuracy: 0.5863 - val_loss: 0.6759 - val_accuracy: 0.5794\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6828 - accuracy: 0.5469 - val_loss: 0.6765 - val_accuracy: 0.5580\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6706 - accuracy: 0.6081 - val_loss: 0.6757 - val_accuracy: 0.5794\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6837 - accuracy: 0.5306 - val_loss: 0.6757 - val_accuracy: 0.5794\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6785 - accuracy: 0.5838 - val_loss: 0.6756 - val_accuracy: 0.5794\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6881 - accuracy: 0.5356 - val_loss: 0.6759 - val_accuracy: 0.5567\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6689 - accuracy: 0.5981 - val_loss: 0.6770 - val_accuracy: 0.5794\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.6784 - accuracy: 0.5612 - val_loss: 0.6762 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.6254 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_test_batch_end` time: 0.0137s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6734 - accuracy: 0.5877\n",
      "training acc:  0.5612499713897705 , training loss:  0.6783690452575684 , val acc:  0.5793776512145996 , val loss:  0.676167368888855 , test acc:  0.5876677632331848 , test loss:  0.6733983159065247\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_0/opt4/500word_list.txt\n",
      "WORD INDEX STR: . ,  $   's n't would   one like use / also get could time need way using make want 'm work - see first two think even know 1 example may people = different used might much something question 've well case 're set good however number 2 find try new problem say since really many take still possible data answer right another code go point change without value probably things page + better image let ca sure create add back url$ enough actually 'd look 'll system 3 function able file every going long following given    seems second thing part though help around either best lot give etc always x form high note > means power order less check list edit found must bit solution based start made trying % done end option least likely run field & key size \\mathcal word server keep small type 0 already text user read anything works    line maybe instead little version correct day similar looking light rather issue non us information person someone specific output reason single mean result space name de times said simple open quite simply never level show process put years place real often usually input b understand site error far last c world come group large online working object fact que side course hard \\to makes method values layer important whether 5 next el original idea pretty got water called state amount current color test tried getting 10 assume    free kind common    bytes left wo else words call character general sense seem block difference past consider post top matter results created available source full old term 4 great running life range control class address particular god numbers camera three paper human within i.e. e.g. account select access due short # needs area away energy google comes book search changes write low anyone everything provide exactly yes ai easy experience making < thought wrong fine multiple product language hand standard n thanks several game perhaps ask tell design ' year nothing lower although uses feel thus points update takes looks position useful added cases unless true clear return lens _ content higher effect price believe en almost view la model certain days string length files click context sentence others big normal gives step options turn english sort map terms close actual future seen link avoid allow network per problems easily \\\\ 100 meaning main client computer known smaller tool difficult please research save whole generally setting bad mind transaction functions choose sometimes therefore sound ones related instance   characters along play images mode move database cause rate gets support yet table e says background examples web money head reading random inside n$ rules happen taking guess approach mm pages hope via 6 depends explanation photoshop assuming build default become company application air users windows body apply bitcoin later directly writing custom target program rule situation story structure exact parts local stop longer remove algorithm sequence limit service history job larger whatever half\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.4255 - accuracy: 0.9375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0116s). Check your callbacks.\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6743 - accuracy: 0.6111WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0143s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6753 - accuracy: 0.6075 - val_loss: 0.6957 - val_accuracy: 0.5657\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6841 - accuracy: 0.5987 - val_loss: 0.6881 - val_accuracy: 0.5794\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6870 - accuracy: 0.5919 - val_loss: 0.6857 - val_accuracy: 0.5794\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6892 - accuracy: 0.5587 - val_loss: 0.6828 - val_accuracy: 0.5794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6938 - accuracy: 0.5344 - val_loss: 0.6827 - val_accuracy: 0.5794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6827 - accuracy: 0.5581 - val_loss: 0.6824 - val_accuracy: 0.5794\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6831 - accuracy: 0.5831 - val_loss: 0.6810 - val_accuracy: 0.5794\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6831 - accuracy: 0.5744 - val_loss: 0.6786 - val_accuracy: 0.5794\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6606 - accuracy: 0.6025 - val_loss: 0.7085 - val_accuracy: 0.5794\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6949 - accuracy: 0.5581 - val_loss: 0.6770 - val_accuracy: 0.5675\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6940 - accuracy: 0.5306 - val_loss: 0.6833 - val_accuracy: 0.5604\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6763 - accuracy: 0.5819 - val_loss: 0.6767 - val_accuracy: 0.5794\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6689 - accuracy: 0.6106 - val_loss: 0.6844 - val_accuracy: 0.5794\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.6806 - accuracy: 0.5537 - val_loss: 0.6786 - val_accuracy: 0.5662\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.6613 - accuracy: 0.6181 - val_loss: 0.6784 - val_accuracy: 0.5794\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6150 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0151s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6751 - accuracy: 0.5877\n",
      "training acc:  0.6181250214576721 , training loss:  0.6612500548362732 , val acc:  0.5793776512145996 , val loss:  0.6783918738365173 , test acc:  0.5876677632331848 , test loss:  0.675075888633728\n",
      "\n",
      "Started evaluation for: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_200.csv\n",
      "Loading training file from path:  /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_200.csv\n",
      "(13693, 6)\n",
      "Loading testing file from path:  /home/sukanya/PhD/Datasets/PAN SCD/pan19-style-change-detection/processed/test.csv\n",
      "(9538, 5)\n",
      "opt1\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt1/50word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7140 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0166s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6957 - accuracy: 0.4950 - val_loss: 0.6953 - val_accuracy: 0.4821\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6954 - accuracy: 0.4781 - val_loss: 0.6937 - val_accuracy: 0.5183\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6947 - accuracy: 0.5150 - val_loss: 0.6962 - val_accuracy: 0.5179\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6957 - accuracy: 0.4825 - val_loss: 0.6988 - val_accuracy: 0.4821\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6959 - accuracy: 0.5038 - val_loss: 0.6944 - val_accuracy: 0.5183\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7570 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0136s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.7093 - accuracy: 0.4171\n",
      "training acc:  0.5037500262260437 , training loss:  0.695915162563324 , val acc:  0.5182748436927795 , val loss:  0.6943958401679993 , test acc:  0.41705116629600525 , test loss:  0.7092691659927368\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt1/100word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example\n",
      "Epoch 1/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.7037 - accuracy: 0.5044WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_test_batch_end` time: 0.0140s). Check your callbacks.\n",
      "100/100 [==============================] - 9s 86ms/step - loss: 0.7036 - accuracy: 0.5050 - val_loss: 0.7015 - val_accuracy: 0.4781\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6948 - accuracy: 0.4938 - val_loss: 0.7039 - val_accuracy: 0.4799\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6995 - accuracy: 0.5200 - val_loss: 0.7007 - val_accuracy: 0.5179\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.7003 - accuracy: 0.4837 - val_loss: 0.6967 - val_accuracy: 0.4817\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6960 - accuracy: 0.4806 - val_loss: 0.6926 - val_accuracy: 0.5179\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.7001 - accuracy: 0.4787 - val_loss: 0.6923 - val_accuracy: 0.5197\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6992 - accuracy: 0.5075 - val_loss: 0.6924 - val_accuracy: 0.5230\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6961 - accuracy: 0.5181 - val_loss: 0.6927 - val_accuracy: 0.5208\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6977 - accuracy: 0.4869 - val_loss: 0.6930 - val_accuracy: 0.5062\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6951 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_test_batch_end` time: 0.0210s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6910 - accuracy: 0.5900\n",
      "training acc:  0.4868749976158142 , training loss:  0.6976513862609863 , val acc:  0.5062134265899658 , val loss:  0.6930168867111206 , test acc:  0.5899748206138611 , test loss:  0.691031277179718\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt1/150word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example each = most different might much used something he 've such now question am 're well its after good case however set been number find 2 being say new problem since really many take still both try did who possible those another were too answer data right between had why\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 1.3021 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_train_batch_end` time: 0.0105s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.7143 - accuracy: 0.4906 - val_loss: 0.6923 - val_accuracy: 0.5256\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6935 - accuracy: 0.5213 - val_loss: 0.6934 - val_accuracy: 0.4971\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6946 - accuracy: 0.5113 - val_loss: 0.6927 - val_accuracy: 0.5252\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6968 - accuracy: 0.4812 - val_loss: 0.6931 - val_accuracy: 0.5029\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6960 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0155s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6876 - accuracy: 0.5983\n",
      "training acc:  0.48124998807907104 , training loss:  0.6968037486076355 , val acc:  0.5029239654541016 , val loss:  0.6931363940238953 , test acc:  0.5982592105865479 , test loss:  0.6876142621040344\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt1/200word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example each = most different might much used something he 've such now question am 're well its after good case however set been number find 2 being say new problem since really many take still both try did who possible those another were too answer data right between had why point while over go without probably things change value before better + enough let back code actually ca image sure through our his 'll system 'd look page add going long create able down 3 url$ every given function own off thing part seems second around high best though lot\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6485 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6973 - accuracy: 0.4806 - val_loss: 0.6936 - val_accuracy: 0.4708\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6998 - accuracy: 0.4225 - val_loss: 0.6926 - val_accuracy: 0.5179\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6958 - accuracy: 0.4800 - val_loss: 0.6939 - val_accuracy: 0.4817\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6959 - accuracy: 0.5031 - val_loss: 0.6959 - val_accuracy: 0.4821\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6960 - accuracy: 0.4700 - val_loss: 0.6931 - val_accuracy: 0.5179\n",
      "  1/596 [..............................] - ETA: 0s - loss: 0.7404 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0009s vs `on_test_batch_end` time: 0.0067s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7038 - accuracy: 0.4123\n",
      "training acc:  0.4699999988079071 , training loss:  0.6960471868515015 , val acc:  0.5179093480110168 , val loss:  0.6930856704711914 , test acc:  0.4123322069644928 , test loss:  0.7038408517837524\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt1/250word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example each = most different might much used something he 've such now question am 're well its after good case however set been number find 2 being say new problem since really many take still both try did who possible those another were too answer data right between had why point while over go without probably things change value before better + enough let back code actually ca image sure through our his 'll system 'd look page add going long create able down 3 url$ every given function own off thing part seems second around high best though lot file power following etc help either give form x less always means must having    few above made bit note > order based & likely trying done option start % found least doing check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.5302 - accuracy: 0.8750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0122s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6965 - accuracy: 0.5031 - val_loss: 0.6937 - val_accuracy: 0.5227\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6972 - accuracy: 0.4819 - val_loss: 0.6925 - val_accuracy: 0.5274\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6937 - accuracy: 0.5156 - val_loss: 0.6924 - val_accuracy: 0.5303\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6876 - accuracy: 0.5500 - val_loss: 0.7058 - val_accuracy: 0.5183\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.7018 - accuracy: 0.4613 - val_loss: 0.6948 - val_accuracy: 0.4821\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6960 - accuracy: 0.4869 - val_loss: 0.6924 - val_accuracy: 0.5292\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.7249 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0150s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6958 - accuracy: 0.4672\n",
      "training acc:  0.4868749976158142 , training loss:  0.6960334777832031 , val acc:  0.5292397737503052 , val loss:  0.6923741698265076 , test acc:  0.46717700362205505 , test loss:  0.6958281993865967\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt1/300word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example each = most different might much used something he 've such now question am 're well its after good case however set been number find 2 being say new problem since really many take still both try did who possible those another were too answer data right between had why point while over go without probably things change value before better + enough let back code actually ca image sure through our his 'll system 'd look page add going long create able down 3 url$ every given function own off thing part seems second around high best though lot file power following etc help either give form x less always means must having    few above made bit note > order based & likely trying done option start % found least doing check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run again type us user issue once non similar works she information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7228 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0107s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6959 - accuracy: 0.4938 - val_loss: 0.6928 - val_accuracy: 0.5267\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6931 - accuracy: 0.5369 - val_loss: 0.6960 - val_accuracy: 0.5179\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6967 - accuracy: 0.5025 - val_loss: 0.6947 - val_accuracy: 0.4821\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.7001 - accuracy: 0.4863 - val_loss: 0.6935 - val_accuracy: 0.5186\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7486 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0009s vs `on_test_batch_end` time: 0.0067s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7041 - accuracy: 0.4199\n",
      "training acc:  0.48625001311302185 , training loss:  0.7001216411590576 , val acc:  0.5186403393745422 , val loss:  0.6935021281242371 , test acc:  0.41988253593444824 , test loss:  0.7040855884552002\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt1/350word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example each = most different might much used something he 've such now question am 're well its after good case however set been number find 2 being say new problem since really many take still both try did who possible those another were too answer data right between had why point while over go without probably things change value before better + enough let back code actually ca image sure through our his 'll system 'd look page add going long create able down 3 url$ every given function own off thing part seems second around high best though lot file power following etc help either give form x less always means must having    few above made bit note > order based & likely trying done option start % found least doing check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run again type us user issue once non similar works she information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working until him makes under \\to water whether error idea object 5 amount state pretty method below got color her assume called values next original layer current    kind input free getting life sense else\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.7490 - accuracy: 0.5525 - val_loss: 0.6936 - val_accuracy: 0.5296\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.7011 - accuracy: 0.4712 - val_loss: 0.6929 - val_accuracy: 0.5245\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6971 - accuracy: 0.5038 - val_loss: 0.6944 - val_accuracy: 0.4788\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6915 - accuracy: 0.5469 - val_loss: 0.6988 - val_accuracy: 0.4821\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6952 - accuracy: 0.5194 - val_loss: 0.6928 - val_accuracy: 0.5300\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6949 - accuracy: 0.5194 - val_loss: 0.6938 - val_accuracy: 0.5048\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6963 - accuracy: 0.4994 - val_loss: 0.6926 - val_accuracy: 0.5311\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6961 - accuracy: 0.5275 - val_loss: 0.6957 - val_accuracy: 0.5296\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6954 - accuracy: 0.5244 - val_loss: 0.6940 - val_accuracy: 0.4883\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6947 - accuracy: 0.5244 - val_loss: 0.6932 - val_accuracy: 0.5314\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6951 - accuracy: 0.5055\n",
      "training acc:  0.5243750214576721 , training loss:  0.6947076916694641 , val acc:  0.5314327478408813 , val loss:  0.6932129859924316 , test acc:  0.5054529905319214 , test loss:  0.6951307058334351\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt1/400word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example each = most different might much used something he 've such now question am 're well its after good case however set been number find 2 being say new problem since really many take still both try did who possible those another were too answer data right between had why point while over go without probably things change value before better + enough let back code actually ca image sure through our his 'll system 'd look page add going long create able down 3 url$ every given function own off thing part seems second around high best though lot file power following etc help either give form x less always means must having    few above made bit note > order based & likely trying done option start % found least doing check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run again type us user issue once non similar works she information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working until him makes under \\to water whether error idea object 5 amount state pretty method below got color her assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference itself human online 10 test common general seem    character god paper full top block call camera term ai particular source energy short consider due available area control within post range i.e. results words created old class needs de book low away e.g.\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6946 - accuracy: 0.5281 - val_loss: 0.6929 - val_accuracy: 0.5241\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6931 - accuracy: 0.5269 - val_loss: 0.6943 - val_accuracy: 0.4821\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6907 - accuracy: 0.5069 - val_loss: 0.7005 - val_accuracy: 0.5186\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.7007 - accuracy: 0.4863 - val_loss: 0.6925 - val_accuracy: 0.5347\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6947 - accuracy: 0.4800 - val_loss: 0.6923 - val_accuracy: 0.5318\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6952 - accuracy: 0.5144 - val_loss: 0.7003 - val_accuracy: 0.4821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6959 - accuracy: 0.5100 - val_loss: 0.6963 - val_accuracy: 0.4821\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6918 - accuracy: 0.5163 - val_loss: 0.6939 - val_accuracy: 0.5274\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.7570 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0136s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7017 - accuracy: 0.4531\n",
      "training acc:  0.5162500143051147 , training loss:  0.6918249726295471 , val acc:  0.5274122953414917 , val loss:  0.6939191818237305 , test acc:  0.453125 , test loss:  0.701720654964447\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt1/450word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example each = most different might much used something he 've such now question am 're well its after good case however set been number find 2 being say new problem since really many take still both try did who possible those another were too answer data right between had why point while over go without probably things change value before better + enough let back code actually ca image sure through our his 'll system 'd look page add going long create able down 3 url$ every given function own off thing part seems second around high best though lot file power following etc help either give form x less always means must having    few above made bit note > order based & likely trying done option start % found least doing check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run again type us user issue once non similar works she information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working until him makes under \\to water whether error idea object 5 amount state pretty method below got color her assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference itself human online 10 test common general seem    character god paper full top block call camera term ai particular source energy short consider due available area control within post range i.e. results words created old class needs de book low away e.g. three 4 comes running making provide experience address everything < changes easy year thought fine access ask lower account write hand search bytes lens although # perhaps exactly tell numbers google design thus multiple higher several anyone price feel useful ' wrong n yes almost game \\\\ unless nothing position\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.9539 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0130s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.7093 - accuracy: 0.4638 - val_loss: 0.6940 - val_accuracy: 0.4821\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6968 - accuracy: 0.4913 - val_loss: 0.6942 - val_accuracy: 0.4715\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6954 - accuracy: 0.4719 - val_loss: 0.6957 - val_accuracy: 0.4697\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6952 - accuracy: 0.4931 - val_loss: 0.6933 - val_accuracy: 0.4938\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6963 - accuracy: 0.4956 - val_loss: 0.6927 - val_accuracy: 0.5179\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6950 - accuracy: 0.5312 - val_loss: 0.6926 - val_accuracy: 0.5179\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6971 - accuracy: 0.5019 - val_loss: 0.7014 - val_accuracy: 0.4803\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6990 - accuracy: 0.4712 - val_loss: 0.6927 - val_accuracy: 0.5179\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6903 - accuracy: 0.5600 - val_loss: 0.6991 - val_accuracy: 0.5179\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.7892 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0131s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7248 - accuracy: 0.4123\n",
      "training acc:  0.5600000023841858 , training loss:  0.6902519464492798 , val acc:  0.5179093480110168 , val loss:  0.6990768909454346 , test acc:  0.4123322069644928 , test loss:  0.7247788310050964\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt1/500word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example each = most different might much used something he 've such now question am 're well its after good case however set been number find 2 being say new problem since really many take still both try did who possible those another were too answer data right between had why point while over go without probably things change value before better + enough let back code actually ca image sure through our his 'll system 'd look page add going long create able down 3 url$ every given function own off thing part seems second around high best though lot file power following etc help either give form x less always means must having    few above made bit note > order based & likely trying done option start % found least doing check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run again type us user issue once non similar works she information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working until him makes under \\to water whether error idea object 5 amount state pretty method below got color her assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference itself human online 10 test common general seem    character god paper full top block call camera term ai particular source energy short consider due available area control within post range i.e. results words created old class needs de book low away e.g. three 4 comes running making provide experience address everything < changes easy year thought fine access ask lower account write hand search bytes lens although # perhaps exactly tell numbers google design thus multiple higher several anyone price feel useful ' wrong n yes almost game \\\\ unless nothing position further believe effect takes y clear standard product big days true against others select language future close thanks cases return looks certain sort turn points content length actual avoid s model added uses que normal gives smaller research map difficult view terms seen easily computer update options 100 known main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.8038 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0082s vs `on_train_batch_end` time: 0.0230s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.7213 - accuracy: 0.5519 - val_loss: 0.6983 - val_accuracy: 0.5314\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6884 - accuracy: 0.5650 - val_loss: 0.6948 - val_accuracy: 0.5329\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6977 - accuracy: 0.5069 - val_loss: 0.6925 - val_accuracy: 0.5289\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6951 - accuracy: 0.5188 - val_loss: 0.6926 - val_accuracy: 0.5300\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6919 - accuracy: 0.5506 - val_loss: 0.6929 - val_accuracy: 0.5322\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6933 - accuracy: 0.5194 - val_loss: 0.6930 - val_accuracy: 0.5175\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.7048 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0160s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6845 - accuracy: 0.5969\n",
      "training acc:  0.5193750262260437 , training loss:  0.6933108568191528 , val acc:  0.5175438523292542 , val loss:  0.6929939389228821 , test acc:  0.5968959927558899 , test loss:  0.6844952702522278\n",
      "\n",
      "opt2\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt2/50word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we\n",
      "Epoch 1/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6999 - accuracy: 0.4691WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0096s vs `on_test_batch_end` time: 0.0154s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.7002 - accuracy: 0.4663 - val_loss: 0.6965 - val_accuracy: 0.4821\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6974 - accuracy: 0.4825 - val_loss: 0.6956 - val_accuracy: 0.4821\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6940 - accuracy: 0.4969 - val_loss: 0.6999 - val_accuracy: 0.5175\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6954 - accuracy: 0.5344 - val_loss: 0.6964 - val_accuracy: 0.4821\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6896 - accuracy: 0.5594 - val_loss: 0.7280 - val_accuracy: 0.4821\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.5998 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_test_batch_end` time: 0.0106s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6848 - accuracy: 0.5877\n",
      "training acc:  0.559374988079071 , training loss:  0.689555823802948 , val acc:  0.48209065198898315 , val loss:  0.728003203868866 , test acc:  0.5876677632331848 , test loss:  0.6847930550575256\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt2/100word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7011 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.0122s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6959 - accuracy: 0.5113 - val_loss: 0.6974 - val_accuracy: 0.4821\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6970 - accuracy: 0.4938 - val_loss: 0.6926 - val_accuracy: 0.5223\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6960 - accuracy: 0.4794 - val_loss: 0.6926 - val_accuracy: 0.5347\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6948 - accuracy: 0.5213 - val_loss: 0.7022 - val_accuracy: 0.4821\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6990 - accuracy: 0.5163 - val_loss: 0.6928 - val_accuracy: 0.5355\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.6990 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0141s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6934 - accuracy: 0.4797\n",
      "training acc:  0.5162500143051147 , training loss:  0.6989942789077759 , val acc:  0.535453200340271 , val loss:  0.692777156829834 , test acc:  0.47965604066848755 , test loss:  0.69342440366745\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt2/150word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example each = most different might much used something he 've such now question am 're well its after good case however set been number find 2 being say new problem since really many take still both try did who possible those another were too answer data right between\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6199 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_train_batch_end` time: 0.0113s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.7012 - accuracy: 0.4875 - val_loss: 0.7005 - val_accuracy: 0.4821\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6975 - accuracy: 0.5144 - val_loss: 0.6926 - val_accuracy: 0.5190\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6946 - accuracy: 0.4950 - val_loss: 0.6924 - val_accuracy: 0.5183\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6969 - accuracy: 0.4881 - val_loss: 0.7005 - val_accuracy: 0.4821\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6972 - accuracy: 0.4894 - val_loss: 0.6940 - val_accuracy: 0.4821\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6965 - accuracy: 0.4731 - val_loss: 0.6931 - val_accuracy: 0.4993\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.6939 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0139s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6914 - accuracy: 0.5945\n",
      "training acc:  0.47312501072883606 , training loss:  0.6964558959007263 , val acc:  0.4992690086364746 , val loss:  0.6930919289588928 , test acc:  0.5944840312004089 , test loss:  0.6914283037185669\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt2/200word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example each = most different might much used something he 've such now question am 're well its after good case however set been number find 2 being say new problem since really many take still both try did who possible those another were too answer data right between had why point while over go without probably things change value before better + enough let back code actually ca image sure through our his 'll system 'd look page add going long create able down 3 url$ every given function own off thing part seems second around high best\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.9748 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_train_batch_end` time: 0.0105s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.7878 - accuracy: 0.4881 - val_loss: 0.6932 - val_accuracy: 0.5040\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6939 - accuracy: 0.5069 - val_loss: 0.6955 - val_accuracy: 0.4539\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6943 - accuracy: 0.5444 - val_loss: 0.6939 - val_accuracy: 0.5333\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6957 - accuracy: 0.5156 - val_loss: 0.6938 - val_accuracy: 0.4989\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.6953 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0137s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6819 - accuracy: 0.5994\n",
      "training acc:  0.515625 , training loss:  0.6957123279571533 , val acc:  0.4989035129547119 , val loss:  0.6937844157218933 , test acc:  0.599412739276886 , test loss:  0.6819133162498474\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt2/250word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example each = most different might much used something he 've such now question am 're well its after good case however set been number find 2 being say new problem since really many take still both try did who possible those another were too answer data right between had why point while over go without probably things change value before better + enough let back code actually ca image sure through our his 'll system 'd look page add going long create able down 3 url$ every given function own off thing part seems second around high best though lot file power following etc help either give form x less always means must having    few above made bit note > order based & likely trying done option start % found least doing check solution edit end small list \\mathcal keep size anything already key maybe light 0\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.5711 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0192s). Check your callbacks.\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6980 - accuracy: 0.5369WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0112s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6980 - accuracy: 0.5369 - val_loss: 0.6940 - val_accuracy: 0.5194\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6973 - accuracy: 0.4894 - val_loss: 0.6923 - val_accuracy: 0.5369\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6961 - accuracy: 0.4487 - val_loss: 0.6933 - val_accuracy: 0.4931\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6958 - accuracy: 0.5144 - val_loss: 0.6934 - val_accuracy: 0.5212\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6940 - accuracy: 0.5169 - val_loss: 0.6953 - val_accuracy: 0.4821\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.6751 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0141s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6863 - accuracy: 0.5877\n",
      "training acc:  0.5168750286102295 , training loss:  0.6939849257469177 , val acc:  0.48209065198898315 , val loss:  0.695260226726532 , test acc:  0.5876677632331848 , test loss:  0.6862598657608032\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt2/300word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example each = most different might much used something he 've such now question am 're well its after good case however set been number find 2 being say new problem since really many take still both try did who possible those another were too answer data right between had why point while over go without probably things change value before better + enough let back code actually ca image sure through our his 'll system 'd look page add going long create able down 3 url$ every given function own off thing part seems second around high best though lot file power following etc help either give form x less always means must having    few above made bit note > order based & likely trying done option start % found least doing check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run again type us user issue once non similar works she information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.8744 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0150s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6992 - accuracy: 0.5288 - val_loss: 0.6941 - val_accuracy: 0.5230\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6963 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.4828\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6949 - accuracy: 0.4913 - val_loss: 0.6941 - val_accuracy: 0.4550\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6947 - accuracy: 0.5131 - val_loss: 0.6944 - val_accuracy: 0.4510\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6950 - accuracy: 0.5194 - val_loss: 0.6946 - val_accuracy: 0.5289\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.7604 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0172s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7034 - accuracy: 0.4480\n",
      "training acc:  0.5193750262260437 , training loss:  0.6949540972709656 , val acc:  0.5288742780685425 , val loss:  0.694564938545227 , test acc:  0.44798657298088074 , test loss:  0.7033670544624329\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt2/350word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example each = most different might much used something he 've such now question am 're well its after good case however set been number find 2 being say new problem since really many take still both try did who possible those another were too answer data right between had why point while over go without probably things change value before better + enough let back code actually ca image sure through our his 'll system 'd look page add going long create able down 3 url$ every given function own off thing part seems second around high best though lot file power following etc help either give form x less always means must having    few above made bit note > order based & likely trying done option start % found least doing check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run again type us user issue once non similar works she information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working until him makes under \\to water whether error idea object 5 amount state pretty method below got color her assume called values next original layer current    kind input free getting life\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/100 [..............................] - ETA: 0s - loss: 0.6126 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0135s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.7345 - accuracy: 0.5181 - val_loss: 0.6950 - val_accuracy: 0.4510\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6915 - accuracy: 0.5375 - val_loss: 0.6961 - val_accuracy: 0.5340\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6975 - accuracy: 0.5056 - val_loss: 0.6924 - val_accuracy: 0.5311\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6939 - accuracy: 0.5163 - val_loss: 0.6926 - val_accuracy: 0.5376\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.7057 - accuracy: 0.4988 - val_loss: 0.6947 - val_accuracy: 0.4613\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.7016 - accuracy: 0.4338 - val_loss: 0.6936 - val_accuracy: 0.4960\n",
      "  1/596 [..............................] - ETA: 0s - loss: 0.6921 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_test_batch_end` time: 0.0056s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6834 - accuracy: 0.5989\n",
      "training acc:  0.4337500035762787 , training loss:  0.7016038298606873 , val acc:  0.49597951769828796 , val loss:  0.6936277747154236 , test acc:  0.5988883972167969 , test loss:  0.6834208369255066\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt2/400word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example each = most different might much used something he 've such now question am 're well its after good case however set been number find 2 being say new problem since really many take still both try did who possible those another were too answer data right between had why point while over go without probably things change value before better + enough let back code actually ca image sure through our his 'll system 'd look page add going long create able down 3 url$ every given function own off thing part seems second around high best though lot file power following etc help either give form x less always means must having    few above made bit note > order based & likely trying done option start % found least doing check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run again type us user issue once non similar works she information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working until him makes under \\to water whether error idea object 5 amount state pretty method below got color her assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference itself human online 10 test common general seem    character god paper full top block call camera term ai particular source energy short consider due available area control within post range i.e. results words created old class needs de book low\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6942 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0113s vs `on_train_batch_end` time: 0.0247s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6997 - accuracy: 0.5050 - val_loss: 0.6940 - val_accuracy: 0.4821\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6963 - accuracy: 0.4988 - val_loss: 0.6944 - val_accuracy: 0.4821\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6962 - accuracy: 0.4675 - val_loss: 0.6925 - val_accuracy: 0.5384\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6995 - accuracy: 0.4969 - val_loss: 0.6986 - val_accuracy: 0.4821\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6977 - accuracy: 0.5125 - val_loss: 0.6960 - val_accuracy: 0.4821\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6949 - accuracy: 0.5094 - val_loss: 0.7000 - val_accuracy: 0.4821\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6526 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0152s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6833 - accuracy: 0.5877\n",
      "training acc:  0.5093749761581421 , training loss:  0.6948921084403992 , val acc:  0.48209065198898315 , val loss:  0.7000275254249573 , test acc:  0.5876677632331848 , test loss:  0.6833202242851257\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt2/450word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example each = most different might much used something he 've such now question am 're well its after good case however set been number find 2 being say new problem since really many take still both try did who possible those another were too answer data right between had why point while over go without probably things change value before better + enough let back code actually ca image sure through our his 'll system 'd look page add going long create able down 3 url$ every given function own off thing part seems second around high best though lot file power following etc help either give form x less always means must having    few above made bit note > order based & likely trying done option start % found least doing check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run again type us user issue once non similar works she information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working until him makes under \\to water whether error idea object 5 amount state pretty method below got color her assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference itself human online 10 test common general seem    character god paper full top block call camera term ai particular source energy short consider due available area control within post range i.e. results words created old class needs de book low away e.g. three 4 comes running making provide experience address everything < changes easy year thought fine access ask lower account write hand search bytes lens although # perhaps exactly tell numbers google design thus multiple higher several anyone price feel useful ' wrong n yes almost game \\\\ unless\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 8s 77ms/step - loss: 0.7174 - accuracy: 0.5075 - val_loss: 0.6941 - val_accuracy: 0.4631\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6941 - accuracy: 0.4900 - val_loss: 0.6939 - val_accuracy: 0.4737\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6956 - accuracy: 0.5031 - val_loss: 0.6941 - val_accuracy: 0.4656\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6932 - accuracy: 0.5106 - val_loss: 0.6933 - val_accuracy: 0.5029\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6926 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.5355\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6988 - accuracy: 0.4550 - val_loss: 0.6942 - val_accuracy: 0.4627\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6954 - accuracy: 0.5256 - val_loss: 0.6925 - val_accuracy: 0.5307\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6972 - accuracy: 0.5144 - val_loss: 0.6953 - val_accuracy: 0.4795\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6928 - accuracy: 0.5175 - val_loss: 0.6923 - val_accuracy: 0.5358\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6952 - accuracy: 0.5213 - val_loss: 0.6954 - val_accuracy: 0.4817\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6938 - accuracy: 0.5131 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6961 - accuracy: 0.5100 - val_loss: 0.6932 - val_accuracy: 0.5022\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6958 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0139s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6851 - accuracy: 0.6014\n",
      "training acc:  0.5099999904632568 , training loss:  0.6960992217063904 , val acc:  0.5021929740905762 , val loss:  0.6932482123374939 , test acc:  0.6014052033424377 , test loss:  0.6851112842559814\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt2/500word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not can or but your 's would n't do an so from there will at by they one my which what   more all we some when was then like use / has just any also about get how only no could other time out than them does up need should way make using same their want where 'm work because - very see first into even here think these two know me 1 may people example each = most different might much used something he 've such now question am 're well its after good case however set been number find 2 being say new problem since really many take still both try did who possible those another were too answer data right between had why point while over go without probably things change value before better + enough let back code actually ca image sure through our his 'll system 'd look page add going long create able down 3 url$ every given function own off thing part seems second around high best though lot file power following etc help either give form x less always means must having    few above made bit note > order based & likely trying done option start % found least doing check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run again type us user issue once non similar works she information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working until him makes under \\to water whether error idea object 5 amount state pretty method below got color her assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference itself human online 10 test common general seem    character god paper full top block call camera term ai particular source energy short consider due available area control within post range i.e. results words created old class needs de book low away e.g. three 4 comes running making provide experience address everything < changes easy year thought fine access ask lower account write hand search bytes lens although # perhaps exactly tell numbers google design thus multiple higher several anyone price feel useful ' wrong n yes almost game \\\\ unless nothing position further believe effect takes y clear standard product big days true against others select language future close thanks cases return looks certain sort turn points content length actual avoid s model added uses que normal gives smaller research map difficult view terms seen easily computer update options 100\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.7129 - accuracy: 0.5113 - val_loss: 0.6936 - val_accuracy: 0.4642\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6951 - accuracy: 0.5106 - val_loss: 0.6931 - val_accuracy: 0.5256\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6915 - accuracy: 0.5406 - val_loss: 0.7016 - val_accuracy: 0.4821\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6979 - accuracy: 0.4812 - val_loss: 0.6941 - val_accuracy: 0.4814\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6917 - accuracy: 0.5200 - val_loss: 0.6945 - val_accuracy: 0.5197\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7595 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0088s vs `on_test_batch_end` time: 0.0139s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7058 - accuracy: 0.4297\n",
      "training acc:  0.5199999809265137 , training loss:  0.6916855573654175 , val acc:  0.5197368264198303 , val loss:  0.6944851875305176 , test acc:  0.42973992228507996 , test loss:  0.7058393955230713\n",
      "\n",
      "opt3\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt3/50word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 1s - loss: 0.6787 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0146s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6977 - accuracy: 0.4956 - val_loss: 0.6905 - val_accuracy: 0.5461\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6884 - accuracy: 0.5425 - val_loss: 0.6884 - val_accuracy: 0.5197\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6870 - accuracy: 0.5169 - val_loss: 0.6877 - val_accuracy: 0.5197\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6851 - accuracy: 0.5506 - val_loss: 0.6835 - val_accuracy: 0.5563\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6871 - accuracy: 0.5450 - val_loss: 0.6827 - val_accuracy: 0.5552\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6833 - accuracy: 0.5606 - val_loss: 0.6847 - val_accuracy: 0.5406\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6820 - accuracy: 0.5625 - val_loss: 0.6807 - val_accuracy: 0.5607\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6916 - accuracy: 0.5250 - val_loss: 0.6806 - val_accuracy: 0.5570\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6850 - accuracy: 0.5475 - val_loss: 0.6802 - val_accuracy: 0.5574\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6826 - accuracy: 0.5656 - val_loss: 0.6799 - val_accuracy: 0.5581\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6786 - accuracy: 0.5406 - val_loss: 0.6797 - val_accuracy: 0.5512\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6812 - accuracy: 0.5619 - val_loss: 0.6791 - val_accuracy: 0.5577\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6810 - accuracy: 0.5731 - val_loss: 0.6800 - val_accuracy: 0.5442\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6781 - accuracy: 0.5562 - val_loss: 0.6792 - val_accuracy: 0.5592\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6804 - accuracy: 0.5719 - val_loss: 0.6785 - val_accuracy: 0.5548\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6782 - accuracy: 0.5550 - val_loss: 0.6787 - val_accuracy: 0.5512\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6856 - accuracy: 0.5394 - val_loss: 0.6781 - val_accuracy: 0.5537\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6752 - accuracy: 0.5550 - val_loss: 0.6777 - val_accuracy: 0.5545\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6828 - accuracy: 0.5500 - val_loss: 0.6798 - val_accuracy: 0.5574\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6849 - accuracy: 0.5537 - val_loss: 0.6774 - val_accuracy: 0.5563\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6876 - accuracy: 0.5375 - val_loss: 0.6782 - val_accuracy: 0.5592\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6815 - accuracy: 0.5631 - val_loss: 0.6833 - val_accuracy: 0.5406\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6845 - accuracy: 0.5444 - val_loss: 0.6884 - val_accuracy: 0.5179\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.7935 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0063s vs `on_test_batch_end` time: 0.0145s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7118 - accuracy: 0.4123\n",
      "training acc:  0.5443750023841858 , training loss:  0.684496283531189 , val acc:  0.5179093480110168 , val loss:  0.6884356141090393 , test acc:  0.4123322069644928 , test loss:  0.7118291258811951\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt3/100word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6973 - accuracy: 0.5181 - val_loss: 0.6928 - val_accuracy: 0.5179\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6918 - accuracy: 0.4988 - val_loss: 0.6892 - val_accuracy: 0.5289\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6916 - accuracy: 0.5125 - val_loss: 0.6890 - val_accuracy: 0.5179\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6880 - accuracy: 0.5281 - val_loss: 0.6850 - val_accuracy: 0.5329\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6871 - accuracy: 0.5475 - val_loss: 0.6831 - val_accuracy: 0.5556\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6828 - accuracy: 0.5519 - val_loss: 0.6814 - val_accuracy: 0.5585\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6849 - accuracy: 0.5556 - val_loss: 0.6820 - val_accuracy: 0.5471\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6863 - accuracy: 0.5350 - val_loss: 0.6816 - val_accuracy: 0.5625\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6791 - accuracy: 0.5731 - val_loss: 0.6805 - val_accuracy: 0.5545\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6818 - accuracy: 0.5619 - val_loss: 0.6794 - val_accuracy: 0.5545\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6855 - accuracy: 0.5675 - val_loss: 0.6795 - val_accuracy: 0.5548\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6892 - accuracy: 0.5225 - val_loss: 0.6812 - val_accuracy: 0.5625\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6858 - accuracy: 0.5387 - val_loss: 0.6795 - val_accuracy: 0.5556\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.7502 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0133s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 23ms/step - loss: 0.6875 - accuracy: 0.4941\n",
      "training acc:  0.5387499928474426 , training loss:  0.6858384609222412 , val acc:  0.5555555820465088 , val loss:  0.6794558763504028 , test acc:  0.4941275119781494 , test loss:  0.6875289082527161\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt3/150word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second around high best though lot file power following etc help either give form x less always means must    made bit note > order based & likely trying done option start % found least check solution edit end small list \\mathcal keep size anything already key maybe light 0 person\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6936 - accuracy: 0.5194 - val_loss: 0.6894 - val_accuracy: 0.5493\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6886 - accuracy: 0.5487 - val_loss: 0.6875 - val_accuracy: 0.5647\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6881 - accuracy: 0.5738 - val_loss: 0.6851 - val_accuracy: 0.5493\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6900 - accuracy: 0.4938 - val_loss: 0.6849 - val_accuracy: 0.5490\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6878 - accuracy: 0.5131 - val_loss: 0.6844 - val_accuracy: 0.5450\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6798 - accuracy: 0.5906 - val_loss: 0.6833 - val_accuracy: 0.5567\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6838 - accuracy: 0.5688 - val_loss: 0.6836 - val_accuracy: 0.5654\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6812 - accuracy: 0.5900 - val_loss: 0.6867 - val_accuracy: 0.5501\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6764 - accuracy: 0.6000 - val_loss: 0.6822 - val_accuracy: 0.5563\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6836 - accuracy: 0.5744 - val_loss: 0.6824 - val_accuracy: 0.5577\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6858 - accuracy: 0.5450 - val_loss: 0.6816 - val_accuracy: 0.5552\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6752 - accuracy: 0.5825 - val_loss: 0.6822 - val_accuracy: 0.5493\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6881 - accuracy: 0.5437 - val_loss: 0.6818 - val_accuracy: 0.5574\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6808 - accuracy: 0.5319 - val_loss: 0.6828 - val_accuracy: 0.5493\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.7266 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0063s vs `on_test_batch_end` time: 0.0154s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6869 - accuracy: 0.4970\n",
      "training acc:  0.5318750143051147 , training loss:  0.680807888507843 , val acc:  0.5493420958518982 , val loss:  0.6827570199966431 , test acc:  0.4969588816165924 , test loss:  0.686856746673584\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt3/200word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second around high best though lot file power following etc help either give form x less always means must    made bit note > order based & likely trying done option start % found least check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run type us user issue non similar works information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6822 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_train_batch_end` time: 0.0142s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6959 - accuracy: 0.5181 - val_loss: 0.6885 - val_accuracy: 0.5464\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6870 - accuracy: 0.5288 - val_loss: 0.6856 - val_accuracy: 0.5263\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6913 - accuracy: 0.5444 - val_loss: 0.6896 - val_accuracy: 0.5179\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6859 - accuracy: 0.5606 - val_loss: 0.6821 - val_accuracy: 0.5537\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6805 - accuracy: 0.5794 - val_loss: 0.6830 - val_accuracy: 0.5519\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6918 - accuracy: 0.5312 - val_loss: 0.6811 - val_accuracy: 0.5596\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6827 - accuracy: 0.5719 - val_loss: 0.6808 - val_accuracy: 0.5504\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6797 - accuracy: 0.5825 - val_loss: 0.6804 - val_accuracy: 0.5534\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6897 - accuracy: 0.5525 - val_loss: 0.6804 - val_accuracy: 0.5556\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6837 - accuracy: 0.5562 - val_loss: 0.6803 - val_accuracy: 0.5592\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6808 - accuracy: 0.5700 - val_loss: 0.6805 - val_accuracy: 0.5588\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6777 - accuracy: 0.5869 - val_loss: 0.6800 - val_accuracy: 0.5621\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6901 - accuracy: 0.5437 - val_loss: 0.6814 - val_accuracy: 0.5636\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6834 - accuracy: 0.5612 - val_loss: 0.6796 - val_accuracy: 0.5530\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6814 - accuracy: 0.5581 - val_loss: 0.6795 - val_accuracy: 0.5534\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6854 - accuracy: 0.5425 - val_loss: 0.6803 - val_accuracy: 0.5585\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6839 - accuracy: 0.5506 - val_loss: 0.6804 - val_accuracy: 0.5457\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6927 - accuracy: 0.5356 - val_loss: 0.6801 - val_accuracy: 0.5577\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7451 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0233s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 23ms/step - loss: 0.6879 - accuracy: 0.4878\n",
      "training acc:  0.5356249809265137 , training loss:  0.6927018761634827 , val acc:  0.557748556137085 , val loss:  0.6800853610038757 , test acc:  0.487835556268692 , test loss:  0.6879233121871948\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt3/250word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second around high best though lot file power following etc help either give form x less always means must    made bit note > order based & likely trying done option start % found least check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run type us user issue non similar works information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working makes \\to water whether error idea object 5 amount state pretty method got color assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6921 - accuracy: 0.4969 - val_loss: 0.6893 - val_accuracy: 0.5391\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6906 - accuracy: 0.5425 - val_loss: 0.6885 - val_accuracy: 0.5285\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6922 - accuracy: 0.5169 - val_loss: 0.6847 - val_accuracy: 0.5493\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6864 - accuracy: 0.5669 - val_loss: 0.6864 - val_accuracy: 0.5519\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6903 - accuracy: 0.5288 - val_loss: 0.6833 - val_accuracy: 0.5486\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6806 - accuracy: 0.5656 - val_loss: 0.6878 - val_accuracy: 0.5292\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6814 - accuracy: 0.5594 - val_loss: 0.6817 - val_accuracy: 0.5475\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6846 - accuracy: 0.5719 - val_loss: 0.6815 - val_accuracy: 0.5643\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6872 - accuracy: 0.5656 - val_loss: 0.6829 - val_accuracy: 0.5625\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6814 - accuracy: 0.5725 - val_loss: 0.6812 - val_accuracy: 0.5512\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6738 - accuracy: 0.5781 - val_loss: 0.6819 - val_accuracy: 0.5471\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6837 - accuracy: 0.5544 - val_loss: 0.6819 - val_accuracy: 0.5461\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6893 - accuracy: 0.5356 - val_loss: 0.6827 - val_accuracy: 0.5515\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.7198 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_test_batch_end` time: 0.0118s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6847 - accuracy: 0.5008\n",
      "training acc:  0.5356249809265137 , training loss:  0.6892752647399902 , val acc:  0.5515350699424744 , val loss:  0.682720959186554 , test acc:  0.5008389353752136 , test loss:  0.6846765279769897\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt3/300word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second around high best though lot file power following etc help either give form x less always means must    made bit note > order based & likely trying done option start % found least check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run type us user issue non similar works information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working makes \\to water whether error idea object 5 amount state pretty method got color assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference human online 10 test common general seem    character god paper full top block call camera term ai particular source energy short consider due available area control within post range i.e. results words created old class needs de book low away e.g. three 4 comes running making provide experience address\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6974 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0105s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.7039 - accuracy: 0.4981 - val_loss: 0.6946 - val_accuracy: 0.4821\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6913 - accuracy: 0.5281 - val_loss: 0.6941 - val_accuracy: 0.4821\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6960 - accuracy: 0.5131 - val_loss: 0.6893 - val_accuracy: 0.5468\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6886 - accuracy: 0.5350 - val_loss: 0.6872 - val_accuracy: 0.5409\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6886 - accuracy: 0.5431 - val_loss: 0.6835 - val_accuracy: 0.5475\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6834 - accuracy: 0.5719 - val_loss: 0.6832 - val_accuracy: 0.5669\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6818 - accuracy: 0.5663 - val_loss: 0.6888 - val_accuracy: 0.5289\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6754 - accuracy: 0.5719 - val_loss: 0.6856 - val_accuracy: 0.5442\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6874 - accuracy: 0.5500 - val_loss: 0.6817 - val_accuracy: 0.5658\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6916 - accuracy: 0.5213 - val_loss: 0.6816 - val_accuracy: 0.5581\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6825 - accuracy: 0.5781 - val_loss: 0.6812 - val_accuracy: 0.5636\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6943 - accuracy: 0.5306 - val_loss: 0.6816 - val_accuracy: 0.5439\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6746 - accuracy: 0.5987 - val_loss: 0.6838 - val_accuracy: 0.5486\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6790 - accuracy: 0.5669 - val_loss: 0.6812 - val_accuracy: 0.5497\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6961 - accuracy: 0.5350 - val_loss: 0.6810 - val_accuracy: 0.5592\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6812 - accuracy: 0.5569 - val_loss: 0.6806 - val_accuracy: 0.5618\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6880 - accuracy: 0.5494 - val_loss: 0.6821 - val_accuracy: 0.5475\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6804 - accuracy: 0.5594 - val_loss: 0.6860 - val_accuracy: 0.5453\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6844 - accuracy: 0.5638 - val_loss: 0.6819 - val_accuracy: 0.5640\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.7761 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0131s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7007 - accuracy: 0.4704\n",
      "training acc:  0.5637500286102295 , training loss:  0.6844386458396912 , val acc:  0.5639619827270508 , val loss:  0.6819438338279724 , test acc:  0.47042784094810486 , test loss:  0.7007101774215698\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt3/350word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second around high best though lot file power following etc help either give form x less always means must    made bit note > order based & likely trying done option start % found least check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run type us user issue non similar works information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working makes \\to water whether error idea object 5 amount state pretty method got color assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference human online 10 test common general seem    character god paper full top block call camera term ai particular source energy short consider due available area control within post range i.e. results words created old class needs de book low away e.g. three 4 comes running making provide experience address everything < changes easy year thought fine access ask lower account write hand search bytes lens although # perhaps exactly tell numbers google design thus multiple higher several anyone price feel useful ' wrong n yes almost game \\\\ unless nothing position believe effect takes clear standard product big days\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7818 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0114s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6999 - accuracy: 0.5288 - val_loss: 0.6865 - val_accuracy: 0.5588\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6817 - accuracy: 0.5719 - val_loss: 0.6857 - val_accuracy: 0.5336\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6838 - accuracy: 0.5656 - val_loss: 0.6836 - val_accuracy: 0.5632\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6886 - accuracy: 0.5519 - val_loss: 0.6831 - val_accuracy: 0.5632\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6848 - accuracy: 0.5500 - val_loss: 0.6829 - val_accuracy: 0.5439\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6815 - accuracy: 0.5744 - val_loss: 0.6823 - val_accuracy: 0.5607\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6875 - accuracy: 0.5525 - val_loss: 0.6907 - val_accuracy: 0.5238\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6922 - accuracy: 0.5250 - val_loss: 0.6867 - val_accuracy: 0.5325\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6749 - accuracy: 0.5663 - val_loss: 0.6908 - val_accuracy: 0.5314\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6984 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0138s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6843 - accuracy: 0.5016\n",
      "training acc:  0.5662500262260437 , training loss:  0.6748948693275452 , val acc:  0.5314327478408813 , val loss:  0.6907864212989807 , test acc:  0.5015729665756226 , test loss:  0.6842672824859619\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt3/400word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second around high best though lot file power following etc help either give form x less always means must    made bit note > order based & likely trying done option start % found least check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run type us user issue non similar works information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working makes \\to water whether error idea object 5 amount state pretty method got color assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference human online 10 test common general seem    character god paper full top block call camera term ai particular source energy short consider due available area control within post range i.e. results words created old class needs de book low away e.g. three 4 comes running making provide experience address everything < changes easy year thought fine access ask lower account write hand search bytes lens although # perhaps exactly tell numbers google design thus multiple higher several anyone price feel useful ' wrong n yes almost game \\\\ unless nothing position believe effect takes clear standard product big days true others select language future close thanks cases return looks certain sort turn points content length actual avoid model added uses que normal gives smaller research map difficult view terms seen easily computer update options 100 known main allow per step sound context network whole problems english link el generally\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7317 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0159s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6936 - accuracy: 0.4831 - val_loss: 0.6870 - val_accuracy: 0.5439\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6897 - accuracy: 0.5125 - val_loss: 0.6867 - val_accuracy: 0.5662\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6896 - accuracy: 0.5481 - val_loss: 0.6870 - val_accuracy: 0.5567\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6857 - accuracy: 0.5644 - val_loss: 0.6845 - val_accuracy: 0.5599\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6896 - accuracy: 0.5512 - val_loss: 0.6845 - val_accuracy: 0.5643\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6839 - accuracy: 0.5694 - val_loss: 0.6834 - val_accuracy: 0.5588\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6863 - accuracy: 0.5663 - val_loss: 0.6838 - val_accuracy: 0.5632\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6850 - accuracy: 0.5531 - val_loss: 0.6841 - val_accuracy: 0.5482\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6875 - accuracy: 0.5481 - val_loss: 0.6853 - val_accuracy: 0.5420\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.7159 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_test_batch_end` time: 0.0186s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6874 - accuracy: 0.5052\n",
      "training acc:  0.5481250286102295 , training loss:  0.6875463724136353 , val acc:  0.5420321822166443 , val loss:  0.6852560043334961 , test acc:  0.5052433013916016 , test loss:  0.6874250173568726\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt3/450word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second around high best though lot file power following etc help either give form x less always means must    made bit note > order based & likely trying done option start % found least check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run type us user issue non similar works information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working makes \\to water whether error idea object 5 amount state pretty method got color assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference human online 10 test common general seem    character god paper full top block call camera term ai particular source energy short consider due available area control within post range i.e. results words created old class needs de book low away e.g. three 4 comes running making provide experience address everything < changes easy year thought fine access ask lower account write hand search bytes lens although # perhaps exactly tell numbers google design thus multiple higher several anyone price feel useful ' wrong n yes almost game \\\\ unless nothing position believe effect takes clear standard product big days true others select language future close thanks cases return looks certain sort turn points content length actual avoid model added uses que normal gives smaller research map difficult view terms seen easily computer update options 100 known main allow per step sound context network whole problems english link el generally setting rate play tool mind mm meaning bad along client ones money sometimes cause air therefore company files gets e head happen support click move sentence says become choose string yet characters target instance _ functions   related body n$ taking depends save inside rules story reading images please background\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.7365 - accuracy: 0.5069WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_test_batch_end` time: 0.0160s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.7365 - accuracy: 0.5069 - val_loss: 0.7000 - val_accuracy: 0.4437\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6891 - accuracy: 0.5344 - val_loss: 0.7001 - val_accuracy: 0.4821\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6929 - accuracy: 0.5081 - val_loss: 0.6908 - val_accuracy: 0.4839\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6918 - accuracy: 0.5169 - val_loss: 0.6876 - val_accuracy: 0.5490\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6904 - accuracy: 0.5094 - val_loss: 0.6882 - val_accuracy: 0.5164\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6873 - accuracy: 0.5175 - val_loss: 0.6855 - val_accuracy: 0.5588\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6844 - accuracy: 0.5462 - val_loss: 0.6857 - val_accuracy: 0.5322\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6833 - accuracy: 0.5806 - val_loss: 0.6871 - val_accuracy: 0.5340\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6893 - accuracy: 0.5406 - val_loss: 0.6831 - val_accuracy: 0.5570\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6884 - accuracy: 0.5306 - val_loss: 0.6837 - val_accuracy: 0.5461\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6854 - accuracy: 0.5638 - val_loss: 0.6837 - val_accuracy: 0.5618\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6805 - accuracy: 0.5856 - val_loss: 0.6819 - val_accuracy: 0.5625\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6835 - accuracy: 0.5550 - val_loss: 0.6820 - val_accuracy: 0.5570\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6800 - accuracy: 0.5681 - val_loss: 0.6818 - val_accuracy: 0.5610\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6927 - accuracy: 0.5556 - val_loss: 0.6817 - val_accuracy: 0.5603\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6787 - accuracy: 0.5688 - val_loss: 0.6885 - val_accuracy: 0.5347\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6870 - accuracy: 0.5450 - val_loss: 0.6820 - val_accuracy: 0.5537\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6895 - accuracy: 0.5350 - val_loss: 0.6817 - val_accuracy: 0.5629\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.7377 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0135s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6902 - accuracy: 0.4909\n",
      "training acc:  0.5350000262260437 , training loss:  0.6895188093185425 , val acc:  0.5628654956817627 , val loss:  0.6817249655723572 , test acc:  0.4908766746520996 , test loss:  0.6901658773422241\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt3/500word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second around high best though lot file power following etc help either give form x less always means must    made bit note > order based & likely trying done option start % found least check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run type us user issue non similar works information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working makes \\to water whether error idea object 5 amount state pretty method got color assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference human online 10 test common general seem    character god paper full top block call camera term ai particular source energy short consider due available area control within post range i.e. results words created old class needs de book low away e.g. three 4 comes running making provide experience address everything < changes easy year thought fine access ask lower account write hand search bytes lens although # perhaps exactly tell numbers google design thus multiple higher several anyone price feel useful ' wrong n yes almost game \\\\ unless nothing position believe effect takes clear standard product big days true others select language future close thanks cases return looks certain sort turn points content length actual avoid model added uses que normal gives smaller research map difficult view terms seen easily computer update options 100 known main allow per step sound context network whole problems english link el generally setting rate play tool mind mm meaning bad along client ones money sometimes cause air therefore company files gets e head happen support click move sentence says become choose string yet characters target instance _ functions   related body n$ taking depends save inside rules story reading images please background examples web theory build assuming earth transaction mode local photoshop job directly random writing later approach database pages algorithm flash stop history parts application guess hope situation larger structure apply quality half 6 table whatever rule windows users exact slightly especially worth limit software car remember remove necessary force longer\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6968 - accuracy: 0.5044 - val_loss: 0.6858 - val_accuracy: 0.5625\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6911 - accuracy: 0.5369 - val_loss: 0.6855 - val_accuracy: 0.5428\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6819 - accuracy: 0.5706 - val_loss: 0.6836 - val_accuracy: 0.5607\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6866 - accuracy: 0.5487 - val_loss: 0.6832 - val_accuracy: 0.5588\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6844 - accuracy: 0.5619 - val_loss: 0.6829 - val_accuracy: 0.5629\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6850 - accuracy: 0.5756 - val_loss: 0.6828 - val_accuracy: 0.5629\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6877 - accuracy: 0.5575 - val_loss: 0.6843 - val_accuracy: 0.5581\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6863 - accuracy: 0.5581 - val_loss: 0.6827 - val_accuracy: 0.5625\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6783 - accuracy: 0.5850 - val_loss: 0.6820 - val_accuracy: 0.5603\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6897 - accuracy: 0.5394 - val_loss: 0.6837 - val_accuracy: 0.5493\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6809 - accuracy: 0.5688 - val_loss: 0.6824 - val_accuracy: 0.5577\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6844 - accuracy: 0.5481 - val_loss: 0.6825 - val_accuracy: 0.5588\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.7337 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0008s vs `on_test_batch_end` time: 0.0224s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6897 - accuracy: 0.4972\n",
      "training acc:  0.5481250286102295 , training loss:  0.684401273727417 , val acc:  0.558845043182373 , val loss:  0.6825234293937683 , test acc:  0.497168630361557 , test loss:  0.6896526217460632\n",
      "\n",
      "opt4\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt4/50word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6983 - accuracy: 0.4894 - val_loss: 0.7025 - val_accuracy: 0.4821\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6938 - accuracy: 0.5294 - val_loss: 0.6953 - val_accuracy: 0.4821\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6995 - accuracy: 0.4819 - val_loss: 0.6932 - val_accuracy: 0.5219\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6978 - accuracy: 0.4869 - val_loss: 0.6963 - val_accuracy: 0.5179\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6965 - accuracy: 0.5119 - val_loss: 0.6932 - val_accuracy: 0.4872\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6948 - accuracy: 0.5050 - val_loss: 0.6947 - val_accuracy: 0.5179\n",
      "  1/596 [..............................] - ETA: 0s - loss: 0.7457 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_test_batch_end` time: 0.0134s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7102 - accuracy: 0.4123\n",
      "training acc:  0.5049999952316284 , training loss:  0.6947953104972839 , val acc:  0.5179093480110168 , val loss:  0.6947116255760193 , test acc:  0.4123322069644928 , test loss:  0.7101795673370361\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt4/100word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 1.5276 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_train_batch_end` time: 0.0123s). Check your callbacks.\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.8242 - accuracy: 0.4756WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_test_batch_end` time: 0.0114s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.8242 - accuracy: 0.4756 - val_loss: 0.6927 - val_accuracy: 0.5179\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6930 - accuracy: 0.5256 - val_loss: 0.6963 - val_accuracy: 0.5179\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6927 - accuracy: 0.5250 - val_loss: 0.6927 - val_accuracy: 0.5168\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6965 - accuracy: 0.5013 - val_loss: 0.6920 - val_accuracy: 0.5464\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6960 - accuracy: 0.4913 - val_loss: 0.6942 - val_accuracy: 0.5179\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6943 - accuracy: 0.5094 - val_loss: 0.6931 - val_accuracy: 0.5179\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6982 - accuracy: 0.4881 - val_loss: 0.6936 - val_accuracy: 0.4934\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6840 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0138s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6995 - accuracy: 0.4693\n",
      "training acc:  0.4881249964237213 , training loss:  0.6982009410858154 , val acc:  0.4934210479259491 , val loss:  0.6936034560203552 , test acc:  0.4692743420600891 , test loss:  0.6994908452033997\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt4/150word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second around high best though lot file power following etc help either give form x less always means must    made bit note > order based & likely trying done option start % found least check solution edit end small list \\mathcal keep size anything already key maybe light\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.7432 - accuracy: 0.4575 - val_loss: 0.6942 - val_accuracy: 0.5322\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6970 - accuracy: 0.5100 - val_loss: 0.6939 - val_accuracy: 0.5164\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6965 - accuracy: 0.4931 - val_loss: 0.6939 - val_accuracy: 0.5260\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6959 - accuracy: 0.5000 - val_loss: 0.6952 - val_accuracy: 0.4821\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6978 - accuracy: 0.4650 - val_loss: 0.6934 - val_accuracy: 0.5318\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6956 - accuracy: 0.4869 - val_loss: 0.6934 - val_accuracy: 0.5252\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6951 - accuracy: 0.5063 - val_loss: 0.6946 - val_accuracy: 0.4821\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6950 - accuracy: 0.5150 - val_loss: 0.6932 - val_accuracy: 0.5296\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6936 - accuracy: 0.5150 - val_loss: 0.6951 - val_accuracy: 0.5219\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6956 - accuracy: 0.4950 - val_loss: 0.6929 - val_accuracy: 0.5205\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6953 - accuracy: 0.4731 - val_loss: 0.6935 - val_accuracy: 0.4821\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6954 - accuracy: 0.4819 - val_loss: 0.6928 - val_accuracy: 0.5230\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6951 - accuracy: 0.4850 - val_loss: 0.6992 - val_accuracy: 0.4821\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6981 - accuracy: 0.4994 - val_loss: 0.7099 - val_accuracy: 0.4821\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6905 - accuracy: 0.5294 - val_loss: 0.6948 - val_accuracy: 0.5179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/596 [..............................] - ETA: 7s - loss: 0.7441 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_test_batch_end` time: 0.0188s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7105 - accuracy: 0.4123\n",
      "training acc:  0.5293750166893005 , training loss:  0.6905259490013123 , val acc:  0.5179093480110168 , val loss:  0.6947565674781799 , test acc:  0.4123322069644928 , test loss:  0.7104635834693909\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt4/200word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second around high best though lot file power following etc help either give form x less always means must    made bit note > order based & likely trying done option start % found least check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run type us user issue non similar works information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.7046 - accuracy: 0.5238 - val_loss: 0.6940 - val_accuracy: 0.5179\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6937 - accuracy: 0.5163 - val_loss: 0.6927 - val_accuracy: 0.5179\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6958 - accuracy: 0.4787 - val_loss: 0.6924 - val_accuracy: 0.5457\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6954 - accuracy: 0.4931 - val_loss: 0.6929 - val_accuracy: 0.5223\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6982 - accuracy: 0.4725 - val_loss: 0.6945 - val_accuracy: 0.4645\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6941 - accuracy: 0.5231 - val_loss: 0.6978 - val_accuracy: 0.4700\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.6657 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0152s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6941 - accuracy: 0.5007\n",
      "training acc:  0.5231249928474426 , training loss:  0.6941419243812561 , val acc:  0.47002923488616943 , val loss:  0.6978100538253784 , test acc:  0.5007340312004089 , test loss:  0.6940991878509521\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt4/250word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second around high best though lot file power following etc help either give form x less always means must    made bit note > order based & likely trying done option start % found least check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run type us user issue non similar works information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working makes \\to water whether error idea object 5 amount state pretty method got color assume called values next original layer current    kind input free getting life sense else past tried matter wo great\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.7673 - accuracy: 0.5231 - val_loss: 0.6941 - val_accuracy: 0.5347\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6942 - accuracy: 0.5188 - val_loss: 0.6945 - val_accuracy: 0.5278\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6941 - accuracy: 0.5088 - val_loss: 0.6980 - val_accuracy: 0.4821\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6967 - accuracy: 0.5306 - val_loss: 0.6952 - val_accuracy: 0.5314\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.7289 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0143s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6990 - accuracy: 0.4971\n",
      "training acc:  0.5306249856948853 , training loss:  0.69672030210495 , val acc:  0.5314327478408813 , val loss:  0.6951561570167542 , test acc:  0.4970637559890747 , test loss:  0.6989877223968506\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt4/300word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second around high best though lot file power following etc help either give form x less always means must    made bit note > order based & likely trying done option start % found least check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run type us user issue non similar works information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working makes \\to water whether error idea object 5 amount state pretty method got color assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference human online 10 test common general seem    character god paper full top block call camera term ai particular source energy short consider due available area control within post range i.e. results words created old class needs de book low away e.g. three 4 comes running making provide\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 1.3071 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0248s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.7119 - accuracy: 0.5138 - val_loss: 0.6932 - val_accuracy: 0.5303\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.6965 - accuracy: 0.5006 - val_loss: 0.6933 - val_accuracy: 0.5208\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6995 - accuracy: 0.4725 - val_loss: 0.6949 - val_accuracy: 0.4821\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6941 - accuracy: 0.5238 - val_loss: 0.6947 - val_accuracy: 0.4821\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.6785 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0143s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6883 - accuracy: 0.5877\n",
      "training acc:  0.5237500071525574 , training loss:  0.6940872073173523 , val acc:  0.48209065198898315 , val loss:  0.6947231888771057 , test acc:  0.5876677632331848 , test loss:  0.6882575750350952\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt4/350word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second around high best though lot file power following etc help either give form x less always means must    made bit note > order based & likely trying done option start % found least check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run type us user issue non similar works information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working makes \\to water whether error idea object 5 amount state pretty method got color assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference human online 10 test common general seem    character god paper full top block call camera term ai particular source energy short consider due available area control within post range i.e. results words created old class needs de book low away e.g. three 4 comes running making provide experience address everything < changes easy year thought fine access ask lower account write hand search bytes lens although # perhaps exactly tell numbers google design thus multiple higher several anyone price feel useful ' wrong n yes almost game \\\\ unless nothing position believe effect takes clear standard product\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6971 - accuracy: 0.4787 - val_loss: 0.6951 - val_accuracy: 0.4821\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6927 - accuracy: 0.5294 - val_loss: 0.6928 - val_accuracy: 0.5179\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6953 - accuracy: 0.5100 - val_loss: 0.7043 - val_accuracy: 0.4821\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6950 - accuracy: 0.5225 - val_loss: 0.6926 - val_accuracy: 0.5179\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6944 - accuracy: 0.5069 - val_loss: 0.6927 - val_accuracy: 0.5179\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6945 - accuracy: 0.4981 - val_loss: 0.6926 - val_accuracy: 0.5179\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6947 - accuracy: 0.5337 - val_loss: 0.6933 - val_accuracy: 0.5179\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.7266 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0187s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.7026 - accuracy: 0.4123\n",
      "training acc:  0.5337499976158142 , training loss:  0.694689154624939 , val acc:  0.5179093480110168 , val loss:  0.693259060382843 , test acc:  0.4123322069644928 , test loss:  0.7025789618492126\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt4/400word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second around high best though lot file power following etc help either give form x less always means must    made bit note > order based & likely trying done option start % found least check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run type us user issue non similar works information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working makes \\to water whether error idea object 5 amount state pretty method got color assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference human online 10 test common general seem    character god paper full top block call camera term ai particular source energy short consider due available area control within post range i.e. results words created old class needs de book low away e.g. three 4 comes running making provide experience address everything < changes easy year thought fine access ask lower account write hand search bytes lens although # perhaps exactly tell numbers google design thus multiple higher several anyone price feel useful ' wrong n yes almost game \\\\ unless nothing position believe effect takes clear standard product big days true others select language future close thanks cases return looks certain sort turn points content length actual avoid model added uses que normal gives smaller research map difficult view terms seen easily computer update options 100 known main allow per step sound context network whole problems english link\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6689 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0128s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6985 - accuracy: 0.4756 - val_loss: 0.6928 - val_accuracy: 0.5230\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6950 - accuracy: 0.4919 - val_loss: 0.6929 - val_accuracy: 0.5322\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6944 - accuracy: 0.5063 - val_loss: 0.6930 - val_accuracy: 0.5216\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6979 - accuracy: 0.4613 - val_loss: 0.6941 - val_accuracy: 0.4821\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.6831 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0009s vs `on_test_batch_end` time: 0.0226s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6905 - accuracy: 0.5877\n",
      "training acc:  0.4612500071525574 , training loss:  0.6979067921638489 , val acc:  0.48209065198898315 , val loss:  0.6941092610359192 , test acc:  0.5876677632331848 , test loss:  0.690548300743103\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt4/450word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second around high best though lot file power following etc help either give form x less always means must    made bit note > order based & likely trying done option start % found least check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run type us user issue non similar works information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working makes \\to water whether error idea object 5 amount state pretty method got color assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference human online 10 test common general seem    character god paper full top block call camera term ai particular source energy short consider due available area control within post range i.e. results words created old class needs de book low away e.g. three 4 comes running making provide experience address everything < changes easy year thought fine access ask lower account write hand search bytes lens although # perhaps exactly tell numbers google design thus multiple higher several anyone price feel useful ' wrong n yes almost game \\\\ unless nothing position believe effect takes clear standard product big days true others select language future close thanks cases return looks certain sort turn points content length actual avoid model added uses que normal gives smaller research map difficult view terms seen easily computer update options 100 known main allow per step sound context network whole problems english link el generally setting rate play tool mind mm meaning bad along client ones money sometimes cause air therefore company files gets e head happen support click move sentence says become choose string yet characters target instance _ functions   related body n$ taking depends save inside rules story reading images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6059 - accuracy: 0.9375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0028s vs `on_train_batch_end` time: 0.0105s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6923 - accuracy: 0.5144 - val_loss: 0.6929 - val_accuracy: 0.5179\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6973 - accuracy: 0.4831 - val_loss: 0.6965 - val_accuracy: 0.4762\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6952 - accuracy: 0.5194 - val_loss: 0.6940 - val_accuracy: 0.5179\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6936 - accuracy: 0.5075 - val_loss: 0.6933 - val_accuracy: 0.4642\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6906 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0133s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6933 - accuracy: 0.4812\n",
      "training acc:  0.5074999928474426 , training loss:  0.6936139464378357 , val acc:  0.4641812741756439 , val loss:  0.6933385729789734 , test acc:  0.4812290370464325 , test loss:  0.6932676434516907\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_200/opt4/500word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one   like use / also get could time need way make using want 'm work - see first even think two know 1 may people example = different might much used something 've question 're well good case however set number find 2 say new problem since really many take still try possible another answer data right point go without probably things change value better + enough let back code actually ca image sure 'll system 'd look page add going long create able 3 url$ every given function thing part seems second around high best though lot file power following etc help either give form x less always means must    made bit note > order based & likely trying done option start % found least check solution edit end small list \\mathcal keep size anything already key maybe light 0 person day field read server text reason little run type us user issue non similar works information specific word rather space line someone looking instead correct said single times level mean process years simple    world real quite place often version never simply show put usually open result understand name output large b far site come last hard group side c important course fact working makes \\to water whether error idea object 5 amount state pretty method got color assume called values next original layer current    kind input free getting life sense else past tried matter wo great left difference human online 10 test common general seem    character god paper full top block call camera term ai particular source energy short consider due available area control within post range i.e. results words created old class needs de book low away e.g. three 4 comes running making provide experience address everything < changes easy year thought fine access ask lower account write hand search bytes lens although # perhaps exactly tell numbers google design thus multiple higher several anyone price feel useful ' wrong n yes almost game \\\\ unless nothing position believe effect takes clear standard product big days true others select language future close thanks cases return looks certain sort turn points content length actual avoid model added uses que normal gives smaller research map difficult view terms seen easily computer update options 100 known main allow per step sound context network whole problems english link el generally setting rate play tool mind mm meaning bad along client ones money sometimes cause air therefore company files gets e head happen support click move sentence says become choose string yet characters target instance _ functions   related body n$ taking depends save inside rules story reading images please background examples web theory build assuming earth transaction mode local photoshop job directly random writing later approach database pages algorithm flash stop history parts application guess hope situation larger structure apply quality half 6 table whatever rule windows users exact slightly especially worth limit software car remember remove necessary\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.6973 - accuracy: 0.4938 - val_loss: 0.7035 - val_accuracy: 0.4821\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6984 - accuracy: 0.4856 - val_loss: 0.6959 - val_accuracy: 0.4766\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.6951 - accuracy: 0.4931 - val_loss: 0.7020 - val_accuracy: 0.4792\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6956 - accuracy: 0.5106 - val_loss: 0.6926 - val_accuracy: 0.5179\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.6927 - accuracy: 0.5138 - val_loss: 0.7077 - val_accuracy: 0.4781\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6962 - accuracy: 0.5175 - val_loss: 0.6927 - val_accuracy: 0.5376\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.6945 - accuracy: 0.5094 - val_loss: 0.6952 - val_accuracy: 0.5179\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7518 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0135s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7143 - accuracy: 0.4123\n",
      "training acc:  0.5093749761581421 , training loss:  0.6945104002952576 , val acc:  0.5179093480110168 , val loss:  0.6952069997787476 , test acc:  0.4123322069644928 , test loss:  0.7142599821090698\n",
      "\n",
      "Started evaluation for: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_400.csv\n",
      "Loading training file from path:  /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_400.csv\n",
      "(8326, 6)\n",
      "Loading testing file from path:  /home/sukanya/PhD/Datasets/PAN SCD/pan19-style-change-detection/processed/test.csv\n",
      "(9538, 5)\n",
      "opt1\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt1/50word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.8488 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0239s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6699 - accuracy: 0.6106 - val_loss: 0.6541 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6616 - accuracy: 0.6431 - val_loss: 0.6533 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6481 - accuracy: 0.6594 - val_loss: 0.6544 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6677 - accuracy: 0.6263 - val_loss: 0.6546 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6622 - accuracy: 0.6306 - val_loss: 0.6606 - val_accuracy: 0.6526\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7683 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0139s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7272 - accuracy: 0.4123\n",
      "training acc:  0.6306250095367432 , training loss:  0.6621730327606201 , val acc:  0.6526442170143127 , val loss:  0.6605821847915649 , test acc:  0.4123322069644928 , test loss:  0.7271608114242554\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt1/100word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7227 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_train_batch_end` time: 0.0133s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6744 - accuracy: 0.6144 - val_loss: 0.6892 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6649 - accuracy: 0.6400 - val_loss: 0.6552 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6730 - accuracy: 0.6206 - val_loss: 0.6562 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6624 - accuracy: 0.6394 - val_loss: 0.6642 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6603 - accuracy: 0.6388 - val_loss: 0.6488 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6450 - accuracy: 0.6625 - val_loss: 0.6464 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6374 - accuracy: 0.6719 - val_loss: 0.6449 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6690 - accuracy: 0.5962 - val_loss: 0.6491 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6475 - accuracy: 0.6519 - val_loss: 0.6447 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6513 - accuracy: 0.6406 - val_loss: 0.6416 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6394 - accuracy: 0.6587 - val_loss: 0.6400 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6773 - accuracy: 0.5775 - val_loss: 0.6457 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6582 - accuracy: 0.6237 - val_loss: 0.6423 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6490 - accuracy: 0.6400 - val_loss: 0.6470 - val_accuracy: 0.6689\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7753 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0160s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.7738 - accuracy: 0.4209\n",
      "training acc:  0.6399999856948853 , training loss:  0.6489765048027039 , val acc:  0.668870210647583 , val loss:  0.6469817757606506 , test acc:  0.42093122005462646 , test loss:  0.7737711071968079\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt1/150word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most much = might different example each such something he used 've question now well 're its good am however been after case set really number being find many since take say 2 problem still new who those both try possible did another too were between had why right things while\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.2535 - accuracy: 1.0000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_train_batch_end` time: 0.0127s). Check your callbacks.\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6778 - accuracy: 0.6369WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_test_batch_end` time: 0.0105s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6778 - accuracy: 0.6369 - val_loss: 0.6619 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6638 - accuracy: 0.6425 - val_loss: 0.6595 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6532 - accuracy: 0.6700 - val_loss: 0.6569 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6602 - accuracy: 0.6475 - val_loss: 0.6552 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6671 - accuracy: 0.6313 - val_loss: 0.6533 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6712 - accuracy: 0.6250 - val_loss: 0.6514 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6593 - accuracy: 0.6400 - val_loss: 0.6510 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6554 - accuracy: 0.6438 - val_loss: 0.6490 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6399 - accuracy: 0.6687 - val_loss: 0.6472 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6398 - accuracy: 0.6669 - val_loss: 0.6477 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6505 - accuracy: 0.6475 - val_loss: 0.6451 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6515 - accuracy: 0.6450 - val_loss: 0.6436 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6401 - accuracy: 0.6694 - val_loss: 0.6467 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6658 - accuracy: 0.6150 - val_loss: 0.6436 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6635 - accuracy: 0.6194 - val_loss: 0.6399 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6624 - accuracy: 0.6219 - val_loss: 0.6400 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6527 - accuracy: 0.6288 - val_loss: 0.6380 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6559 - accuracy: 0.6250 - val_loss: 0.6378 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6410 - accuracy: 0.6575 - val_loss: 0.6370 - val_accuracy: 0.6526\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6456 - accuracy: 0.6506 - val_loss: 0.6367 - val_accuracy: 0.6526\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6582 - accuracy: 0.6150 - val_loss: 0.6386 - val_accuracy: 0.6532\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6561 - accuracy: 0.6194 - val_loss: 0.6437 - val_accuracy: 0.6827\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6555 - accuracy: 0.6263 - val_loss: 0.6346 - val_accuracy: 0.6526\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6443 - accuracy: 0.6500 - val_loss: 0.6339 - val_accuracy: 0.6526\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6202 - accuracy: 0.6775 - val_loss: 0.6328 - val_accuracy: 0.6526\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6386 - accuracy: 0.6538 - val_loss: 0.6370 - val_accuracy: 0.6526\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6217 - accuracy: 0.6769 - val_loss: 0.6323 - val_accuracy: 0.6526\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6582 - accuracy: 0.6125 - val_loss: 0.6373 - val_accuracy: 0.6863\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6429 - accuracy: 0.6556 - val_loss: 0.6320 - val_accuracy: 0.6526\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6575 - accuracy: 0.6313 - val_loss: 0.6312 - val_accuracy: 0.6538\n",
      "  1/596 [..............................] - ETA: 0s - loss: 0.8614 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0136s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.8386 - accuracy: 0.4132\n",
      "training acc:  0.6312500238418579 , training loss:  0.6575407385826111 , val acc:  0.6538461446762085 , val loss:  0.6311854124069214 , test acc:  0.4131711423397064 , test loss:  0.8386432528495789\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt1/200word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most much = might different example each such something he used 've question now well 're its good am however been after case set really number being find many since take say 2 problem still new who those both try possible did another too were between had why right things while probably point over answer go enough our + without better data value let through before change his actually ca back 'd system going 'll image long sure down able look every code add 3 given own off high page create power thing part url$ second function around seems though lot\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6741 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0069s vs `on_train_batch_end` time: 0.0150s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6535 - accuracy: 0.6650 - val_loss: 0.6571 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6751 - accuracy: 0.6225 - val_loss: 0.6551 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6625 - accuracy: 0.6431 - val_loss: 0.6576 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6620 - accuracy: 0.6431 - val_loss: 0.6519 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6387 - accuracy: 0.6787 - val_loss: 0.6500 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6625 - accuracy: 0.6319 - val_loss: 0.6516 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6507 - accuracy: 0.6562 - val_loss: 0.6464 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6468 - accuracy: 0.6581 - val_loss: 0.6541 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6625 - accuracy: 0.6231 - val_loss: 0.6531 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6431 - accuracy: 0.6631 - val_loss: 0.6447 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6631 - accuracy: 0.6225 - val_loss: 0.6558 - val_accuracy: 0.6761\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6459 - accuracy: 0.6587 - val_loss: 0.6402 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6418 - accuracy: 0.6587 - val_loss: 0.6503 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6593 - accuracy: 0.6288 - val_loss: 0.6392 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6560 - accuracy: 0.6338 - val_loss: 0.6445 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6591 - accuracy: 0.6237 - val_loss: 0.6419 - val_accuracy: 0.6538\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6497 - accuracy: 0.6438 - val_loss: 0.6362 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6369 - accuracy: 0.6494 - val_loss: 0.6433 - val_accuracy: 0.6827\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6650 - accuracy: 0.6112 - val_loss: 0.6400 - val_accuracy: 0.6671\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6537 - accuracy: 0.6338 - val_loss: 0.6340 - val_accuracy: 0.6526\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6683 - accuracy: 0.6131 - val_loss: 0.6339 - val_accuracy: 0.6526\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6328 - accuracy: 0.6644 - val_loss: 0.6381 - val_accuracy: 0.6526\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6580 - accuracy: 0.6144 - val_loss: 0.6326 - val_accuracy: 0.6526\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6456 - accuracy: 0.6506 - val_loss: 0.6379 - val_accuracy: 0.6526\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6451 - accuracy: 0.6444 - val_loss: 0.6316 - val_accuracy: 0.6526\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6454 - accuracy: 0.6488 - val_loss: 0.6384 - val_accuracy: 0.6526\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6529 - accuracy: 0.6350 - val_loss: 0.6404 - val_accuracy: 0.6827\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6470 - accuracy: 0.6369 - val_loss: 0.6599 - val_accuracy: 0.5998\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7870 - accuracy: 0.4033\n",
      "training acc:  0.6368749737739563 , training loss:  0.6470243334770203 , val acc:  0.5997596383094788 , val loss:  0.6599143743515015 , test acc:  0.4033137559890747 , test loss:  0.7869652509689331\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt1/250word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most much = might different example each such something he used 've question now well 're its good am however been after case set really number being find many since take say 2 problem still new who those both try possible did another too were between had why right things while probably point over answer go enough our + without better data value let through before change his actually ca back 'd system going 'll image long sure down able look every code add 3 given own off high page create power thing part url$ second function around seems though lot always best etc less give either having help x & likely \\mathcal means must form few made following least bit note small above trying order doing light end anything based done option file start % keep size maybe found us already she person world real > reason edit space 0\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6222 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0145s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6548 - accuracy: 0.6631 - val_loss: 0.6583 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6507 - accuracy: 0.6644 - val_loss: 0.6574 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6755 - accuracy: 0.6150 - val_loss: 0.6629 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6631 - accuracy: 0.6431 - val_loss: 0.6580 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6751 - accuracy: 0.6100 - val_loss: 0.6613 - val_accuracy: 0.6526\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.7672 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0162s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7251 - accuracy: 0.4123\n",
      "training acc:  0.6100000143051147 , training loss:  0.6750839948654175 , val acc:  0.6526442170143127 , val loss:  0.6613285541534424 , test acc:  0.4123322069644928 , test loss:  0.7250651121139526\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt1/300word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most much = might different example each such something he used 've question now well 're its good am however been after case set really number being find many since take say 2 problem still new who those both try possible did another too were between had why right things while probably point over answer go enough our + without better data value let through before change his actually ca back 'd system going 'll image long sure down able look every code add 3 given own off high page create power thing part url$ second function around seems though lot always best etc less give either having help x & likely \\mathcal means must form few made following least bit note small above trying order doing light end anything based done option file start % keep size maybe found us already she person world real > reason edit space 0    years check said little read solution process rather non day times list again field level someone once issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.8413 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0064s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.7062 - accuracy: 0.5756 - val_loss: 0.6546 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6686 - accuracy: 0.6275 - val_loss: 0.6581 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6563 - accuracy: 0.6494 - val_loss: 0.6534 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6650 - accuracy: 0.6206 - val_loss: 0.6474 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6670 - accuracy: 0.6237 - val_loss: 0.6504 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6459 - accuracy: 0.6569 - val_loss: 0.6442 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6461 - accuracy: 0.6456 - val_loss: 0.6495 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6562 - accuracy: 0.6431 - val_loss: 0.6411 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6544 - accuracy: 0.6413 - val_loss: 0.6467 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6490 - accuracy: 0.6456 - val_loss: 0.6462 - val_accuracy: 0.6544\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6504 - accuracy: 0.6425 - val_loss: 0.6385 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6410 - accuracy: 0.6550 - val_loss: 0.6388 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6491 - accuracy: 0.6469 - val_loss: 0.6365 - val_accuracy: 0.6526 accuracy: 0.64\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6396 - accuracy: 0.6600 - val_loss: 0.6374 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6615 - accuracy: 0.6244 - val_loss: 0.6380 - val_accuracy: 0.6629\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6389 - accuracy: 0.6569 - val_loss: 0.6339 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6559 - accuracy: 0.6344 - val_loss: 0.6359 - val_accuracy: 0.6647\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6370 - accuracy: 0.6525 - val_loss: 0.6490 - val_accuracy: 0.6659\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.6522 - accuracy: 0.6300 - val_loss: 0.6385 - val_accuracy: 0.6893\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.8006 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0125s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.8102 - accuracy: 0.4255\n",
      "training acc:  0.6299999952316284 , training loss:  0.6522152423858643 , val acc:  0.6893028616905212 , val loss:  0.6384535431861877 , test acc:  0.4255453050136566 , test loss:  0.8101583123207092\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt1/350word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most much = might different example each such something he used 've question now well 're its good am however been after case set really number being find many since take say 2 problem still new who those both try possible did another too were between had why right things while probably point over answer go enough our + without better data value let through before change his actually ca back 'd system going 'll image long sure down able look every code add 3 given own off high page create power thing part url$ second function around seems though lot always best etc less give either having help x & likely \\mathcal means must form few made following least bit note small above trying order doing light end anything based done option file start % keep size maybe found us already she person world real > reason edit space 0    years check said little read solution process rather non day times list again field level someone once issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show him result course understand user makes open hard life group output fact last idea under human working her until name site whether amount pretty version water assume matter state kind past error method itself    got color    5 energy great original paper due sense else called character\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.7096 - accuracy: 0.5425 - val_loss: 0.6533 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6795 - accuracy: 0.5975 - val_loss: 0.6702 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6415 - accuracy: 0.6687 - val_loss: 0.6559 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6717 - accuracy: 0.6075 - val_loss: 0.6619 - val_accuracy: 0.6526\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.7512 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0161s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7280 - accuracy: 0.4123\n",
      "training acc:  0.6075000166893005 , training loss:  0.6717345714569092 , val acc:  0.6526442170143127 , val loss:  0.6619006395339966 , test acc:  0.4123322069644928 , test loss:  0.7279635667800903\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt1/400word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most much = might different example each such something he used 've question now well 're its good am however been after case set really number being find many since take say 2 problem still new who those both try possible did another too were between had why right things while probably point over answer go enough our + without better data value let through before change his actually ca back 'd system going 'll image long sure down able look every code add 3 given own off high page create power thing part url$ second function around seems though lot always best etc less give either having help x & likely \\mathcal means must form few made following least bit note small above trying order doing light end anything based done option file start % keep size maybe found us already she person world real > reason edit space 0    years check said little read solution process rather non day times list again field level someone once issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show him result course understand user makes open hard life group output fact last idea under human working her until name site whether amount pretty version water assume matter state kind past error method itself    got color    5 energy great original paper due sense else called character next general range getting god camera control ai current below low seem away particular values object free term consider left common 10 wo short words full difference i.e. test old area experience tried < changes online year thought    making although input book call available lower lens thus comes layer\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7273 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0028s vs `on_train_batch_end` time: 0.0104s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6769 - accuracy: 0.6037 - val_loss: 0.6682 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6658 - accuracy: 0.6219 - val_loss: 0.6572 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6580 - accuracy: 0.6488 - val_loss: 0.6521 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6486 - accuracy: 0.6612 - val_loss: 0.6504 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6525 - accuracy: 0.6525 - val_loss: 0.6486 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6593 - accuracy: 0.6369 - val_loss: 0.6468 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6481 - accuracy: 0.6575 - val_loss: 0.6456 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6532 - accuracy: 0.6419 - val_loss: 0.6459 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6561 - accuracy: 0.6400 - val_loss: 0.6438 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6400 - accuracy: 0.6606 - val_loss: 0.6474 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6333 - accuracy: 0.6706 - val_loss: 0.6396 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6514 - accuracy: 0.6406 - val_loss: 0.6385 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6412 - accuracy: 0.6550 - val_loss: 0.6375 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6425 - accuracy: 0.6494 - val_loss: 0.6371 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6396 - accuracy: 0.6562 - val_loss: 0.6356 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6298 - accuracy: 0.6625 - val_loss: 0.6359 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6313 - accuracy: 0.6669 - val_loss: 0.6353 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6292 - accuracy: 0.6644 - val_loss: 0.6341 - val_accuracy: 0.6581\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6463 - accuracy: 0.6469 - val_loss: 0.6314 - val_accuracy: 0.6526\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6218 - accuracy: 0.6762 - val_loss: 0.6451 - val_accuracy: 0.6526\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6289 - accuracy: 0.6594 - val_loss: 0.6305 - val_accuracy: 0.6526\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6572 - accuracy: 0.6363 - val_loss: 0.6348 - val_accuracy: 0.6845\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6469 - accuracy: 0.6456 - val_loss: 0.6318 - val_accuracy: 0.6821\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6332 - accuracy: 0.6513 - val_loss: 0.6287 - val_accuracy: 0.6581\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6319 - accuracy: 0.6750 - val_loss: 0.6333 - val_accuracy: 0.6526\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6441 - accuracy: 0.6519 - val_loss: 0.6278 - val_accuracy: 0.6593\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6418 - accuracy: 0.6606 - val_loss: 0.6277 - val_accuracy: 0.6683\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6474 - accuracy: 0.6463 - val_loss: 0.6280 - val_accuracy: 0.6538\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6312 - accuracy: 0.6631 - val_loss: 0.6276 - val_accuracy: 0.6821\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6365 - accuracy: 0.6556 - val_loss: 0.6271 - val_accuracy: 0.6857\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.8637 - accuracy: 0.4244\n",
      "training acc:  0.6556249856948853 , training loss:  0.636466920375824 , val acc:  0.6856971383094788 , val loss:  0.6271043419837952 , test acc:  0.4243917763233185 , test loss:  0.8637294769287109\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt1/450word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most much = might different example each such something he used 've question now well 're its good am however been after case set really number being find many since take say 2 problem still new who those both try possible did another too were between had why right things while probably point over answer go enough our + without better data value let through before change his actually ca back 'd system going 'll image long sure down able look every code add 3 given own off high page create power thing part url$ second function around seems though lot always best etc less give either having help x & likely \\mathcal means must form few made following least bit note small above trying order doing light end anything based done option file start % keep size maybe found us already she person world real > reason edit space 0    years check said little read solution process rather non day times list again field level someone once issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show him result course understand user makes open hard life group output fact last idea under human working her until name site whether amount pretty version water assume matter state kind past error method itself    got color    5 energy great original paper due sense else called character next general range getting god camera control ai current below low seem away particular values object free term consider left common 10 wo short words full difference i.e. test old area experience tried < changes online year thought    making although input book call available lower lens thus comes layer needs provide running within everything results feel hand almost source class ask e.g. easy perhaps top useful anyone \\\\ higher google big block created 4 post price access write several tell true further future others smaller search bytes clear nothing design exactly n multiple wrong three unless numbers avoid close\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6738 - accuracy: 0.6427WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0097s vs `on_test_batch_end` time: 0.0158s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6746 - accuracy: 0.6413 - val_loss: 0.6600 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6628 - accuracy: 0.6494 - val_loss: 0.6581 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6587 - accuracy: 0.6587 - val_loss: 0.6557 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6568 - accuracy: 0.6475 - val_loss: 0.6544 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6462 - accuracy: 0.6687 - val_loss: 0.6522 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6535 - accuracy: 0.6506 - val_loss: 0.6503 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6627 - accuracy: 0.6281 - val_loss: 0.6484 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6775 - accuracy: 0.5925 - val_loss: 0.6533 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6489 - accuracy: 0.6594 - val_loss: 0.6493 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6722 - accuracy: 0.6100 - val_loss: 0.6455 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6518 - accuracy: 0.6469 - val_loss: 0.6449 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6614 - accuracy: 0.6200 - val_loss: 0.6465 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6402 - accuracy: 0.6631 - val_loss: 0.6416 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6479 - accuracy: 0.6450 - val_loss: 0.6449 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6635 - accuracy: 0.6263 - val_loss: 0.6393 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6599 - accuracy: 0.6181 - val_loss: 0.6392 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6707 - accuracy: 0.6069 - val_loss: 0.6400 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6328 - accuracy: 0.6719 - val_loss: 0.6365 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6564 - accuracy: 0.6331 - val_loss: 0.6362 - val_accuracy: 0.6526\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6445 - accuracy: 0.6456 - val_loss: 0.6351 - val_accuracy: 0.6526\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6467 - accuracy: 0.6431 - val_loss: 0.6344 - val_accuracy: 0.6526\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6357 - accuracy: 0.6612 - val_loss: 0.6340 - val_accuracy: 0.6526\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6284 - accuracy: 0.6650 - val_loss: 0.6333 - val_accuracy: 0.6526\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6451 - accuracy: 0.6431 - val_loss: 0.6323 - val_accuracy: 0.6526\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6539 - accuracy: 0.6313 - val_loss: 0.6426 - val_accuracy: 0.6821\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6260 - accuracy: 0.6800 - val_loss: 0.6366 - val_accuracy: 0.6526\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6282 - accuracy: 0.6644 - val_loss: 0.6307 - val_accuracy: 0.6526\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6330 - accuracy: 0.6650 - val_loss: 0.6316 - val_accuracy: 0.6526\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6223 - accuracy: 0.6781 - val_loss: 0.6339 - val_accuracy: 0.6526\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6408 - accuracy: 0.6531 - val_loss: 0.6324 - val_accuracy: 0.6875\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.8174 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0142s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.8194 - accuracy: 0.4247\n",
      "training acc:  0.653124988079071 , training loss:  0.6408379077911377 , val acc:  0.6875 , val loss:  0.632388174533844 , test acc:  0.424706369638443 , test loss:  0.8194099068641663\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt1/500word_list.txt\n",
      "WORD INDEX STR: the to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most much = might different example each such something he used 've question now well 're its good am however been after case set really number being find many since take say 2 problem still new who those both try possible did another too were between had why right things while probably point over answer go enough our + without better data value let through before change his actually ca back 'd system going 'll image long sure down able look every code add 3 given own off high page create power thing part url$ second function around seems though lot always best etc less give either having help x & likely \\mathcal means must form few made following least bit note small above trying order doing light end anything based done option file start % keep size maybe found us already she person world real > reason edit space 0    years check said little read solution process rather non day times list again field level someone once issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show him result course understand user makes open hard life group output fact last idea under human working her until name site whether amount pretty version water assume matter state kind past error method itself    got color    5 energy great original paper due sense else called character next general range getting god camera control ai current below low seem away particular values object free term consider left common 10 wo short words full difference i.e. test old area experience tried < changes online year thought    making although input book call available lower lens thus comes layer needs provide running within everything results feel hand almost source class ask e.g. easy perhaps top useful anyone \\\\ higher google big block created 4 post price access write several tell true further future others smaller search bytes clear nothing design exactly n multiple wrong three unless numbers avoid close fine position ' effect days against believe # s mm sort certain language standard air actual account turn research difficult model yes takes _ length address product game uses normal gives known generally easily computer map cases along seen thanks sound money ones bad step cause terms whole mind rate\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.9362 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0228s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 67ms/step - loss: 0.7091 - accuracy: 0.5225 - val_loss: 0.6571 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6566 - accuracy: 0.6481 - val_loss: 0.6519 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6658 - accuracy: 0.6263 - val_loss: 0.6511 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6667 - accuracy: 0.6244 - val_loss: 0.6513 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6450 - accuracy: 0.6619 - val_loss: 0.6499 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6602 - accuracy: 0.6325 - val_loss: 0.6488 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6342 - accuracy: 0.6769 - val_loss: 0.6478 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6655 - accuracy: 0.6219 - val_loss: 0.6464 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6513 - accuracy: 0.6488 - val_loss: 0.6458 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6545 - accuracy: 0.6381 - val_loss: 0.6440 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6442 - accuracy: 0.6562 - val_loss: 0.6424 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6380 - accuracy: 0.6637 - val_loss: 0.6435 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6563 - accuracy: 0.6306 - val_loss: 0.6403 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6563 - accuracy: 0.6281 - val_loss: 0.6442 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6242 - accuracy: 0.6869 - val_loss: 0.6443 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6459 - accuracy: 0.6481 - val_loss: 0.6386 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6462 - accuracy: 0.6444 - val_loss: 0.6368 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6525 - accuracy: 0.6294 - val_loss: 0.6358 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6539 - accuracy: 0.6281 - val_loss: 0.6353 - val_accuracy: 0.6526\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6482 - accuracy: 0.6338 - val_loss: 0.6365 - val_accuracy: 0.6532\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6519 - accuracy: 0.6288 - val_loss: 0.6359 - val_accuracy: 0.6538\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6459 - accuracy: 0.6469 - val_loss: 0.6381 - val_accuracy: 0.6743\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.7993 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0135s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.7946 - accuracy: 0.4224\n",
      "training acc:  0.6468750238418579 , training loss:  0.6458846926689148 , val acc:  0.6742788553237915 , val loss:  0.6380599737167358 , test acc:  0.4223993420600891 , test loss:  0.7945986986160278\n",
      "\n",
      "opt2\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt2/50word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6675 - accuracy: 0.6256WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_test_batch_end` time: 0.0107s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6675 - accuracy: 0.6256 - val_loss: 0.6560 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6601 - accuracy: 0.6431 - val_loss: 0.6558 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6641 - accuracy: 0.6331 - val_loss: 0.6529 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6470 - accuracy: 0.6669 - val_loss: 0.6510 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6664 - accuracy: 0.6338 - val_loss: 0.6499 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6654 - accuracy: 0.6256 - val_loss: 0.6532 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6503 - accuracy: 0.6538 - val_loss: 0.6546 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6575 - accuracy: 0.6425 - val_loss: 0.6479 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6544 - accuracy: 0.6463 - val_loss: 0.6466 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6362 - accuracy: 0.6719 - val_loss: 0.6457 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6444 - accuracy: 0.6562 - val_loss: 0.6495 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6668 - accuracy: 0.6194 - val_loss: 0.6455 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6524 - accuracy: 0.6431 - val_loss: 0.6447 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6478 - accuracy: 0.6488 - val_loss: 0.6428 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6548 - accuracy: 0.6331 - val_loss: 0.6453 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6539 - accuracy: 0.6325 - val_loss: 0.6422 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6454 - accuracy: 0.6475 - val_loss: 0.6403 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6613 - accuracy: 0.6244 - val_loss: 0.6394 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6437 - accuracy: 0.6469 - val_loss: 0.6385 - val_accuracy: 0.6526\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6496 - accuracy: 0.6381 - val_loss: 0.6378 - val_accuracy: 0.6526\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6543 - accuracy: 0.6288 - val_loss: 0.6379 - val_accuracy: 0.6526\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6442 - accuracy: 0.6388 - val_loss: 0.6378 - val_accuracy: 0.6526\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6536 - accuracy: 0.6306 - val_loss: 0.6358 - val_accuracy: 0.6526\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6318 - accuracy: 0.6644 - val_loss: 0.6362 - val_accuracy: 0.6526\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6499 - accuracy: 0.6369 - val_loss: 0.6355 - val_accuracy: 0.6526\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6430 - accuracy: 0.6513 - val_loss: 0.6342 - val_accuracy: 0.6526\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6494 - accuracy: 0.6288 - val_loss: 0.6350 - val_accuracy: 0.6526\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6307 - accuracy: 0.6644 - val_loss: 0.6331 - val_accuracy: 0.6526\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6268 - accuracy: 0.6731 - val_loss: 0.6327 - val_accuracy: 0.6526\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6693 - accuracy: 0.6075 - val_loss: 0.6381 - val_accuracy: 0.6562\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.8153 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0119s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.8018 - accuracy: 0.4134\n",
      "training acc:  0.6075000166893005 , training loss:  0.6692646741867065 , val acc:  0.65625 , val loss:  0.6380918025970459 , test acc:  0.41338086128234863 , test loss:  0.8018208742141724\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt2/100word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 1.6539 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0026s vs `on_train_batch_end` time: 0.0104s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6798 - accuracy: 0.6513 - val_loss: 0.6618 - val_accuracy: 0.6532\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6487 - accuracy: 0.6719 - val_loss: 0.6611 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6624 - accuracy: 0.6544 - val_loss: 0.6564 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6553 - accuracy: 0.6550 - val_loss: 0.6543 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6741 - accuracy: 0.6131 - val_loss: 0.6550 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6623 - accuracy: 0.6394 - val_loss: 0.6531 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6492 - accuracy: 0.6606 - val_loss: 0.6486 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6521 - accuracy: 0.6525 - val_loss: 0.6531 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6562 - accuracy: 0.6450 - val_loss: 0.6494 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6258 - accuracy: 0.6888 - val_loss: 0.6540 - val_accuracy: 0.6526\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.0000 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0141s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.8517 - accuracy: 0.4123\n",
      "training acc:  0.6887500286102295 , training loss:  0.6257585883140564 , val acc:  0.6526442170143127 , val loss:  0.6540146470069885 , test acc:  0.4123322069644928 , test loss:  0.8516936898231506\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt2/150word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most much = might different example each such something he used 've question now well 're its good am however been after case set really number being find many since take say 2 problem still new who those both try possible did another too were between had why right\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7547 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_train_batch_end` time: 0.0133s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6677 - accuracy: 0.6281 - val_loss: 0.6553 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6566 - accuracy: 0.6506 - val_loss: 0.6599 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6636 - accuracy: 0.6450 - val_loss: 0.6587 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6565 - accuracy: 0.6569 - val_loss: 0.6517 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6463 - accuracy: 0.6681 - val_loss: 0.6566 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6747 - accuracy: 0.6237 - val_loss: 0.6494 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6721 - accuracy: 0.6075 - val_loss: 0.6482 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6532 - accuracy: 0.6481 - val_loss: 0.6473 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6677 - accuracy: 0.6144 - val_loss: 0.6515 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6492 - accuracy: 0.6519 - val_loss: 0.6456 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6483 - accuracy: 0.6450 - val_loss: 0.6569 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6671 - accuracy: 0.6250 - val_loss: 0.6441 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6238 - accuracy: 0.6850 - val_loss: 0.6490 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6620 - accuracy: 0.6300 - val_loss: 0.6421 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6197 - accuracy: 0.6925 - val_loss: 0.6458 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6623 - accuracy: 0.6269 - val_loss: 0.6407 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6571 - accuracy: 0.6294 - val_loss: 0.6389 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6514 - accuracy: 0.6381 - val_loss: 0.6414 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6526 - accuracy: 0.6263 - val_loss: 0.6371 - val_accuracy: 0.6526\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6336 - accuracy: 0.6650 - val_loss: 0.6380 - val_accuracy: 0.6526\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6358 - accuracy: 0.6575 - val_loss: 0.6357 - val_accuracy: 0.6526\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6542 - accuracy: 0.6300 - val_loss: 0.6347 - val_accuracy: 0.6526\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6325 - accuracy: 0.6600 - val_loss: 0.6339 - val_accuracy: 0.6526\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6356 - accuracy: 0.6575 - val_loss: 0.6384 - val_accuracy: 0.6569\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6433 - accuracy: 0.6444 - val_loss: 0.6320 - val_accuracy: 0.6526\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6348 - accuracy: 0.6550 - val_loss: 0.6349 - val_accuracy: 0.6526\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6373 - accuracy: 0.6475 - val_loss: 0.6314 - val_accuracy: 0.6526\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6353 - accuracy: 0.6550 - val_loss: 0.6380 - val_accuracy: 0.6929\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6178 - accuracy: 0.6787 - val_loss: 0.6294 - val_accuracy: 0.6526\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6509 - accuracy: 0.6331 - val_loss: 0.6331 - val_accuracy: 0.6755\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.8210 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_test_batch_end` time: 0.0191s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.8179 - accuracy: 0.4247\n",
      "training acc:  0.6331250071525574 , training loss:  0.6509214043617249 , val acc:  0.6754807829856873 , val loss:  0.6330805420875549 , test acc:  0.424706369638443 , test loss:  0.8178592324256897\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt2/200word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most much = might different example each such something he used 've question now well 're its good am however been after case set really number being find many since take say 2 problem still new who those both try possible did another too were between had why right things while probably point over answer go enough our + without better data value let through before change his actually ca back 'd system going 'll image long sure down able look every code add 3 given own off high page create power thing part url$ second function around seems\n",
      "Epoch 1/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6873 - accuracy: 0.5789 ETA: 0s - loss:WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0106s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6857 - accuracy: 0.5819 - val_loss: 0.6535 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6608 - accuracy: 0.6413 - val_loss: 0.6522 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6464 - accuracy: 0.6612 - val_loss: 0.6584 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6594 - accuracy: 0.6406 - val_loss: 0.6545 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6511 - accuracy: 0.6506 - val_loss: 0.6497 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6471 - accuracy: 0.6581 - val_loss: 0.6479 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6552 - accuracy: 0.6425 - val_loss: 0.6468 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6613 - accuracy: 0.6275 - val_loss: 0.6518 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6540 - accuracy: 0.6450 - val_loss: 0.6475 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6586 - accuracy: 0.6300 - val_loss: 0.6457 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6397 - accuracy: 0.6662 - val_loss: 0.6435 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6598 - accuracy: 0.6313 - val_loss: 0.6430 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6461 - accuracy: 0.6506 - val_loss: 0.6449 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6424 - accuracy: 0.6550 - val_loss: 0.6416 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6551 - accuracy: 0.6338 - val_loss: 0.6402 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6270 - accuracy: 0.6787 - val_loss: 0.6411 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6577 - accuracy: 0.6219 - val_loss: 0.6382 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6370 - accuracy: 0.6631 - val_loss: 0.6391 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6550 - accuracy: 0.6294 - val_loss: 0.6379 - val_accuracy: 0.6526\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6197 - accuracy: 0.6819 - val_loss: 0.6366 - val_accuracy: 0.6526\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6356 - accuracy: 0.6550 - val_loss: 0.6358 - val_accuracy: 0.6526\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6459 - accuracy: 0.6325 - val_loss: 0.6347 - val_accuracy: 0.6526\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6501 - accuracy: 0.6237 - val_loss: 0.6350 - val_accuracy: 0.6526\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6438 - accuracy: 0.6375 - val_loss: 0.6324 - val_accuracy: 0.6526\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6394 - accuracy: 0.6413 - val_loss: 0.6314 - val_accuracy: 0.6526\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6387 - accuracy: 0.6450 - val_loss: 0.6307 - val_accuracy: 0.6526\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6458 - accuracy: 0.6319 - val_loss: 0.6340 - val_accuracy: 0.6611\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6416 - accuracy: 0.6463 - val_loss: 0.6310 - val_accuracy: 0.6532\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6315 - accuracy: 0.6587 - val_loss: 0.6306 - val_accuracy: 0.6526\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6481 - accuracy: 0.6256 - val_loss: 0.6289 - val_accuracy: 0.6526\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.8752 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0133s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.8440 - accuracy: 0.4123\n",
      "training acc:  0.6256250143051147 , training loss:  0.6481090784072876 , val acc:  0.6526442170143127 , val loss:  0.6289262175559998 , test acc:  0.4123322069644928 , test loss:  0.8439949750900269\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt2/250word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most much = might different example each such something he used 've question now well 're its good am however been after case set really number being find many since take say 2 problem still new who those both try possible did another too were between had why right things while probably point over answer go enough our + without better data value let through before change his actually ca back 'd system going 'll image long sure down able look every code add 3 given own off high page create power thing part url$ second function around seems though lot always best etc less give either having help x & likely \\mathcal means must form few made following least bit note small above trying order doing light end anything based done option file start % keep size maybe found us already she person world real > reason edit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6937 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0108s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6805 - accuracy: 0.6094 - val_loss: 0.6591 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6500 - accuracy: 0.6619 - val_loss: 0.6549 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6678 - accuracy: 0.6300 - val_loss: 0.6536 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6611 - accuracy: 0.6369 - val_loss: 0.6530 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6616 - accuracy: 0.6338 - val_loss: 0.6549 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6490 - accuracy: 0.6600 - val_loss: 0.6526 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6457 - accuracy: 0.6644 - val_loss: 0.6535 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6708 - accuracy: 0.6081 - val_loss: 0.6544 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6532 - accuracy: 0.6425 - val_loss: 0.6501 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6544 - accuracy: 0.6406 - val_loss: 0.6458 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6436 - accuracy: 0.6600 - val_loss: 0.6437 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6511 - accuracy: 0.6450 - val_loss: 0.6424 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6424 - accuracy: 0.6594 - val_loss: 0.6425 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6472 - accuracy: 0.6450 - val_loss: 0.6404 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6687 - accuracy: 0.6106 - val_loss: 0.6398 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6325 - accuracy: 0.6681 - val_loss: 0.6384 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6392 - accuracy: 0.6575 - val_loss: 0.6386 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6297 - accuracy: 0.6731 - val_loss: 0.6412 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6448 - accuracy: 0.6450 - val_loss: 0.6389 - val_accuracy: 0.6526\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.8185 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0151s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7902 - accuracy: 0.4123\n",
      "training acc:  0.6449999809265137 , training loss:  0.6447849869728088 , val acc:  0.6526442170143127 , val loss:  0.6389027833938599 , test acc:  0.4123322069644928 , test loss:  0.7902416586875916\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt2/300word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most much = might different example each such something he used 've question now well 're its good am however been after case set really number being find many since take say 2 problem still new who those both try possible did another too were between had why right things while probably point over answer go enough our + without better data value let through before change his actually ca back 'd system going 'll image long sure down able look every code add 3 given own off high page create power thing part url$ second function around seems though lot always best etc less give either having help x & likely \\mathcal means must form few made following least bit note small above trying order doing light end anything based done option file start % keep size maybe found us already she person world real > reason edit space 0    years check said little read solution process rather non day times list again field level someone once issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 1.7855 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0081s vs `on_train_batch_end` time: 0.0148s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.7976 - accuracy: 0.6313 - val_loss: 0.6679 - val_accuracy: 0.6514\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6773 - accuracy: 0.6294 - val_loss: 0.6657 - val_accuracy: 0.6532\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6866 - accuracy: 0.6112 - val_loss: 0.6645 - val_accuracy: 0.6532\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6670 - accuracy: 0.6481 - val_loss: 0.6619 - val_accuracy: 0.6532\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6686 - accuracy: 0.6438 - val_loss: 0.6600 - val_accuracy: 0.6532\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6655 - accuracy: 0.6363 - val_loss: 0.6588 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6599 - accuracy: 0.6544 - val_loss: 0.6575 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6645 - accuracy: 0.6388 - val_loss: 0.6560 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6610 - accuracy: 0.6456 - val_loss: 0.6557 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6697 - accuracy: 0.6219 - val_loss: 0.6553 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6632 - accuracy: 0.6388 - val_loss: 0.6520 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6633 - accuracy: 0.6325 - val_loss: 0.6492 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6557 - accuracy: 0.6431 - val_loss: 0.6480 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6524 - accuracy: 0.6438 - val_loss: 0.6500 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6406 - accuracy: 0.6694 - val_loss: 0.6468 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6624 - accuracy: 0.6237 - val_loss: 0.6483 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6466 - accuracy: 0.6538 - val_loss: 0.6423 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6536 - accuracy: 0.6381 - val_loss: 0.6452 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6398 - accuracy: 0.6606 - val_loss: 0.6426 - val_accuracy: 0.6526\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6186 - accuracy: 0.6938 - val_loss: 0.6443 - val_accuracy: 0.6526\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.9712 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_test_batch_end` time: 0.0165s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.8493 - accuracy: 0.4123\n",
      "training acc:  0.6937500238418579 , training loss:  0.6185700297355652 , val acc:  0.6526442170143127 , val loss:  0.6443253755569458 , test acc:  0.4123322069644928 , test loss:  0.84927898645401\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt2/350word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most much = might different example each such something he used 've question now well 're its good am however been after case set really number being find many since take say 2 problem still new who those both try possible did another too were between had why right things while probably point over answer go enough our + without better data value let through before change his actually ca back 'd system going 'll image long sure down able look every code add 3 given own off high page create power thing part url$ second function around seems though lot always best etc less give either having help x & likely \\mathcal means must form few made following least bit note small above trying order doing light end anything based done option file start % keep size maybe found us already she person world real > reason edit space 0    years check said little read solution process rather non day times list again field level someone once issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show him result course understand user makes open hard life group output fact last idea under human working her until name site whether amount pretty version water assume matter state kind past error method itself    got color    5 energy great original paper due sense else\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6688 - accuracy: 0.6350 - val_loss: 0.6579 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6633 - accuracy: 0.6419 - val_loss: 0.6645 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6506 - accuracy: 0.6650 - val_loss: 0.6547 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6705 - accuracy: 0.6175 - val_loss: 0.6755 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6664 - accuracy: 0.6237 - val_loss: 0.6564 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6791 - accuracy: 0.6037 - val_loss: 0.6546 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6641 - accuracy: 0.6350 - val_loss: 0.6499 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6516 - accuracy: 0.6525 - val_loss: 0.6483 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6627 - accuracy: 0.6294 - val_loss: 0.6486 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6564 - accuracy: 0.6419 - val_loss: 0.6481 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6515 - accuracy: 0.6488 - val_loss: 0.6468 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6479 - accuracy: 0.6506 - val_loss: 0.6482 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6640 - accuracy: 0.6212 - val_loss: 0.6455 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6473 - accuracy: 0.6519 - val_loss: 0.6425 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6301 - accuracy: 0.6775 - val_loss: 0.6426 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6388 - accuracy: 0.6600 - val_loss: 0.6440 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6628 - accuracy: 0.6256 - val_loss: 0.6400 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6555 - accuracy: 0.6281 - val_loss: 0.6395 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6552 - accuracy: 0.6281 - val_loss: 0.6387 - val_accuracy: 0.6526\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6486 - accuracy: 0.6450 - val_loss: 0.6480 - val_accuracy: 0.6526\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6487 - accuracy: 0.6406 - val_loss: 0.6379 - val_accuracy: 0.6526\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6374 - accuracy: 0.6550 - val_loss: 0.6354 - val_accuracy: 0.6526\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6444 - accuracy: 0.6419 - val_loss: 0.6399 - val_accuracy: 0.6532\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6466 - accuracy: 0.6400 - val_loss: 0.6340 - val_accuracy: 0.6526\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6566 - accuracy: 0.6294 - val_loss: 0.6339 - val_accuracy: 0.6526\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6413 - accuracy: 0.6438 - val_loss: 0.6331 - val_accuracy: 0.6526\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6424 - accuracy: 0.6456 - val_loss: 0.6345 - val_accuracy: 0.6532\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6455 - accuracy: 0.6612 - val_loss: 0.6316 - val_accuracy: 0.6526\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6406 - accuracy: 0.6444 - val_loss: 0.6313 - val_accuracy: 0.6526\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6320 - accuracy: 0.6637 - val_loss: 0.6323 - val_accuracy: 0.6526\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.9442 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_test_batch_end` time: 0.0056s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.8675 - accuracy: 0.4123\n",
      "training acc:  0.6637499928474426 , training loss:  0.6320132613182068 , val acc:  0.6526442170143127 , val loss:  0.6322540044784546 , test acc:  0.4123322069644928 , test loss:  0.8674538731575012\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt2/400word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most much = might different example each such something he used 've question now well 're its good am however been after case set really number being find many since take say 2 problem still new who those both try possible did another too were between had why right things while probably point over answer go enough our + without better data value let through before change his actually ca back 'd system going 'll image long sure down able look every code add 3 given own off high page create power thing part url$ second function around seems though lot always best etc less give either having help x & likely \\mathcal means must form few made following least bit note small above trying order doing light end anything based done option file start % keep size maybe found us already she person world real > reason edit space 0    years check said little read solution process rather non day times list again field level someone once issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show him result course understand user makes open hard life group output fact last idea under human working her until name site whether amount pretty version water assume matter state kind past error method itself    got color    5 energy great original paper due sense else called character next general range getting god camera control ai current below low seem away particular values object free term consider left common 10 wo short words full difference i.e. test old area experience tried < changes online year thought    making although input book call available lower lens thus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7580 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0152s). Check your callbacks.\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6724 - accuracy: 0.6395WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_test_batch_end` time: 0.0151s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6744 - accuracy: 0.6356 - val_loss: 0.6597 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6814 - accuracy: 0.5994 - val_loss: 0.6691 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6710 - accuracy: 0.6263 - val_loss: 0.6662 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6674 - accuracy: 0.6281 - val_loss: 0.6555 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6659 - accuracy: 0.6306 - val_loss: 0.6544 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6494 - accuracy: 0.6637 - val_loss: 0.6518 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6562 - accuracy: 0.6475 - val_loss: 0.6532 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6605 - accuracy: 0.6350 - val_loss: 0.6497 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6636 - accuracy: 0.6281 - val_loss: 0.6476 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6577 - accuracy: 0.6394 - val_loss: 0.6497 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6519 - accuracy: 0.6431 - val_loss: 0.6618 - val_accuracy: 0.6532\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6505 - accuracy: 0.6550 - val_loss: 0.6438 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6530 - accuracy: 0.6419 - val_loss: 0.6423 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6447 - accuracy: 0.6513 - val_loss: 0.6452 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6534 - accuracy: 0.6356 - val_loss: 0.6439 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6310 - accuracy: 0.6731 - val_loss: 0.6462 - val_accuracy: 0.6526\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.9794 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_test_batch_end` time: 0.0143s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.8545 - accuracy: 0.4123\n",
      "training acc:  0.6731250286102295 , training loss:  0.631018340587616 , val acc:  0.6526442170143127 , val loss:  0.6462351083755493 , test acc:  0.4123322069644928 , test loss:  0.8544506430625916\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt2/450word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most much = might different example each such something he used 've question now well 're its good am however been after case set really number being find many since take say 2 problem still new who those both try possible did another too were between had why right things while probably point over answer go enough our + without better data value let through before change his actually ca back 'd system going 'll image long sure down able look every code add 3 given own off high page create power thing part url$ second function around seems though lot always best etc less give either having help x & likely \\mathcal means must form few made following least bit note small above trying order doing light end anything based done option file start % keep size maybe found us already she person world real > reason edit space 0    years check said little read solution process rather non day times list again field level someone once issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show him result course understand user makes open hard life group output fact last idea under human working her until name site whether amount pretty version water assume matter state kind past error method itself    got color    5 energy great original paper due sense else called character next general range getting god camera control ai current below low seem away particular values object free term consider left common 10 wo short words full difference i.e. test old area experience tried < changes online year thought    making although input book call available lower lens thus comes layer needs provide running within everything results feel hand almost source class ask e.g. easy perhaps top useful anyone \\\\ higher google big block created 4 post price access write several tell true further future others smaller search bytes clear nothing design exactly n multiple wrong three unless numbers\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7847 - accuracy: 0.0625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_train_batch_end` time: 0.0178s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6593 - accuracy: 0.6338 - val_loss: 0.6561 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6536 - accuracy: 0.6550 - val_loss: 0.6553 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6689 - accuracy: 0.6275 - val_loss: 0.6527 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6597 - accuracy: 0.6431 - val_loss: 0.6514 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6559 - accuracy: 0.6450 - val_loss: 0.6494 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6598 - accuracy: 0.6375 - val_loss: 0.6526 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6617 - accuracy: 0.6263 - val_loss: 0.6517 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6563 - accuracy: 0.6400 - val_loss: 0.6452 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6485 - accuracy: 0.6500 - val_loss: 0.6441 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6573 - accuracy: 0.6338 - val_loss: 0.6426 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6359 - accuracy: 0.6681 - val_loss: 0.6425 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6632 - accuracy: 0.6131 - val_loss: 0.6465 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6563 - accuracy: 0.6288 - val_loss: 0.6417 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6476 - accuracy: 0.6450 - val_loss: 0.6382 - val_accuracy: 0.6526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6599 - accuracy: 0.6250 - val_loss: 0.6428 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6400 - accuracy: 0.6538 - val_loss: 0.6369 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6411 - accuracy: 0.6494 - val_loss: 0.6378 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6274 - accuracy: 0.6750 - val_loss: 0.6413 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6381 - accuracy: 0.6481 - val_loss: 0.6333 - val_accuracy: 0.6526\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6371 - accuracy: 0.6531 - val_loss: 0.6328 - val_accuracy: 0.6526\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6387 - accuracy: 0.6475 - val_loss: 0.6350 - val_accuracy: 0.6562\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6454 - accuracy: 0.6338 - val_loss: 0.6339 - val_accuracy: 0.6593\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6456 - accuracy: 0.6350 - val_loss: 0.6362 - val_accuracy: 0.6857\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.8041 - accuracy: 0.4285\n",
      "training acc:  0.6349999904632568 , training loss:  0.6455557346343994 , val acc:  0.6856971383094788 , val loss:  0.6362046599388123 , test acc:  0.4284815490245819 , test loss:  0.8040848970413208\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt2/500word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is i in that you it $ for be this   with as on are have if not or can but 's your would n't do an so from there will they at by one my which what more we all some when was then like /   has use just also about any get only could how time no other than out them up does need should way their make same where using 'm work because want - very even think into first these here see know people two may 1 me most much = might different example each such something he used 've question now well 're its good am however been after case set really number being find many since take say 2 problem still new who those both try possible did another too were between had why right things while probably point over answer go enough our + without better data value let through before change his actually ca back 'd system going 'll image long sure down able look every code add 3 given own off high page create power thing part url$ second function around seems though lot always best etc less give either having help x & likely \\mathcal means must form few made following least bit note small above trying order doing light end anything based done option file start % keep size maybe found us already she person world real > reason edit space 0    years check said little read solution process rather non day times list again field level someone once issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show him result course understand user makes open hard life group output fact last idea under human working her until name site whether amount pretty version water assume matter state kind past error method itself    got color    5 energy great original paper due sense else called character next general range getting god camera control ai current below low seem away particular values object free term consider left common 10 wo short words full difference i.e. test old area experience tried < changes online year thought    making although input book call available lower lens thus comes layer needs provide running within everything results feel hand almost source class ask e.g. easy perhaps top useful anyone \\\\ higher google big block created 4 post price access write several tell true further future others smaller search bytes clear nothing design exactly n multiple wrong three unless numbers avoid close fine position ' effect days against believe # s mm sort certain language standard air actual account turn research difficult model yes takes _ length address product game uses normal gives known generally easily computer map cases along seen thanks sound money ones bad step cause terms whole\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.7458 - accuracy: 0.2188WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0156s vs `on_train_batch_end` time: 0.0359s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6620 - accuracy: 0.6331 - val_loss: 0.6557 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6642 - accuracy: 0.6425 - val_loss: 0.6543 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6645 - accuracy: 0.6363 - val_loss: 0.6543 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6454 - accuracy: 0.6719 - val_loss: 0.6509 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6703 - accuracy: 0.6169 - val_loss: 0.6533 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6546 - accuracy: 0.6413 - val_loss: 0.6500 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6582 - accuracy: 0.6369 - val_loss: 0.6465 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6465 - accuracy: 0.6575 - val_loss: 0.6465 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6601 - accuracy: 0.6331 - val_loss: 0.6472 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6364 - accuracy: 0.6681 - val_loss: 0.6437 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6539 - accuracy: 0.6388 - val_loss: 0.6449 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6611 - accuracy: 0.6269 - val_loss: 0.6412 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6430 - accuracy: 0.6556 - val_loss: 0.6403 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6400 - accuracy: 0.6575 - val_loss: 0.6395 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6416 - accuracy: 0.6500 - val_loss: 0.6396 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6571 - accuracy: 0.6225 - val_loss: 0.6380 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6522 - accuracy: 0.6325 - val_loss: 0.6484 - val_accuracy: 0.6941\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6439 - accuracy: 0.6513 - val_loss: 0.6348 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6401 - accuracy: 0.6569 - val_loss: 0.6338 - val_accuracy: 0.6526\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6468 - accuracy: 0.6388 - val_loss: 0.6354 - val_accuracy: 0.6526\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6430 - accuracy: 0.6456 - val_loss: 0.6337 - val_accuracy: 0.6526\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6402 - accuracy: 0.6581 - val_loss: 0.6380 - val_accuracy: 0.6785\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6538 - accuracy: 0.6331 - val_loss: 0.6307 - val_accuracy: 0.6526\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6295 - accuracy: 0.6619 - val_loss: 0.6318 - val_accuracy: 0.6526\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6567 - accuracy: 0.6231 - val_loss: 0.6386 - val_accuracy: 0.6977\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6525 - accuracy: 0.6425 - val_loss: 0.6285 - val_accuracy: 0.6526\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6177 - accuracy: 0.6762 - val_loss: 0.6327 - val_accuracy: 0.6526\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6163 - accuracy: 0.6762 - val_loss: 0.6271 - val_accuracy: 0.6526\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6408 - accuracy: 0.6456 - val_loss: 0.6272 - val_accuracy: 0.6617\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6313 - accuracy: 0.6594 - val_loss: 0.6263 - val_accuracy: 0.6526\n",
      "  1/596 [..............................] - ETA: 9s - loss: 0.9162 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0104s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.8733 - accuracy: 0.4124\n",
      "training acc:  0.659375011920929 , training loss:  0.6313315629959106 , val acc:  0.6526442170143127 , val loss:  0.6263030171394348 , test acc:  0.4124370813369751 , test loss:  0.8732920289039612\n",
      "\n",
      "opt3\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt3/50word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.9396 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0108s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.7080 - accuracy: 0.5487 - val_loss: 0.6535 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6420 - accuracy: 0.6775 - val_loss: 0.6433 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6408 - accuracy: 0.6438 - val_loss: 0.6349 - val_accuracy: 0.6532\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6338 - accuracy: 0.6556 - val_loss: 0.6298 - val_accuracy: 0.6677\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6335 - accuracy: 0.6669 - val_loss: 0.6254 - val_accuracy: 0.6550\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6434 - accuracy: 0.6275 - val_loss: 0.6230 - val_accuracy: 0.6689\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6124 - accuracy: 0.6862 - val_loss: 0.6212 - val_accuracy: 0.6701\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6335 - accuracy: 0.6413 - val_loss: 0.6207 - val_accuracy: 0.6839\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6060 - accuracy: 0.6925 - val_loss: 0.6195 - val_accuracy: 0.6647\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6127 - accuracy: 0.6650 - val_loss: 0.6179 - val_accuracy: 0.6731\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6225 - accuracy: 0.6556 - val_loss: 0.6165 - val_accuracy: 0.6815\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6061 - accuracy: 0.6850 - val_loss: 0.6166 - val_accuracy: 0.6797\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6270 - accuracy: 0.6631 - val_loss: 0.6165 - val_accuracy: 0.6803\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6145 - accuracy: 0.6719 - val_loss: 0.6161 - val_accuracy: 0.6857\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6372 - accuracy: 0.6463 - val_loss: 0.6161 - val_accuracy: 0.6803\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6290 - accuracy: 0.6681 - val_loss: 0.6171 - val_accuracy: 0.6773\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6340 - accuracy: 0.6562 - val_loss: 0.6160 - val_accuracy: 0.6791\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6238 - accuracy: 0.6562 - val_loss: 0.6197 - val_accuracy: 0.6623\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6533 - accuracy: 0.6288 - val_loss: 0.6179 - val_accuracy: 0.6839\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6251 - accuracy: 0.6594 - val_loss: 0.6195 - val_accuracy: 0.6629\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.0133 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0156s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.8292 - accuracy: 0.4277\n",
      "training acc:  0.659375011920929 , training loss:  0.6251063346862793 , val acc:  0.6628605723381042 , val loss:  0.6195173263549805 , test acc:  0.4277474880218506 , test loss:  0.8292344212532043\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt3/100word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.5762 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0167s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.7156 - accuracy: 0.5975 - val_loss: 0.6845 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6744 - accuracy: 0.6400 - val_loss: 0.6586 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6538 - accuracy: 0.6506 - val_loss: 0.6431 - val_accuracy: 0.6556\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6481 - accuracy: 0.6612 - val_loss: 0.6327 - val_accuracy: 0.6689\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6312 - accuracy: 0.6544 - val_loss: 0.6272 - val_accuracy: 0.6875\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6398 - accuracy: 0.6494 - val_loss: 0.6257 - val_accuracy: 0.6875\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6289 - accuracy: 0.6594 - val_loss: 0.6204 - val_accuracy: 0.6689\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6359 - accuracy: 0.6450 - val_loss: 0.6201 - val_accuracy: 0.6899\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6401 - accuracy: 0.6331 - val_loss: 0.6197 - val_accuracy: 0.6893\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6365 - accuracy: 0.6569 - val_loss: 0.6189 - val_accuracy: 0.6881\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6481 - accuracy: 0.6463 - val_loss: 0.6180 - val_accuracy: 0.6899\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6034 - accuracy: 0.7088 - val_loss: 0.6150 - val_accuracy: 0.6803\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6460 - accuracy: 0.6375 - val_loss: 0.6183 - val_accuracy: 0.6887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6235 - accuracy: 0.6562 - val_loss: 0.6163 - val_accuracy: 0.6689\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6306 - accuracy: 0.6681 - val_loss: 0.6147 - val_accuracy: 0.6803\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6379 - accuracy: 0.6475 - val_loss: 0.6142 - val_accuracy: 0.6857\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6404 - accuracy: 0.6350 - val_loss: 0.6144 - val_accuracy: 0.6875\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6391 - accuracy: 0.6369 - val_loss: 0.6149 - val_accuracy: 0.6881\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6092 - accuracy: 0.6794 - val_loss: 0.6181 - val_accuracy: 0.6683\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.0520 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0152s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.8240 - accuracy: 0.4254\n",
      "training acc:  0.6793749928474426 , training loss:  0.6092303991317749 , val acc:  0.6682692170143127 , val loss:  0.6181173324584961 , test acc:  0.4254404306411743 , test loss:  0.8239631056785583\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt3/150word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second function around seems though lot always best etc less give either help x & likely \\mathcal means must form made following least bit note small trying order light end anything based done option file start % keep size maybe found us already person world real > reason edit space 0\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6955 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_train_batch_end` time: 0.0107s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6805 - accuracy: 0.6000 - val_loss: 0.6579 - val_accuracy: 0.6665\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6598 - accuracy: 0.6294 - val_loss: 0.6453 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6484 - accuracy: 0.6356 - val_loss: 0.6327 - val_accuracy: 0.6587\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6439 - accuracy: 0.6488 - val_loss: 0.6256 - val_accuracy: 0.6701\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6289 - accuracy: 0.6550 - val_loss: 0.6234 - val_accuracy: 0.6544\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6428 - accuracy: 0.6444 - val_loss: 0.6198 - val_accuracy: 0.6695\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6335 - accuracy: 0.6594 - val_loss: 0.6175 - val_accuracy: 0.6809\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6322 - accuracy: 0.6388 - val_loss: 0.6163 - val_accuracy: 0.6869\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6326 - accuracy: 0.6506 - val_loss: 0.6167 - val_accuracy: 0.6707\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6200 - accuracy: 0.6669 - val_loss: 0.6186 - val_accuracy: 0.6905\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6342 - accuracy: 0.6513 - val_loss: 0.6149 - val_accuracy: 0.6923\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6378 - accuracy: 0.6450 - val_loss: 0.6151 - val_accuracy: 0.6785\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6235 - accuracy: 0.6606 - val_loss: 0.6146 - val_accuracy: 0.6929\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6284 - accuracy: 0.6675 - val_loss: 0.6177 - val_accuracy: 0.6683\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6351 - accuracy: 0.6538 - val_loss: 0.6147 - val_accuracy: 0.6893\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6329 - accuracy: 0.6587 - val_loss: 0.6140 - val_accuracy: 0.6875\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6233 - accuracy: 0.6669 - val_loss: 0.6154 - val_accuracy: 0.6701\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6385 - accuracy: 0.6394 - val_loss: 0.6157 - val_accuracy: 0.6899\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6237 - accuracy: 0.6612 - val_loss: 0.6163 - val_accuracy: 0.6689\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.0422 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0152s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.8209 - accuracy: 0.4308\n",
      "training acc:  0.6612499952316284 , training loss:  0.6237415075302124 , val acc:  0.668870210647583 , val loss:  0.6162843108177185 , test acc:  0.4307885766029358 , test loss:  0.8209152817726135\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt3/200word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second function around seems though lot always best etc less give either help x & likely \\mathcal means must form made following least bit note small trying order light end anything based done option file start % keep size maybe found us already person world real > reason edit space 0    years check said little read solution process rather non day times list field level someone issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6895 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0128s). Check your callbacks.\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6834 - accuracy: 0.5856WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_test_batch_end` time: 0.0107s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6834 - accuracy: 0.5856 - val_loss: 0.6619 - val_accuracy: 0.6665\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6561 - accuracy: 0.6475 - val_loss: 0.6468 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6500 - accuracy: 0.6263 - val_loss: 0.6346 - val_accuracy: 0.6713\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6345 - accuracy: 0.6494 - val_loss: 0.6268 - val_accuracy: 0.6550\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6199 - accuracy: 0.6675 - val_loss: 0.6232 - val_accuracy: 0.6550\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6300 - accuracy: 0.6531 - val_loss: 0.6191 - val_accuracy: 0.6701\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6573 - accuracy: 0.6344 - val_loss: 0.6182 - val_accuracy: 0.6947\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6220 - accuracy: 0.6725 - val_loss: 0.6205 - val_accuracy: 0.6611\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6420 - accuracy: 0.6375 - val_loss: 0.6161 - val_accuracy: 0.6857\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6360 - accuracy: 0.6644 - val_loss: 0.6212 - val_accuracy: 0.6851\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6226 - accuracy: 0.6687 - val_loss: 0.6160 - val_accuracy: 0.6713\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6416 - accuracy: 0.6500 - val_loss: 0.6145 - val_accuracy: 0.6803\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6517 - accuracy: 0.6381 - val_loss: 0.6169 - val_accuracy: 0.6713\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6216 - accuracy: 0.6544 - val_loss: 0.6184 - val_accuracy: 0.6671\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6290 - accuracy: 0.6637 - val_loss: 0.6154 - val_accuracy: 0.6719\n",
      "  1/596 [..............................] - ETA: 8s - loss: 1.0099 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_test_batch_end` time: 0.0153s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.8141 - accuracy: 0.4348\n",
      "training acc:  0.6637499928474426 , training loss:  0.6289598345756531 , val acc:  0.671875 , val loss:  0.6153540015220642 , test acc:  0.4347735047340393 , test loss:  0.8141295909881592\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt3/250word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second function around seems though lot always best etc less give either help x & likely \\mathcal means must form made following least bit note small trying order light end anything based done option file start % keep size maybe found us already person world real > reason edit space 0    years check said little read solution process rather non day times list field level someone issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show result course understand user makes open hard life group output fact last idea human working name site whether amount pretty version water assume matter state kind past error method    got color    5 energy great original paper due sense else called character next general range getting god camera control\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6944 - accuracy: 0.5794 - val_loss: 0.6484 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6406 - accuracy: 0.6619 - val_loss: 0.6358 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6402 - accuracy: 0.6463 - val_loss: 0.6284 - val_accuracy: 0.6641\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6317 - accuracy: 0.6619 - val_loss: 0.6243 - val_accuracy: 0.6623\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6342 - accuracy: 0.6431 - val_loss: 0.6192 - val_accuracy: 0.6875\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6440 - accuracy: 0.6500 - val_loss: 0.6188 - val_accuracy: 0.6965\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6435 - accuracy: 0.6356 - val_loss: 0.6188 - val_accuracy: 0.6719\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6318 - accuracy: 0.6456 - val_loss: 0.6179 - val_accuracy: 0.6731\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6506 - accuracy: 0.6338 - val_loss: 0.6239 - val_accuracy: 0.6881\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6398 - accuracy: 0.6569 - val_loss: 0.6163 - val_accuracy: 0.6839\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6283 - accuracy: 0.6531 - val_loss: 0.6221 - val_accuracy: 0.6562\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6255 - accuracy: 0.6606 - val_loss: 0.6192 - val_accuracy: 0.6659\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6301 - accuracy: 0.6419 - val_loss: 0.6209 - val_accuracy: 0.6629\n",
      "  1/596 [..............................] - ETA: 8s - loss: 1.0207 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0141s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.8190 - accuracy: 0.4187\n",
      "training acc:  0.6418750286102295 , training loss:  0.6301280856132507 , val acc:  0.6628605723381042 , val loss:  0.6208558678627014 , test acc:  0.4187290370464325 , test loss:  0.8189641237258911\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt3/300word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second function around seems though lot always best etc less give either help x & likely \\mathcal means must form made following least bit note small trying order light end anything based done option file start % keep size maybe found us already person world real > reason edit space 0    years check said little read solution process rather non day times list field level someone issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show result course understand user makes open hard life group output fact last idea human working name site whether amount pretty version water assume matter state kind past error method    got color    5 energy great original paper due sense else called character next general range getting god camera control ai current low seem away particular values object free term consider left common 10 wo short words full difference i.e. test old area experience tried < changes online year thought    making although input book call available lower lens thus comes layer needs provide running within everything results feel hand\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7236 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0242s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6731 - accuracy: 0.6237 - val_loss: 0.6498 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6508 - accuracy: 0.6419 - val_loss: 0.6369 - val_accuracy: 0.6677\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6514 - accuracy: 0.6281 - val_loss: 0.6328 - val_accuracy: 0.6917\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6322 - accuracy: 0.6587 - val_loss: 0.6232 - val_accuracy: 0.6881\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6311 - accuracy: 0.6569 - val_loss: 0.6210 - val_accuracy: 0.6677\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6245 - accuracy: 0.6681 - val_loss: 0.6183 - val_accuracy: 0.6947\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6406 - accuracy: 0.6469 - val_loss: 0.6165 - val_accuracy: 0.6839\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6401 - accuracy: 0.6475 - val_loss: 0.6164 - val_accuracy: 0.6845\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6205 - accuracy: 0.6750 - val_loss: 0.6156 - val_accuracy: 0.6839\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6045 - accuracy: 0.6881 - val_loss: 0.6164 - val_accuracy: 0.6713\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6266 - accuracy: 0.6694 - val_loss: 0.6141 - val_accuracy: 0.6893\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6190 - accuracy: 0.6687 - val_loss: 0.6140 - val_accuracy: 0.6887\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6273 - accuracy: 0.6712 - val_loss: 0.6140 - val_accuracy: 0.6887\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6391 - accuracy: 0.6431 - val_loss: 0.6165 - val_accuracy: 0.6713\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6327 - accuracy: 0.6506 - val_loss: 0.6171 - val_accuracy: 0.6701\n",
      "  1/596 [..............................] - ETA: 4s - loss: 1.0234 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0151s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.8129 - accuracy: 0.4309\n",
      "training acc:  0.6506249904632568 , training loss:  0.6326542496681213 , val acc:  0.6700721383094788 , val loss:  0.6170794367790222 , test acc:  0.4308934509754181 , test loss:  0.8128868341445923\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt3/350word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second function around seems though lot always best etc less give either help x & likely \\mathcal means must form made following least bit note small trying order light end anything based done option file start % keep size maybe found us already person world real > reason edit space 0    years check said little read solution process rather non day times list field level someone issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show result course understand user makes open hard life group output fact last idea human working name site whether amount pretty version water assume matter state kind past error method    got color    5 energy great original paper due sense else called character next general range getting god camera control ai current low seem away particular values object free term consider left common 10 wo short words full difference i.e. test old area experience tried < changes online year thought    making although input book call available lower lens thus comes layer needs provide running within everything results feel hand almost source class ask e.g. easy perhaps top useful anyone \\\\ higher google big block created 4 post price access write several tell true future others smaller search bytes clear nothing design exactly n multiple wrong three unless numbers avoid close fine position ' effect days believe # mm sort\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 1s - loss: 0.6937 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_train_batch_end` time: 0.0132s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6839 - accuracy: 0.6631 - val_loss: 0.6720 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6761 - accuracy: 0.6125 - val_loss: 0.6562 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6633 - accuracy: 0.6331 - val_loss: 0.6442 - val_accuracy: 0.6623\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6415 - accuracy: 0.6594 - val_loss: 0.6341 - val_accuracy: 0.6508\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6485 - accuracy: 0.6275 - val_loss: 0.6283 - val_accuracy: 0.6797\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6310 - accuracy: 0.6662 - val_loss: 0.6240 - val_accuracy: 0.6935\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6374 - accuracy: 0.6606 - val_loss: 0.6209 - val_accuracy: 0.6887\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6295 - accuracy: 0.6556 - val_loss: 0.6189 - val_accuracy: 0.6857\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6389 - accuracy: 0.6450 - val_loss: 0.6176 - val_accuracy: 0.6935\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6296 - accuracy: 0.6694 - val_loss: 0.6166 - val_accuracy: 0.6821\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6238 - accuracy: 0.6506 - val_loss: 0.6174 - val_accuracy: 0.6707\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6348 - accuracy: 0.6519 - val_loss: 0.6155 - val_accuracy: 0.6881\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6346 - accuracy: 0.6488 - val_loss: 0.6158 - val_accuracy: 0.6809\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6424 - accuracy: 0.6350 - val_loss: 0.6162 - val_accuracy: 0.6779\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6282 - accuracy: 0.6538 - val_loss: 0.6165 - val_accuracy: 0.6725\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.9924 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0164s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.8066 - accuracy: 0.4338\n",
      "training acc:  0.6537500023841858 , training loss:  0.6281569004058838 , val acc:  0.6724759340286255 , val loss:  0.6164703369140625 , test acc:  0.4338296949863434 , test loss:  0.8066477179527283\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt3/400word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second function around seems though lot always best etc less give either help x & likely \\mathcal means must form made following least bit note small trying order light end anything based done option file start % keep size maybe found us already person world real > reason edit space 0    years check said little read solution process rather non day times list field level someone issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show result course understand user makes open hard life group output fact last idea human working name site whether amount pretty version water assume matter state kind past error method    got color    5 energy great original paper due sense else called character next general range getting god camera control ai current low seem away particular values object free term consider left common 10 wo short words full difference i.e. test old area experience tried < changes online year thought    making although input book call available lower lens thus comes layer needs provide running within everything results feel hand almost source class ask e.g. easy perhaps top useful anyone \\\\ higher google big block created 4 post price access write several tell true future others smaller search bytes clear nothing design exactly n multiple wrong three unless numbers avoid close fine position ' effect days believe # mm sort certain language standard air actual account turn research difficult model yes takes _ length address product game uses normal gives known generally easily computer map cases along seen thanks sound money ones bad step cause terms whole mind rate content company problems play looks history main story select per happen\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.5785 - accuracy: 0.9375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0063s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6933 - accuracy: 0.6594 - val_loss: 0.6752 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6660 - accuracy: 0.6450 - val_loss: 0.6514 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6582 - accuracy: 0.6294 - val_loss: 0.6401 - val_accuracy: 0.6665\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6319 - accuracy: 0.6662 - val_loss: 0.6305 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6509 - accuracy: 0.6187 - val_loss: 0.6263 - val_accuracy: 0.6827\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6271 - accuracy: 0.6544 - val_loss: 0.6224 - val_accuracy: 0.6707\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6452 - accuracy: 0.6350 - val_loss: 0.6249 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6310 - accuracy: 0.6406 - val_loss: 0.6237 - val_accuracy: 0.6550\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6198 - accuracy: 0.6538 - val_loss: 0.6181 - val_accuracy: 0.6737\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6253 - accuracy: 0.6631 - val_loss: 0.6171 - val_accuracy: 0.6737\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6250 - accuracy: 0.6556 - val_loss: 0.6153 - val_accuracy: 0.6869\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6365 - accuracy: 0.6413 - val_loss: 0.6155 - val_accuracy: 0.6827\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6198 - accuracy: 0.6762 - val_loss: 0.6144 - val_accuracy: 0.6923\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6184 - accuracy: 0.6600 - val_loss: 0.6206 - val_accuracy: 0.6641\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6320 - accuracy: 0.6438 - val_loss: 0.6202 - val_accuracy: 0.6629\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6209 - accuracy: 0.6556 - val_loss: 0.6171 - val_accuracy: 0.6707\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.0153 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0195s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.8161 - accuracy: 0.4268\n",
      "training acc:  0.6556249856948853 , training loss:  0.6209008693695068 , val acc:  0.6706730723381042 , val loss:  0.617101788520813 , test acc:  0.42680367827415466 , test loss:  0.8161010146141052\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt3/450word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second function around seems though lot always best etc less give either help x & likely \\mathcal means must form made following least bit note small trying order light end anything based done option file start % keep size maybe found us already person world real > reason edit space 0    years check said little read solution process rather non day times list field level someone issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show result course understand user makes open hard life group output fact last idea human working name site whether amount pretty version water assume matter state kind past error method    got color    5 energy great original paper due sense else called character next general range getting god camera control ai current low seem away particular values object free term consider left common 10 wo short words full difference i.e. test old area experience tried < changes online year thought    making although input book call available lower lens thus comes layer needs provide running within everything results feel hand almost source class ask e.g. easy perhaps top useful anyone \\\\ higher google big block created 4 post price access write several tell true future others smaller search bytes clear nothing design exactly n multiple wrong three unless numbers avoid close fine position ' effect days believe # mm sort certain language standard air actual account turn research difficult model yes takes _ length address product game uses normal gives known generally easily computer map cases along seen thanks sound money ones bad step cause terms whole mind rate content company problems play looks history main story select per happen sometimes   added says allow return points therefore context support taking functions 100 writing target e structure options become earth meaning setting theory update head network view n$ gets move larger flash choose inside english examples related reading food body yet 6 children build mode situation random sentence parts characters\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6562 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0149s). Check your callbacks.\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.7022 - accuracy: 0.6168WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_test_batch_end` time: 0.0136s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.7025 - accuracy: 0.6144 - val_loss: 0.6739 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6628 - accuracy: 0.6587 - val_loss: 0.6537 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6532 - accuracy: 0.6456 - val_loss: 0.6424 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6529 - accuracy: 0.6263 - val_loss: 0.6352 - val_accuracy: 0.6959\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6409 - accuracy: 0.6419 - val_loss: 0.6285 - val_accuracy: 0.6611\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6567 - accuracy: 0.6175 - val_loss: 0.6265 - val_accuracy: 0.6923\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6439 - accuracy: 0.6356 - val_loss: 0.6257 - val_accuracy: 0.6905\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6380 - accuracy: 0.6694 - val_loss: 0.6215 - val_accuracy: 0.6707\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6278 - accuracy: 0.6781 - val_loss: 0.6184 - val_accuracy: 0.6803\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6219 - accuracy: 0.6662 - val_loss: 0.6258 - val_accuracy: 0.6520\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6360 - accuracy: 0.6488 - val_loss: 0.6168 - val_accuracy: 0.6917\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6257 - accuracy: 0.6669 - val_loss: 0.6172 - val_accuracy: 0.6719\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6102 - accuracy: 0.6769 - val_loss: 0.6148 - val_accuracy: 0.6851\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6172 - accuracy: 0.6744 - val_loss: 0.6166 - val_accuracy: 0.6713\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6400 - accuracy: 0.6450 - val_loss: 0.6195 - val_accuracy: 0.6671\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6363 - accuracy: 0.6413 - val_loss: 0.6186 - val_accuracy: 0.6677\n",
      "  1/596 [..............................] - ETA: 8s - loss: 1.0069 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0140s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.8158 - accuracy: 0.4241\n",
      "training acc:  0.6412500143051147 , training loss:  0.6363167762756348 , val acc:  0.6676682829856873 , val loss:  0.6185916066169739 , test acc:  0.42407718300819397 , test loss:  0.8157988786697388\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt3/500word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second function around seems though lot always best etc less give either help x & likely \\mathcal means must form made following least bit note small trying order light end anything based done option file start % keep size maybe found us already person world real > reason edit space 0    years check said little read solution process rather non day times list field level someone issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show result course understand user makes open hard life group output fact last idea human working name site whether amount pretty version water assume matter state kind past error method    got color    5 energy great original paper due sense else called character next general range getting god camera control ai current low seem away particular values object free term consider left common 10 wo short words full difference i.e. test old area experience tried < changes online year thought    making although input book call available lower lens thus comes layer needs provide running within everything results feel hand almost source class ask e.g. easy perhaps top useful anyone \\\\ higher google big block created 4 post price access write several tell true future others smaller search bytes clear nothing design exactly n multiple wrong three unless numbers avoid close fine position ' effect days believe # mm sort certain language standard air actual account turn research difficult model yes takes _ length address product game uses normal gives known generally easily computer map cases along seen thanks sound money ones bad step cause terms whole mind rate content company problems play looks history main story select per happen sometimes   added says allow return points therefore context support taking functions 100 writing target e structure options become earth meaning setting theory update head network view n$ gets move larger flash choose inside english examples related reading food body yet 6 children build mode situation random sentence parts characters job rules necessary directly guess half link tool remember string assuming background slightly client stop images worth apply depends force instance personal later local whatever approach fire car quality interesting entire knowledge major goes please mentioned magic software hope de web rule algorithm pages noise outside save especially limit exact\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6824 - accuracy: 0.6056 - val_loss: 0.6584 - val_accuracy: 0.6544\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6539 - accuracy: 0.6513 - val_loss: 0.6441 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6578 - accuracy: 0.6237 - val_loss: 0.6382 - val_accuracy: 0.6617\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6390 - accuracy: 0.6669 - val_loss: 0.6339 - val_accuracy: 0.6881\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6253 - accuracy: 0.6687 - val_loss: 0.6266 - val_accuracy: 0.6520\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6387 - accuracy: 0.6381 - val_loss: 0.6235 - val_accuracy: 0.6653\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6287 - accuracy: 0.6606 - val_loss: 0.6202 - val_accuracy: 0.6809\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6176 - accuracy: 0.6669 - val_loss: 0.6239 - val_accuracy: 0.6538\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6231 - accuracy: 0.6712 - val_loss: 0.6179 - val_accuracy: 0.6899\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6335 - accuracy: 0.6463 - val_loss: 0.6181 - val_accuracy: 0.6737\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6345 - accuracy: 0.6519 - val_loss: 0.6179 - val_accuracy: 0.6881\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6178 - accuracy: 0.6812 - val_loss: 0.6179 - val_accuracy: 0.6701\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6401 - accuracy: 0.6388 - val_loss: 0.6170 - val_accuracy: 0.6803\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6279 - accuracy: 0.6569 - val_loss: 0.6178 - val_accuracy: 0.6689\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6302 - accuracy: 0.6494 - val_loss: 0.6170 - val_accuracy: 0.6767\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6289 - accuracy: 0.6694 - val_loss: 0.6167 - val_accuracy: 0.6767\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6222 - accuracy: 0.6612 - val_loss: 0.6171 - val_accuracy: 0.6893\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6214 - accuracy: 0.6869 - val_loss: 0.6190 - val_accuracy: 0.6863\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6311 - accuracy: 0.6619 - val_loss: 0.6161 - val_accuracy: 0.6905\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6412 - accuracy: 0.6469 - val_loss: 0.6167 - val_accuracy: 0.6767\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6083 - accuracy: 0.6781 - val_loss: 0.6161 - val_accuracy: 0.6881\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6305 - accuracy: 0.6544 - val_loss: 0.6180 - val_accuracy: 0.6887\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6388 - accuracy: 0.6619 - val_loss: 0.6162 - val_accuracy: 0.6809\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6256 - accuracy: 0.6569 - val_loss: 0.6205 - val_accuracy: 0.6623\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.0420 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0139s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.8383 - accuracy: 0.4211\n",
      "training acc:  0.6568750143051147 , training loss:  0.625622034072876 , val acc:  0.6622596383094788 , val loss:  0.6205291152000427 , test acc:  0.4211409389972687 , test loss:  0.8383414149284363\n",
      "\n",
      "opt4\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt4/50word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 1.1030 - accuracy: 0.4646WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_test_batch_end` time: 0.0064s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 1.0975 - accuracy: 0.4688 - val_loss: 0.6489 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6473 - accuracy: 0.6606 - val_loss: 0.6459 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6624 - accuracy: 0.6256 - val_loss: 0.6459 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6577 - accuracy: 0.6331 - val_loss: 0.6500 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6377 - accuracy: 0.6669 - val_loss: 0.6487 - val_accuracy: 0.6526\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.9687 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0063s vs `on_test_batch_end` time: 0.0155s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.8359 - accuracy: 0.4123\n",
      "training acc:  0.6668750047683716 , training loss:  0.6377213001251221 , val acc:  0.6526442170143127 , val loss:  0.6486579179763794 , test acc:  0.4123322069644928 , test loss:  0.8358728289604187\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt4/100word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6495 - accuracy: 0.6800 - val_loss: 0.6687 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6757 - accuracy: 0.6375 - val_loss: 0.6608 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6652 - accuracy: 0.6413 - val_loss: 0.6561 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6455 - accuracy: 0.6712 - val_loss: 0.6549 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6717 - accuracy: 0.6237 - val_loss: 0.6512 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6625 - accuracy: 0.6356 - val_loss: 0.6587 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6568 - accuracy: 0.6456 - val_loss: 0.6530 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6528 - accuracy: 0.6531 - val_loss: 0.6450 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6638 - accuracy: 0.6200 - val_loss: 0.6501 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6628 - accuracy: 0.6212 - val_loss: 0.6567 - val_accuracy: 0.6538\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6521 - accuracy: 0.6388 - val_loss: 0.6399 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6314 - accuracy: 0.6712 - val_loss: 0.6392 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6355 - accuracy: 0.6581 - val_loss: 0.6364 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6432 - accuracy: 0.6494 - val_loss: 0.6362 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6403 - accuracy: 0.6363 - val_loss: 0.6413 - val_accuracy: 0.6538\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6480 - accuracy: 0.6219 - val_loss: 0.6321 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6480 - accuracy: 0.6275 - val_loss: 0.6398 - val_accuracy: 0.6773\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6475 - accuracy: 0.6394 - val_loss: 0.6327 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6227 - accuracy: 0.6762 - val_loss: 0.6303 - val_accuracy: 0.6526\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6500 - accuracy: 0.6294 - val_loss: 0.6286 - val_accuracy: 0.6526\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6395 - accuracy: 0.6319 - val_loss: 0.6521 - val_accuracy: 0.6448\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6343 - accuracy: 0.6594 - val_loss: 0.6267 - val_accuracy: 0.6526\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6293 - accuracy: 0.6562 - val_loss: 0.6261 - val_accuracy: 0.6526\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6436 - accuracy: 0.6300 - val_loss: 0.6264 - val_accuracy: 0.6526\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6252 - accuracy: 0.6513 - val_loss: 0.6283 - val_accuracy: 0.6779\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6368 - accuracy: 0.6463 - val_loss: 0.6242 - val_accuracy: 0.6526\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6350 - accuracy: 0.6531 - val_loss: 0.6266 - val_accuracy: 0.6707\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6379 - accuracy: 0.6400 - val_loss: 0.6315 - val_accuracy: 0.6905\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6346 - accuracy: 0.6569 - val_loss: 0.6230 - val_accuracy: 0.6526\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6172 - accuracy: 0.6606 - val_loss: 0.6326 - val_accuracy: 0.6526\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.1106 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_test_batch_end` time: 0.0111s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9378 - accuracy: 0.4123\n",
      "training acc:  0.6606249809265137 , training loss:  0.6171770691871643 , val acc:  0.6526442170143127 , val loss:  0.6325844526290894 , test acc:  0.4123322069644928 , test loss:  0.9378090500831604\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt4/150word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second function around seems though lot always best etc less give either help x & likely \\mathcal means must form made following least bit note small trying order light end anything based done option file start % keep size maybe found us already person world real > reason edit\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6764 - accuracy: 0.6288 - val_loss: 0.6592 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6641 - accuracy: 0.6431 - val_loss: 0.6559 - val_accuracy: 0.6526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6494 - accuracy: 0.6700 - val_loss: 0.6559 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6743 - accuracy: 0.6219 - val_loss: 0.6515 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6585 - accuracy: 0.6413 - val_loss: 0.6493 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6567 - accuracy: 0.6394 - val_loss: 0.6471 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6605 - accuracy: 0.6300 - val_loss: 0.6447 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6497 - accuracy: 0.6450 - val_loss: 0.6427 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6512 - accuracy: 0.6381 - val_loss: 0.6505 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6437 - accuracy: 0.6569 - val_loss: 0.6388 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6416 - accuracy: 0.6481 - val_loss: 0.6372 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6458 - accuracy: 0.6438 - val_loss: 0.6356 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6259 - accuracy: 0.6737 - val_loss: 0.6382 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6467 - accuracy: 0.6369 - val_loss: 0.6409 - val_accuracy: 0.6562\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6365 - accuracy: 0.6525 - val_loss: 0.6333 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6358 - accuracy: 0.6481 - val_loss: 0.6324 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6333 - accuracy: 0.6544 - val_loss: 0.6309 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6537 - accuracy: 0.6119 - val_loss: 0.6285 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6376 - accuracy: 0.6394 - val_loss: 0.6275 - val_accuracy: 0.6526\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6382 - accuracy: 0.6425 - val_loss: 0.6270 - val_accuracy: 0.6526\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6435 - accuracy: 0.6394 - val_loss: 0.6260 - val_accuracy: 0.6526\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6144 - accuracy: 0.6719 - val_loss: 0.6313 - val_accuracy: 0.6526\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6218 - accuracy: 0.6600 - val_loss: 0.6243 - val_accuracy: 0.6526\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6122 - accuracy: 0.6750 - val_loss: 0.6275 - val_accuracy: 0.6526\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6200 - accuracy: 0.6631 - val_loss: 0.6229 - val_accuracy: 0.6532\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6324 - accuracy: 0.6544 - val_loss: 0.6225 - val_accuracy: 0.6532\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6283 - accuracy: 0.6700 - val_loss: 0.6275 - val_accuracy: 0.6526\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6123 - accuracy: 0.6750 - val_loss: 0.6220 - val_accuracy: 0.6526\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6531 - accuracy: 0.6194 - val_loss: 0.6231 - val_accuracy: 0.6785\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6493 - accuracy: 0.6481 - val_loss: 0.6255 - val_accuracy: 0.6923\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.9198 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_test_batch_end` time: 0.0145s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.8420 - accuracy: 0.4332\n",
      "training acc:  0.6481249928474426 , training loss:  0.6492903828620911 , val acc:  0.692307710647583 , val loss:  0.6255387663841248 , test acc:  0.43320050835609436 , test loss:  0.8419600129127502\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt4/200word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second function around seems though lot always best etc less give either help x & likely \\mathcal means must form made following least bit note small trying order light end anything based done option file start % keep size maybe found us already person world real > reason edit space 0    years check said little read solution process rather non day times list field level someone issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually\n",
      "Epoch 1/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.8225 - accuracy: 0.4716WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_test_batch_end` time: 0.0063s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.8222 - accuracy: 0.4706 - val_loss: 0.6533 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6611 - accuracy: 0.6338 - val_loss: 0.6639 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6621 - accuracy: 0.6413 - val_loss: 0.6601 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6734 - accuracy: 0.6000 - val_loss: 0.6502 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6670 - accuracy: 0.6144 - val_loss: 0.6533 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6432 - accuracy: 0.6675 - val_loss: 0.6455 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6457 - accuracy: 0.6581 - val_loss: 0.6442 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6297 - accuracy: 0.6812 - val_loss: 0.6470 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6559 - accuracy: 0.6237 - val_loss: 0.6461 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6494 - accuracy: 0.6444 - val_loss: 0.6406 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6535 - accuracy: 0.6331 - val_loss: 0.6405 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6584 - accuracy: 0.6219 - val_loss: 0.6378 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6369 - accuracy: 0.6587 - val_loss: 0.6454 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6501 - accuracy: 0.6263 - val_loss: 0.6388 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6400 - accuracy: 0.6475 - val_loss: 0.6355 - val_accuracy: 0.6526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6450 - accuracy: 0.6363 - val_loss: 0.6336 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6418 - accuracy: 0.6438 - val_loss: 0.6328 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6438 - accuracy: 0.6356 - val_loss: 0.6334 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6369 - accuracy: 0.6494 - val_loss: 0.6347 - val_accuracy: 0.6544\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6532 - accuracy: 0.6119 - val_loss: 0.6348 - val_accuracy: 0.6629\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.8699 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0155s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7962 - accuracy: 0.4193\n",
      "training acc:  0.6118749976158142 , training loss:  0.6531773209571838 , val acc:  0.6628605723381042 , val loss:  0.6347514390945435 , test acc:  0.4192533493041992 , test loss:  0.7962318062782288\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt4/250word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second function around seems though lot always best etc less give either help x & likely \\mathcal means must form made following least bit note small trying order light end anything based done option file start % keep size maybe found us already person world real > reason edit space 0    years check said little read solution process rather non day times list field level someone issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show result course understand user makes open hard life group output fact last idea human working name site whether amount pretty version water assume matter state kind past error method    got color    5 energy great original paper due sense else called character next general range getting god\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 1.8363 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0167s). Check your callbacks.\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.7081 - accuracy: 0.6566WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0108s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.7102 - accuracy: 0.6525 - val_loss: 0.6704 - val_accuracy: 0.6418\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6789 - accuracy: 0.6306 - val_loss: 0.6721 - val_accuracy: 0.6454\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6608 - accuracy: 0.6519 - val_loss: 0.6612 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6717 - accuracy: 0.6263 - val_loss: 0.6741 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6730 - accuracy: 0.6250 - val_loss: 0.6541 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6761 - accuracy: 0.5950 - val_loss: 0.6748 - val_accuracy: 0.6827\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6493 - accuracy: 0.6662 - val_loss: 0.6473 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6326 - accuracy: 0.6756 - val_loss: 0.6490 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6387 - accuracy: 0.6644 - val_loss: 0.6438 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6550 - accuracy: 0.6350 - val_loss: 0.6407 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6358 - accuracy: 0.6675 - val_loss: 0.6464 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6379 - accuracy: 0.6594 - val_loss: 0.6365 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6347 - accuracy: 0.6662 - val_loss: 0.6384 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6438 - accuracy: 0.6463 - val_loss: 0.6375 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6475 - accuracy: 0.6344 - val_loss: 0.6331 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6345 - accuracy: 0.6538 - val_loss: 0.6309 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6315 - accuracy: 0.6550 - val_loss: 0.6305 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6467 - accuracy: 0.6525 - val_loss: 0.6285 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6354 - accuracy: 0.6406 - val_loss: 0.6268 - val_accuracy: 0.6526\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6414 - accuracy: 0.6400 - val_loss: 0.6330 - val_accuracy: 0.6526\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6401 - accuracy: 0.6338 - val_loss: 0.6364 - val_accuracy: 0.6815\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6350 - accuracy: 0.6600 - val_loss: 0.6259 - val_accuracy: 0.6725\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6252 - accuracy: 0.6675 - val_loss: 0.6239 - val_accuracy: 0.6581\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6246 - accuracy: 0.6594 - val_loss: 0.6230 - val_accuracy: 0.6532\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6233 - accuracy: 0.6644 - val_loss: 0.6224 - val_accuracy: 0.6581\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6398 - accuracy: 0.6488 - val_loss: 0.6231 - val_accuracy: 0.6809\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6448 - accuracy: 0.6513 - val_loss: 0.6236 - val_accuracy: 0.6839\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6411 - accuracy: 0.6431 - val_loss: 0.6213 - val_accuracy: 0.6581\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6267 - accuracy: 0.6719 - val_loss: 0.6224 - val_accuracy: 0.6881\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6341 - accuracy: 0.6550 - val_loss: 0.6206 - val_accuracy: 0.6593\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.0276 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0063s vs `on_test_batch_end` time: 0.0160s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.8876 - accuracy: 0.4169\n",
      "training acc:  0.6549999713897705 , training loss:  0.6341026425361633 , val acc:  0.659254789352417 , val loss:  0.6205773949623108 , test acc:  0.41694632172584534 , test loss:  0.8876127600669861\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt4/300word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second function around seems though lot always best etc less give either help x & likely \\mathcal means must form made following least bit note small trying order light end anything based done option file start % keep size maybe found us already person world real > reason edit space 0    years check said little read solution process rather non day times list field level someone issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show result course understand user makes open hard life group output fact last idea human working name site whether amount pretty version water assume matter state kind past error method    got color    5 energy great original paper due sense else called character next general range getting god camera control ai current low seem away particular values object free term consider left common 10 wo short words full difference i.e. test old area experience tried < changes online year thought    making although input book call available lower lens thus comes layer needs provide running within everything results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.8585 - accuracy: 0.6419 - val_loss: 0.6878 - val_accuracy: 0.6388\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6936 - accuracy: 0.6075 - val_loss: 0.6737 - val_accuracy: 0.6364\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6731 - accuracy: 0.6463 - val_loss: 0.6679 - val_accuracy: 0.6514\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6675 - accuracy: 0.6475 - val_loss: 0.6646 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6493 - accuracy: 0.6812 - val_loss: 0.6581 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6687 - accuracy: 0.6319 - val_loss: 0.6551 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6668 - accuracy: 0.6263 - val_loss: 0.6505 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6489 - accuracy: 0.6550 - val_loss: 0.6466 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6626 - accuracy: 0.6281 - val_loss: 0.6437 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6492 - accuracy: 0.6575 - val_loss: 0.6413 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6379 - accuracy: 0.6662 - val_loss: 0.6387 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6418 - accuracy: 0.6494 - val_loss: 0.6361 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6441 - accuracy: 0.6375 - val_loss: 0.6349 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6428 - accuracy: 0.6338 - val_loss: 0.6328 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6541 - accuracy: 0.6269 - val_loss: 0.6347 - val_accuracy: 0.6725\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6215 - accuracy: 0.6706 - val_loss: 0.6284 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6461 - accuracy: 0.6300 - val_loss: 0.6388 - val_accuracy: 0.6833\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6500 - accuracy: 0.6275 - val_loss: 0.6275 - val_accuracy: 0.6556\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6254 - accuracy: 0.6656 - val_loss: 0.6317 - val_accuracy: 0.6911\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6334 - accuracy: 0.6531 - val_loss: 0.6256 - val_accuracy: 0.6671\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6289 - accuracy: 0.6687 - val_loss: 0.6271 - val_accuracy: 0.6526\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6305 - accuracy: 0.6581 - val_loss: 0.6268 - val_accuracy: 0.6911\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6347 - accuracy: 0.6575 - val_loss: 0.6247 - val_accuracy: 0.6526\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6382 - accuracy: 0.6344 - val_loss: 0.6265 - val_accuracy: 0.6863\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6441 - accuracy: 0.6587 - val_loss: 0.6223 - val_accuracy: 0.6767\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6371 - accuracy: 0.6562 - val_loss: 0.6266 - val_accuracy: 0.6526\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6303 - accuracy: 0.6656 - val_loss: 0.6214 - val_accuracy: 0.6761\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6144 - accuracy: 0.6712 - val_loss: 0.6253 - val_accuracy: 0.6821\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6343 - accuracy: 0.6625 - val_loss: 0.6249 - val_accuracy: 0.6821\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6278 - accuracy: 0.6644 - val_loss: 0.6202 - val_accuracy: 0.6761\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.0237 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0009s vs `on_test_batch_end` time: 0.0202s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.8893 - accuracy: 0.4261\n",
      "training acc:  0.6643750071525574 , training loss:  0.6277685761451721 , val acc:  0.6760817170143127 , val loss:  0.6202115416526794 , test acc:  0.42606961727142334 , test loss:  0.889274537563324\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt4/350word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second function around seems though lot always best etc less give either help x & likely \\mathcal means must form made following least bit note small trying order light end anything based done option file start % keep size maybe found us already person world real > reason edit space 0    years check said little read solution process rather non day times list field level someone issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show result course understand user makes open hard life group output fact last idea human working name site whether amount pretty version water assume matter state kind past error method    got color    5 energy great original paper due sense else called character next general range getting god camera control ai current low seem away particular values object free term consider left common 10 wo short words full difference i.e. test old area experience tried < changes online year thought    making although input book call available lower lens thus comes layer needs provide running within everything results feel hand almost source class ask e.g. easy perhaps top useful anyone \\\\ higher google big block created 4 post price access write several tell true future others smaller search bytes clear nothing design exactly n multiple wrong three unless numbers avoid close fine position ' effect days believe #\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.8793 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0107s). Check your callbacks.\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6860 - accuracy: 0.5758WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_test_batch_end` time: 0.0150s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6854 - accuracy: 0.5775 - val_loss: 0.6570 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6701 - accuracy: 0.6263 - val_loss: 0.6536 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6612 - accuracy: 0.6413 - val_loss: 0.6507 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6493 - accuracy: 0.6644 - val_loss: 0.6486 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6724 - accuracy: 0.6100 - val_loss: 0.6510 - val_accuracy: 0.6526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6479 - accuracy: 0.6569 - val_loss: 0.6437 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6543 - accuracy: 0.6369 - val_loss: 0.6421 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6526 - accuracy: 0.6413 - val_loss: 0.6398 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6459 - accuracy: 0.6475 - val_loss: 0.6379 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6541 - accuracy: 0.6288 - val_loss: 0.6376 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6558 - accuracy: 0.6300 - val_loss: 0.6361 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6442 - accuracy: 0.6388 - val_loss: 0.6401 - val_accuracy: 0.6695\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6375 - accuracy: 0.6562 - val_loss: 0.6386 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6528 - accuracy: 0.6194 - val_loss: 0.6319 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6478 - accuracy: 0.6425 - val_loss: 0.6330 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6491 - accuracy: 0.6225 - val_loss: 0.6290 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6357 - accuracy: 0.6388 - val_loss: 0.6338 - val_accuracy: 0.6857\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6440 - accuracy: 0.6338 - val_loss: 0.6356 - val_accuracy: 0.6857\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6349 - accuracy: 0.6488 - val_loss: 0.6263 - val_accuracy: 0.6556\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6319 - accuracy: 0.6669 - val_loss: 0.6254 - val_accuracy: 0.6544\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6443 - accuracy: 0.6338 - val_loss: 0.6252 - val_accuracy: 0.6526\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6301 - accuracy: 0.6538 - val_loss: 0.6243 - val_accuracy: 0.6641\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6313 - accuracy: 0.6488 - val_loss: 0.6331 - val_accuracy: 0.6821\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6309 - accuracy: 0.6550 - val_loss: 0.6232 - val_accuracy: 0.6695\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6472 - accuracy: 0.6263 - val_loss: 0.6272 - val_accuracy: 0.6881\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6357 - accuracy: 0.6581 - val_loss: 0.6229 - val_accuracy: 0.6532\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6352 - accuracy: 0.6525 - val_loss: 0.6231 - val_accuracy: 0.6532\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.6355 - accuracy: 0.6550 - val_loss: 0.6317 - val_accuracy: 0.6526\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6439 - accuracy: 0.6400 - val_loss: 0.6235 - val_accuracy: 0.6532\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.0644 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_test_batch_end` time: 0.0136s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.9185 - accuracy: 0.4122\n",
      "training acc:  0.6399999856948853 , training loss:  0.6438856720924377 , val acc:  0.653245210647583 , val loss:  0.6235314607620239 , test acc:  0.4122273623943329 , test loss:  0.9184746742248535\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt4/400word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second function around seems though lot always best etc less give either help x & likely \\mathcal means must form made following least bit note small trying order light end anything based done option file start % keep size maybe found us already person world real > reason edit space 0    years check said little read solution process rather non day times list field level someone issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show result course understand user makes open hard life group output fact last idea human working name site whether amount pretty version water assume matter state kind past error method    got color    5 energy great original paper due sense else called character next general range getting god camera control ai current low seem away particular values object free term consider left common 10 wo short words full difference i.e. test old area experience tried < changes online year thought    making although input book call available lower lens thus comes layer needs provide running within everything results feel hand almost source class ask e.g. easy perhaps top useful anyone \\\\ higher google big block created 4 post price access write several tell true future others smaller search bytes clear nothing design exactly n multiple wrong three unless numbers avoid close fine position ' effect days believe # mm sort certain language standard air actual account turn research difficult model yes takes _ length address product game uses normal gives known generally easily computer map cases along seen thanks sound money ones bad step cause terms whole mind rate content company problems play looks history main story select\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6876 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0107s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.7208 - accuracy: 0.6569 - val_loss: 0.6727 - val_accuracy: 0.6448\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6961 - accuracy: 0.5944 - val_loss: 0.6757 - val_accuracy: 0.6364\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6732 - accuracy: 0.6456 - val_loss: 0.6667 - val_accuracy: 0.6520\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6636 - accuracy: 0.6587 - val_loss: 0.6634 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6719 - accuracy: 0.6369 - val_loss: 0.6612 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6667 - accuracy: 0.6425 - val_loss: 0.6582 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6561 - accuracy: 0.6575 - val_loss: 0.6567 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6657 - accuracy: 0.6419 - val_loss: 0.6603 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6541 - accuracy: 0.6562 - val_loss: 0.6509 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6468 - accuracy: 0.6606 - val_loss: 0.6486 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6546 - accuracy: 0.6431 - val_loss: 0.6461 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6524 - accuracy: 0.6413 - val_loss: 0.6449 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6435 - accuracy: 0.6569 - val_loss: 0.6424 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6525 - accuracy: 0.6338 - val_loss: 0.6403 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6447 - accuracy: 0.6463 - val_loss: 0.6391 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6279 - accuracy: 0.6712 - val_loss: 0.6379 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6423 - accuracy: 0.6494 - val_loss: 0.6349 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6319 - accuracy: 0.6637 - val_loss: 0.6334 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6451 - accuracy: 0.6406 - val_loss: 0.6323 - val_accuracy: 0.6526\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6310 - accuracy: 0.6600 - val_loss: 0.6310 - val_accuracy: 0.6526\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6414 - accuracy: 0.6400 - val_loss: 0.6308 - val_accuracy: 0.6532\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6425 - accuracy: 0.6419 - val_loss: 0.6299 - val_accuracy: 0.6526\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6186 - accuracy: 0.6762 - val_loss: 0.6298 - val_accuracy: 0.6526\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6493 - accuracy: 0.6256 - val_loss: 0.6266 - val_accuracy: 0.6526\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6382 - accuracy: 0.6363 - val_loss: 0.6276 - val_accuracy: 0.6749\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6264 - accuracy: 0.6644 - val_loss: 0.6252 - val_accuracy: 0.6526\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6429 - accuracy: 0.6463 - val_loss: 0.6261 - val_accuracy: 0.6797\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6358 - accuracy: 0.6594 - val_loss: 0.6257 - val_accuracy: 0.6526\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6306 - accuracy: 0.6544 - val_loss: 0.6223 - val_accuracy: 0.6562\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6489 - accuracy: 0.6269 - val_loss: 0.6270 - val_accuracy: 0.6857\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.9030 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0128s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.8132 - accuracy: 0.4345\n",
      "training acc:  0.6268749833106995 , training loss:  0.6488870978355408 , val acc:  0.6856971383094788 , val loss:  0.6270348429679871 , test acc:  0.4344588816165924 , test loss:  0.8131682872772217\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt4/450word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second function around seems though lot always best etc less give either help x & likely \\mathcal means must form made following least bit note small trying order light end anything based done option file start % keep size maybe found us already person world real > reason edit space 0    years check said little read solution process rather non day times list field level someone issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show result course understand user makes open hard life group output fact last idea human working name site whether amount pretty version water assume matter state kind past error method    got color    5 energy great original paper due sense else called character next general range getting god camera control ai current low seem away particular values object free term consider left common 10 wo short words full difference i.e. test old area experience tried < changes online year thought    making although input book call available lower lens thus comes layer needs provide running within everything results feel hand almost source class ask e.g. easy perhaps top useful anyone \\\\ higher google big block created 4 post price access write several tell true future others smaller search bytes clear nothing design exactly n multiple wrong three unless numbers avoid close fine position ' effect days believe # mm sort certain language standard air actual account turn research difficult model yes takes _ length address product game uses normal gives known generally easily computer map cases along seen thanks sound money ones bad step cause terms whole mind rate content company problems play looks history main story select per happen sometimes   added says allow return points therefore context support taking functions 100 writing target e structure options become earth meaning setting theory update head network view n$ gets move larger flash choose inside english examples related reading food body yet 6 children build mode situation random sentence\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6484 - accuracy: 0.9375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0056s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6717 - accuracy: 0.6162 - val_loss: 0.6609 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6590 - accuracy: 0.6544 - val_loss: 0.6569 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6563 - accuracy: 0.6481 - val_loss: 0.6521 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6746 - accuracy: 0.6156 - val_loss: 0.6585 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6544 - accuracy: 0.6494 - val_loss: 0.6458 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6336 - accuracy: 0.6812 - val_loss: 0.6438 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6616 - accuracy: 0.6294 - val_loss: 0.6409 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6612 - accuracy: 0.6275 - val_loss: 0.6442 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6349 - accuracy: 0.6631 - val_loss: 0.6362 - val_accuracy: 0.6526\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6538 - accuracy: 0.6181 - val_loss: 0.6402 - val_accuracy: 0.6653\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6526 - accuracy: 0.6225 - val_loss: 0.6337 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6397 - accuracy: 0.6413 - val_loss: 0.6316 - val_accuracy: 0.6532\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6105 - accuracy: 0.6931 - val_loss: 0.6370 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6306 - accuracy: 0.6637 - val_loss: 0.6274 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6274 - accuracy: 0.6644 - val_loss: 0.6277 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6662 - accuracy: 0.6012 - val_loss: 0.6281 - val_accuracy: 0.6773\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6398 - accuracy: 0.6556 - val_loss: 0.6250 - val_accuracy: 0.6556\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5980 - accuracy: 0.7050 - val_loss: 0.6469 - val_accuracy: 0.6526\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6302 - accuracy: 0.6587 - val_loss: 0.6232 - val_accuracy: 0.6532\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6350 - accuracy: 0.6575 - val_loss: 0.6223 - val_accuracy: 0.6556\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6140 - accuracy: 0.6913 - val_loss: 0.6239 - val_accuracy: 0.6893\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6330 - accuracy: 0.6587 - val_loss: 0.6262 - val_accuracy: 0.6839\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6477 - accuracy: 0.6475 - val_loss: 0.6212 - val_accuracy: 0.6605\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6384 - accuracy: 0.6687 - val_loss: 0.6209 - val_accuracy: 0.6779\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6227 - accuracy: 0.6756 - val_loss: 0.6216 - val_accuracy: 0.6556\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6158 - accuracy: 0.6756 - val_loss: 0.6225 - val_accuracy: 0.6532\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6155 - accuracy: 0.6800 - val_loss: 0.6264 - val_accuracy: 0.6526\n",
      "  1/596 [..............................] - ETA: 8s - loss: 1.1283 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_test_batch_end` time: 0.0134s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.9515 - accuracy: 0.4123\n",
      "training acc:  0.6800000071525574 , training loss:  0.6154998540878296 , val acc:  0.6526442170143127 , val loss:  0.6263992786407471 , test acc:  0.4123322069644928 , test loss:  0.9514980316162109\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_400/opt4/500word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like /   use also get could time need way make using 'm work want - even think first see know people two may 1 much = might different example something used 've question well 're good however case set really number find many since take say 2 problem still new try possible another right things probably point answer go enough + without better data value let change actually ca back 'd system going 'll image long sure able look every code add 3 given high page create power thing part url$ second function around seems though lot always best etc less give either help x & likely \\mathcal means must form made following least bit note small trying order light end anything based done option file start % keep size maybe found us already person world real > reason edit space 0    years check said little read solution process rather non day times list field level someone issue mean similar b key type information single looking line run simple specific large text never often far quite simply word correct instead important come place works \\to put side c usually server show result course understand user makes open hard life group output fact last idea human working name site whether amount pretty version water assume matter state kind past error method    got color    5 energy great original paper due sense else called character next general range getting god camera control ai current low seem away particular values object free term consider left common 10 wo short words full difference i.e. test old area experience tried < changes online year thought    making although input book call available lower lens thus comes layer needs provide running within everything results feel hand almost source class ask e.g. easy perhaps top useful anyone \\\\ higher google big block created 4 post price access write several tell true future others smaller search bytes clear nothing design exactly n multiple wrong three unless numbers avoid close fine position ' effect days believe # mm sort certain language standard air actual account turn research difficult model yes takes _ length address product game uses normal gives known generally easily computer map cases along seen thanks sound money ones bad step cause terms whole mind rate content company problems play looks history main story select per happen sometimes   added says allow return points therefore context support taking functions 100 writing target e structure options become earth meaning setting theory update head network view n$ gets move larger flash choose inside english examples related reading food body yet 6 children build mode situation random sentence parts characters job rules necessary directly guess half link tool remember string assuming background slightly client stop images worth apply depends force instance personal later local whatever approach fire car quality interesting entire knowledge major goes please mentioned magic software hope de web rule algorithm pages noise outside save especially\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6695 - accuracy: 0.6450WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_test_batch_end` time: 0.0063s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6695 - accuracy: 0.6450 - val_loss: 0.6619 - val_accuracy: 0.6526\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6718 - accuracy: 0.6388 - val_loss: 0.6580 - val_accuracy: 0.6526\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6793 - accuracy: 0.6075 - val_loss: 0.6561 - val_accuracy: 0.6526\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6520 - accuracy: 0.6587 - val_loss: 0.6531 - val_accuracy: 0.6526\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6562 - accuracy: 0.6506 - val_loss: 0.6503 - val_accuracy: 0.6526\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6506 - accuracy: 0.6531 - val_loss: 0.6481 - val_accuracy: 0.6526\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6495 - accuracy: 0.6475 - val_loss: 0.6445 - val_accuracy: 0.6526\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6436 - accuracy: 0.6619 - val_loss: 0.6424 - val_accuracy: 0.6526\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6714 - accuracy: 0.6069 - val_loss: 0.6484 - val_accuracy: 0.6532\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6480 - accuracy: 0.6494 - val_loss: 0.6387 - val_accuracy: 0.6526\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6369 - accuracy: 0.6538 - val_loss: 0.6371 - val_accuracy: 0.6526\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6452 - accuracy: 0.6344 - val_loss: 0.6343 - val_accuracy: 0.6526\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6333 - accuracy: 0.6612 - val_loss: 0.6331 - val_accuracy: 0.6526\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6344 - accuracy: 0.6606 - val_loss: 0.6343 - val_accuracy: 0.6526\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6192 - accuracy: 0.6775 - val_loss: 0.6310 - val_accuracy: 0.6526\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6347 - accuracy: 0.6431 - val_loss: 0.6294 - val_accuracy: 0.6526\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6393 - accuracy: 0.6413 - val_loss: 0.6281 - val_accuracy: 0.6526\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6449 - accuracy: 0.6350 - val_loss: 0.6296 - val_accuracy: 0.6737\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6527 - accuracy: 0.6125 - val_loss: 0.6271 - val_accuracy: 0.6611\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6335 - accuracy: 0.6519 - val_loss: 0.6258 - val_accuracy: 0.6526\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6391 - accuracy: 0.6431 - val_loss: 0.6281 - val_accuracy: 0.6833\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6210 - accuracy: 0.6750 - val_loss: 0.6241 - val_accuracy: 0.6532\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6249 - accuracy: 0.6556 - val_loss: 0.6244 - val_accuracy: 0.6526\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6359 - accuracy: 0.6606 - val_loss: 0.6226 - val_accuracy: 0.6556\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6305 - accuracy: 0.6562 - val_loss: 0.6286 - val_accuracy: 0.6797\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6424 - accuracy: 0.6637 - val_loss: 0.6261 - val_accuracy: 0.6827\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6356 - accuracy: 0.6812 - val_loss: 0.6214 - val_accuracy: 0.6767\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6267 - accuracy: 0.6606 - val_loss: 0.6209 - val_accuracy: 0.6767\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6293 - accuracy: 0.6750 - val_loss: 0.6211 - val_accuracy: 0.6851\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6377 - accuracy: 0.6631 - val_loss: 0.6206 - val_accuracy: 0.6677\n",
      "  1/596 [..............................] - ETA: 3s - loss: 1.0287 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0138s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.8925 - accuracy: 0.4212\n",
      "training acc:  0.6631249785423279 , training loss:  0.637657642364502 , val acc:  0.6676682829856873 , val loss:  0.6205660104751587 , test acc:  0.421245813369751 , test loss:  0.8925042748451233\n",
      "\n",
      "Started evaluation for: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_600.csv\n",
      "Loading training file from path:  /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_600.csv\n",
      "(5543, 6)\n",
      "Loading testing file from path:  /home/sukanya/PhD/Datasets/PAN SCD/pan19-style-change-detection/processed/test.csv\n",
      "(9538, 5)\n",
      "opt1\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt1/50word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6255 - accuracy: 0.7081 - val_loss: 0.6331 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6285 - accuracy: 0.7031 - val_loss: 0.6285 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6085 - accuracy: 0.7250 - val_loss: 0.6334 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6282 - accuracy: 0.7013 - val_loss: 0.6210 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6188 - accuracy: 0.7044 - val_loss: 0.6182 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6221 - accuracy: 0.6994 - val_loss: 0.6190 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5979 - accuracy: 0.7231 - val_loss: 0.6122 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6014 - accuracy: 0.7188 - val_loss: 0.6095 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5982 - accuracy: 0.7181 - val_loss: 0.6071 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6147 - accuracy: 0.6913 - val_loss: 0.6126 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5952 - accuracy: 0.7088 - val_loss: 0.6076 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6098 - accuracy: 0.6938 - val_loss: 0.6006 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5975 - accuracy: 0.7044 - val_loss: 0.6015 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5912 - accuracy: 0.7113 - val_loss: 0.5944 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5909 - accuracy: 0.7125 - val_loss: 0.5937 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6040 - accuracy: 0.6925 - val_loss: 0.5898 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5912 - accuracy: 0.7031 - val_loss: 0.5934 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5976 - accuracy: 0.6963 - val_loss: 0.5869 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5914 - accuracy: 0.6981 - val_loss: 0.5837 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5903 - accuracy: 0.6963 - val_loss: 0.5817 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5884 - accuracy: 0.6950 - val_loss: 0.5818 - val_accuracy: 0.7020\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5808 - accuracy: 0.7006 - val_loss: 0.5812 - val_accuracy: 0.7020\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5883 - accuracy: 0.7006 - val_loss: 0.5766 - val_accuracy: 0.7020\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5758 - accuracy: 0.7150 - val_loss: 0.5767 - val_accuracy: 0.7020\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5615 - accuracy: 0.7269 - val_loss: 0.5775 - val_accuracy: 0.7020\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5822 - accuracy: 0.6994 - val_loss: 0.5732 - val_accuracy: 0.7020\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5787 - accuracy: 0.6913 - val_loss: 0.5818 - val_accuracy: 0.7455\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5854 - accuracy: 0.7106 - val_loss: 0.5742 - val_accuracy: 0.7020\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5758 - accuracy: 0.7013 - val_loss: 0.5694 - val_accuracy: 0.7020\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5630 - accuracy: 0.7219 - val_loss: 0.5706 - val_accuracy: 0.7020\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.1242 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0124s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 1.0776 - accuracy: 0.4123\n",
      "training acc:  0.721875011920929 , training loss:  0.5629692673683167 , val acc:  0.7019927501678467 , val loss:  0.570579469203949 , test acc:  0.4123322069644928 , test loss:  1.0775686502456665\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt1/100word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6940 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_train_batch_end` time: 0.0104s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6322 - accuracy: 0.7075 - val_loss: 0.6295 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6246 - accuracy: 0.7063 - val_loss: 0.6254 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6172 - accuracy: 0.7106 - val_loss: 0.6217 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6311 - accuracy: 0.6894 - val_loss: 0.6191 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6264 - accuracy: 0.6894 - val_loss: 0.6157 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6071 - accuracy: 0.7131 - val_loss: 0.6153 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5919 - accuracy: 0.7250 - val_loss: 0.6220 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5991 - accuracy: 0.7144 - val_loss: 0.6079 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6106 - accuracy: 0.7000 - val_loss: 0.6039 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6071 - accuracy: 0.7019 - val_loss: 0.6059 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6086 - accuracy: 0.6944 - val_loss: 0.5982 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5982 - accuracy: 0.7038 - val_loss: 0.5953 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5976 - accuracy: 0.7038 - val_loss: 0.5926 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5770 - accuracy: 0.7250 - val_loss: 0.5914 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5926 - accuracy: 0.7025 - val_loss: 0.5893 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5847 - accuracy: 0.7113 - val_loss: 0.5868 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5986 - accuracy: 0.6969 - val_loss: 0.5844 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6031 - accuracy: 0.6806 - val_loss: 0.5812 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5747 - accuracy: 0.7150 - val_loss: 0.5810 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5798 - accuracy: 0.7100 - val_loss: 0.5767 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5904 - accuracy: 0.6825 - val_loss: 0.5751 - val_accuracy: 0.7020\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5755 - accuracy: 0.7081 - val_loss: 0.5749 - val_accuracy: 0.7020\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5633 - accuracy: 0.7194 - val_loss: 0.5750 - val_accuracy: 0.7020\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5879 - accuracy: 0.6906 - val_loss: 0.5705 - val_accuracy: 0.7020\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5542 - accuracy: 0.7275 - val_loss: 0.5704 - val_accuracy: 0.7020\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5810 - accuracy: 0.7000 - val_loss: 0.5701 - val_accuracy: 0.7264\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5578 - accuracy: 0.7212 - val_loss: 0.5695 - val_accuracy: 0.7020\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5854 - accuracy: 0.6956 - val_loss: 0.5705 - val_accuracy: 0.7020\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5823 - accuracy: 0.6994 - val_loss: 0.5630 - val_accuracy: 0.7038\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5493 - accuracy: 0.7331 - val_loss: 0.5667 - val_accuracy: 0.7020\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.1361 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0114s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 1.0861 - accuracy: 0.4123\n",
      "training acc:  0.7331249713897705 , training loss:  0.5493150949478149 , val acc:  0.7019927501678467 , val loss:  0.5667343139648438 , test acc:  0.4123322069644928 , test loss:  1.0861046314239502\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt1/150word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most me much 1 each such different he something example used now question 've well good its am been however really being after 're case number set take since still many say who find 2 problem new those possible another try both between too things were while probably point had our\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.7896 - accuracy: 0.5250 - val_loss: 0.6218 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6243 - accuracy: 0.6994 - val_loss: 0.6192 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6278 - accuracy: 0.6888 - val_loss: 0.6183 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6210 - accuracy: 0.6938 - val_loss: 0.6178 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6055 - accuracy: 0.7156 - val_loss: 0.6107 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6237 - accuracy: 0.6875 - val_loss: 0.6075 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6172 - accuracy: 0.6919 - val_loss: 0.6066 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6048 - accuracy: 0.7031 - val_loss: 0.6035 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6175 - accuracy: 0.6894 - val_loss: 0.5993 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5990 - accuracy: 0.7025 - val_loss: 0.5962 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5878 - accuracy: 0.7175 - val_loss: 0.5965 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5791 - accuracy: 0.7269 - val_loss: 0.5925 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5940 - accuracy: 0.7031 - val_loss: 0.5957 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5911 - accuracy: 0.7019 - val_loss: 0.5916 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6003 - accuracy: 0.6944 - val_loss: 0.5841 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5901 - accuracy: 0.7025 - val_loss: 0.5826 - val_accuracy: 0.7020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5735 - accuracy: 0.7244 - val_loss: 0.5880 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5932 - accuracy: 0.6938 - val_loss: 0.5779 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5974 - accuracy: 0.6875 - val_loss: 0.5787 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5520 - accuracy: 0.7412 - val_loss: 0.5895 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5812 - accuracy: 0.7138 - val_loss: 0.5882 - val_accuracy: 0.7572\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.8737 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0141s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.9030 - accuracy: 0.4247\n",
      "training acc:  0.7137500047683716 , training loss:  0.5812435746192932 , val acc:  0.7572463750839233 , val loss:  0.5881614685058594 , test acc:  0.424706369638443 , test loss:  0.9029593467712402\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt1/200word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most me much 1 each such different he something example used now question 've well good its am been however really being after 're case number set take since still many say who find 2 problem new those possible another try both between too things were while probably point had our did over why enough go right + answer let without value his through better actually before system data back ca 'll going 'd change down long every sure own given image able look high code 3 \\mathcal power add create thing part off second less page having seems function though\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6391 - accuracy: 0.6975 - val_loss: 0.6303 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6197 - accuracy: 0.7088 - val_loss: 0.6307 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6228 - accuracy: 0.7063 - val_loss: 0.6210 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6194 - accuracy: 0.7106 - val_loss: 0.6164 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6202 - accuracy: 0.6988 - val_loss: 0.6165 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6070 - accuracy: 0.7119 - val_loss: 0.6123 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6099 - accuracy: 0.7038 - val_loss: 0.6058 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5954 - accuracy: 0.7150 - val_loss: 0.6012 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6076 - accuracy: 0.6956 - val_loss: 0.5970 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6006 - accuracy: 0.7013 - val_loss: 0.5946 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5912 - accuracy: 0.7063 - val_loss: 0.5899 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5880 - accuracy: 0.7144 - val_loss: 0.5870 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5882 - accuracy: 0.7094 - val_loss: 0.5883 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5991 - accuracy: 0.6938 - val_loss: 0.5882 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5758 - accuracy: 0.7150 - val_loss: 0.5806 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5540 - accuracy: 0.7350 - val_loss: 0.5806 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5735 - accuracy: 0.7144 - val_loss: 0.5737 - val_accuracy: 0.7029\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5752 - accuracy: 0.7100 - val_loss: 0.5732 - val_accuracy: 0.7120\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5691 - accuracy: 0.7175 - val_loss: 0.5721 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5821 - accuracy: 0.7006 - val_loss: 0.5671 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5784 - accuracy: 0.7019 - val_loss: 0.5648 - val_accuracy: 0.7038\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5805 - accuracy: 0.7006 - val_loss: 0.5638 - val_accuracy: 0.7264\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5643 - accuracy: 0.7138 - val_loss: 0.5611 - val_accuracy: 0.7183\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5653 - accuracy: 0.7281 - val_loss: 0.5609 - val_accuracy: 0.7038\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5673 - accuracy: 0.7113 - val_loss: 0.5585 - val_accuracy: 0.7246\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5714 - accuracy: 0.7169 - val_loss: 0.5581 - val_accuracy: 0.7101\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5532 - accuracy: 0.7262 - val_loss: 0.5618 - val_accuracy: 0.7029\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5663 - accuracy: 0.7131 - val_loss: 0.5550 - val_accuracy: 0.7264\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5416 - accuracy: 0.7412 - val_loss: 0.5615 - val_accuracy: 0.7029\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5579 - accuracy: 0.7237 - val_loss: 0.5537 - val_accuracy: 0.7264\n",
      "  1/596 [..............................] - ETA: 3s - loss: 1.1205 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0130s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.1076 - accuracy: 0.4202\n",
      "training acc:  0.7237499952316284 , training loss:  0.5578502416610718 , val acc:  0.7264492511749268 , val loss:  0.5537075400352478 , test acc:  0.42019715905189514 , test loss:  1.1075855493545532\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt1/250word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most me much 1 each such different he something example used now question 've well good its am been however really being after 're case number set take since still many say who find 2 problem new those possible another try both between too things were while probably point had our did over why enough go right + answer let without value his through better actually before system data back ca 'll going 'd change down long every sure own given image able look high code 3 \\mathcal power add create thing part off second less page having seems function though likely around help lot give always best form either url$ etc means must light bit x above few she made small trying keep end least us world following % option size anything doing note maybe done start day based space 0 found file order real reason said already years read\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6991 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0114s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6449 - accuracy: 0.6825 - val_loss: 0.6307 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6317 - accuracy: 0.6963 - val_loss: 0.6272 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6030 - accuracy: 0.7306 - val_loss: 0.6268 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6026 - accuracy: 0.7275 - val_loss: 0.6240 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6397 - accuracy: 0.6787 - val_loss: 0.6183 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6067 - accuracy: 0.7163 - val_loss: 0.6160 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6200 - accuracy: 0.6956 - val_loss: 0.6135 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5996 - accuracy: 0.7194 - val_loss: 0.6088 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6172 - accuracy: 0.6919 - val_loss: 0.6061 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6162 - accuracy: 0.6906 - val_loss: 0.6033 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6035 - accuracy: 0.7031 - val_loss: 0.6002 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5938 - accuracy: 0.7144 - val_loss: 0.5974 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5931 - accuracy: 0.7081 - val_loss: 0.5976 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5980 - accuracy: 0.7025 - val_loss: 0.5913 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5757 - accuracy: 0.7212 - val_loss: 0.5926 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5769 - accuracy: 0.7194 - val_loss: 0.5855 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5732 - accuracy: 0.7200 - val_loss: 0.5859 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5833 - accuracy: 0.7056 - val_loss: 0.5810 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5777 - accuracy: 0.7044 - val_loss: 0.5802 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5763 - accuracy: 0.7125 - val_loss: 0.5763 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5752 - accuracy: 0.7144 - val_loss: 0.5805 - val_accuracy: 0.7020\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5773 - accuracy: 0.7075 - val_loss: 0.5715 - val_accuracy: 0.7020\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5655 - accuracy: 0.7231 - val_loss: 0.5696 - val_accuracy: 0.7020\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5675 - accuracy: 0.7131 - val_loss: 0.5680 - val_accuracy: 0.7038\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5821 - accuracy: 0.7081 - val_loss: 0.5697 - val_accuracy: 0.7020\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5689 - accuracy: 0.7081 - val_loss: 0.5649 - val_accuracy: 0.7038\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5771 - accuracy: 0.6988 - val_loss: 0.5640 - val_accuracy: 0.7029\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5914 - accuracy: 0.6869 - val_loss: 0.5660 - val_accuracy: 0.7473\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5714 - accuracy: 0.7188 - val_loss: 0.5606 - val_accuracy: 0.7111\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5881 - accuracy: 0.7075 - val_loss: 0.5608 - val_accuracy: 0.7428\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.9949 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0134s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.0113 - accuracy: 0.4227\n",
      "training acc:  0.7074999809265137 , training loss:  0.5881439447402954 , val acc:  0.7427536249160767 , val loss:  0.5608278512954712 , test acc:  0.4227139353752136 , test loss:  1.0112916231155396\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt1/300word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most me much 1 each such different he something example used now question 've well good its am been however really being after 're case number set take since still many say who find 2 problem new those possible another try both between too things were while probably point had our did over why enough go right + answer let without value his through better actually before system data back ca 'll going 'd change down long every sure own given image able look high code 3 \\mathcal power add create thing part off second less page having seems function though likely around help lot give always best form either url$ etc means must light bit x above few she made small trying keep end least us world following % option size anything doing note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution again specific field once check similar > key simple information times far often come never single type c word her line looking text side quite list him important usually instead place works result b\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7769 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0167s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6838 - accuracy: 0.6156 - val_loss: 0.6253 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6232 - accuracy: 0.7050 - val_loss: 0.6219 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6310 - accuracy: 0.6925 - val_loss: 0.6182 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6125 - accuracy: 0.7106 - val_loss: 0.6138 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6204 - accuracy: 0.6913 - val_loss: 0.6215 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6267 - accuracy: 0.6787 - val_loss: 0.6166 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6042 - accuracy: 0.7088 - val_loss: 0.6054 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6052 - accuracy: 0.7056 - val_loss: 0.5981 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6014 - accuracy: 0.6994 - val_loss: 0.5950 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6006 - accuracy: 0.6988 - val_loss: 0.5906 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5891 - accuracy: 0.7119 - val_loss: 0.5872 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5812 - accuracy: 0.7163 - val_loss: 0.5842 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5728 - accuracy: 0.7244 - val_loss: 0.5813 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5966 - accuracy: 0.6925 - val_loss: 0.5785 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5788 - accuracy: 0.7088 - val_loss: 0.5777 - val_accuracy: 0.7038\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5908 - accuracy: 0.6944 - val_loss: 0.5733 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5759 - accuracy: 0.7088 - val_loss: 0.5707 - val_accuracy: 0.7029\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5872 - accuracy: 0.6981 - val_loss: 0.5740 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5817 - accuracy: 0.7075 - val_loss: 0.5677 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5676 - accuracy: 0.7094 - val_loss: 0.5669 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5672 - accuracy: 0.7281 - val_loss: 0.5717 - val_accuracy: 0.7020\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5783 - accuracy: 0.7094 - val_loss: 0.5618 - val_accuracy: 0.7038\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5609 - accuracy: 0.7200 - val_loss: 0.5633 - val_accuracy: 0.7482\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5658 - accuracy: 0.7231 - val_loss: 0.5614 - val_accuracy: 0.7029\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5919 - accuracy: 0.6981 - val_loss: 0.5576 - val_accuracy: 0.7428\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5689 - accuracy: 0.7206 - val_loss: 0.5657 - val_accuracy: 0.7020\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5442 - accuracy: 0.7381 - val_loss: 0.5591 - val_accuracy: 0.7038\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5748 - accuracy: 0.7100 - val_loss: 0.5570 - val_accuracy: 0.7554\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5730 - accuracy: 0.7100 - val_loss: 0.5532 - val_accuracy: 0.7418\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5372 - accuracy: 0.7481 - val_loss: 0.5538 - val_accuracy: 0.7228\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.1740 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0133s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.1605 - accuracy: 0.4183\n",
      "training acc:  0.7481250166893005 , training loss:  0.5372112989425659 , val acc:  0.7228260636329651 , val loss:  0.5538066625595093 , test acc:  0.4183095693588257 , test loss:  1.1605437994003296\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt1/350word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most me much 1 each such different he something example used now question 've well good its am been however really being after 're case number set take since still many say who find 2 problem new those possible another try both between too things were while probably point had our did over why enough go right + answer let without value his through better actually before system data back ca 'll going 'd change down long every sure own given image able look high code 3 \\mathcal power add create thing part off second less page having seems function though likely around help lot give always best form either url$ etc means must light bit x above few she made small trying keep end least us world following % option size anything doing note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution again specific field once check similar > key simple information times far often come never single type c word her line looking text side quite list him important usually instead place works result b life run course put idea fact output open makes correct until hard understand show server group human last working assume water amount whether past pretty itself matter under site paper state god color user ai control range kind next < method due error name great sense version low 5 called\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6296 - accuracy: 0.7056 - val_loss: 0.6317 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6178 - accuracy: 0.7138 - val_loss: 0.6246 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6141 - accuracy: 0.7125 - val_loss: 0.6197 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6333 - accuracy: 0.6812 - val_loss: 0.6205 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6153 - accuracy: 0.7063 - val_loss: 0.6135 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6094 - accuracy: 0.7056 - val_loss: 0.6051 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6013 - accuracy: 0.7100 - val_loss: 0.6010 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6006 - accuracy: 0.7031 - val_loss: 0.6060 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6114 - accuracy: 0.6881 - val_loss: 0.5927 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6022 - accuracy: 0.6894 - val_loss: 0.5896 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5905 - accuracy: 0.7131 - val_loss: 0.5912 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5773 - accuracy: 0.7188 - val_loss: 0.5888 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5858 - accuracy: 0.7063 - val_loss: 0.5818 - val_accuracy: 0.7029\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5826 - accuracy: 0.7025 - val_loss: 0.5799 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5751 - accuracy: 0.7088 - val_loss: 0.5730 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5774 - accuracy: 0.7056 - val_loss: 0.5727 - val_accuracy: 0.7210\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5568 - accuracy: 0.7281 - val_loss: 0.5685 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5793 - accuracy: 0.7025 - val_loss: 0.5657 - val_accuracy: 0.7083\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5672 - accuracy: 0.7075 - val_loss: 0.5637 - val_accuracy: 0.7246\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5637 - accuracy: 0.7256 - val_loss: 0.5615 - val_accuracy: 0.7056\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5681 - accuracy: 0.7156 - val_loss: 0.5596 - val_accuracy: 0.7111\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5593 - accuracy: 0.7150 - val_loss: 0.5582 - val_accuracy: 0.7120\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5504 - accuracy: 0.7350 - val_loss: 0.5665 - val_accuracy: 0.7020\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5769 - accuracy: 0.7094 - val_loss: 0.5550 - val_accuracy: 0.7283\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5851 - accuracy: 0.7050 - val_loss: 0.5593 - val_accuracy: 0.7536\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5645 - accuracy: 0.7163 - val_loss: 0.5546 - val_accuracy: 0.7228\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5540 - accuracy: 0.7406 - val_loss: 0.5519 - val_accuracy: 0.7418\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5920 - accuracy: 0.7075 - val_loss: 0.5517 - val_accuracy: 0.7391\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5584 - accuracy: 0.7437 - val_loss: 0.5511 - val_accuracy: 0.7409\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5573 - accuracy: 0.7262 - val_loss: 0.5501 - val_accuracy: 0.7518\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.0636 - accuracy: 0.4242\n",
      "training acc:  0.7262499928474426 , training loss:  0.5573026537895203 , val acc:  0.7518116235733032 , val loss:  0.5500866174697876 , test acc:  0.42418205738067627 , test loss:  1.063637375831604\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt1/400word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most me much 1 each such different he something example used now question 've well good its am been however really being after 're case number set take since still many say who find 2 problem new those possible another try both between too things were while probably point had our did over why enough go right + answer let without value his through better actually before system data back ca 'll going 'd change down long every sure own given image able look high code 3 \\mathcal power add create thing part off second less page having seems function though likely around help lot give always best form either url$ etc means must light bit x above few she made small trying keep end least us world following % option size anything doing note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution again specific field once check similar > key simple information times far often come never single type c word her line looking text side quite list him important usually instead place works result b life run course put idea fact output open makes correct until hard understand show server group human last working assume water amount whether past pretty itself matter under site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character getting    else left below free got seem object 10 difference short full i.e. values camera online wo away available term common words provide old area needs current making lower everything lens experience although almost feel easy tried comes further book thought future\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6226 - accuracy: 0.7269 - val_loss: 0.6417 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6411 - accuracy: 0.6919 - val_loss: 0.6383 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6466 - accuracy: 0.6806 - val_loss: 0.6340 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6139 - accuracy: 0.7194 - val_loss: 0.6272 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6205 - accuracy: 0.7069 - val_loss: 0.6218 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6215 - accuracy: 0.7075 - val_loss: 0.6175 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6088 - accuracy: 0.7131 - val_loss: 0.6132 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6052 - accuracy: 0.7144 - val_loss: 0.6112 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6030 - accuracy: 0.7106 - val_loss: 0.6054 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6168 - accuracy: 0.6900 - val_loss: 0.6055 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6244 - accuracy: 0.6725 - val_loss: 0.6016 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6057 - accuracy: 0.6969 - val_loss: 0.5966 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6090 - accuracy: 0.6913 - val_loss: 0.5931 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6098 - accuracy: 0.6856 - val_loss: 0.5908 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5706 - accuracy: 0.7287 - val_loss: 0.5946 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5928 - accuracy: 0.7031 - val_loss: 0.5850 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5939 - accuracy: 0.6931 - val_loss: 0.5832 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5874 - accuracy: 0.7056 - val_loss: 0.5800 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5988 - accuracy: 0.6894 - val_loss: 0.5795 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5901 - accuracy: 0.6906 - val_loss: 0.5761 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.5659 - accuracy: 0.7188 - val_loss: 0.5782 - val_accuracy: 0.7020\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5920 - accuracy: 0.6944 - val_loss: 0.5720 - val_accuracy: 0.7020\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5630 - accuracy: 0.7212 - val_loss: 0.5713 - val_accuracy: 0.7020\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5613 - accuracy: 0.7219 - val_loss: 0.5683 - val_accuracy: 0.7020\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5713 - accuracy: 0.7056 - val_loss: 0.5664 - val_accuracy: 0.7083\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5523 - accuracy: 0.7287 - val_loss: 0.5673 - val_accuracy: 0.7020\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5638 - accuracy: 0.7106 - val_loss: 0.5630 - val_accuracy: 0.7056\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5676 - accuracy: 0.7069 - val_loss: 0.5619 - val_accuracy: 0.7283\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5525 - accuracy: 0.7262 - val_loss: 0.5618 - val_accuracy: 0.7029\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5854 - accuracy: 0.7088 - val_loss: 0.5624 - val_accuracy: 0.7029\n",
      "  1/596 [..............................] - ETA: 8s - loss: 1.1356 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0142s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.0887 - accuracy: 0.4125\n",
      "training acc:  0.7087500095367432 , training loss:  0.5854149460792542 , val acc:  0.7028985619544983 , val loss:  0.5623995661735535 , test acc:  0.4125419557094574 , test loss:  1.0887283086776733\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt1/450word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most me much 1 each such different he something example used now question 've well good its am been however really being after 're case number set take since still many say who find 2 problem new those possible another try both between too things were while probably point had our did over why enough go right + answer let without value his through better actually before system data back ca 'll going 'd change down long every sure own given image able look high code 3 \\mathcal power add create thing part off second less page having seems function though likely around help lot give always best form either url$ etc means must light bit x above few she made small trying keep end least us world following % option size anything doing note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution again specific field once check similar > key simple information times far often come never single type c word her line looking text side quite list him important usually instead place works result b life run course put idea fact output open makes correct until hard understand show server group human last working assume water amount whether past pretty itself matter under site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character getting    else left below free got seem object 10 difference short full i.e. values camera online wo away available term common words provide old area needs current making lower everything lens experience although almost feel easy tried comes further book thought future useful call higher source smaller price test changes thus top results within    true input write ask created avoid unless perhaps bytes hand several \\\\ close big tell class design layer mm numbers effect air clear position    others running block access 4 days wrong e.g. against exactly anyone multiple\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6364 - accuracy: 0.7044 - val_loss: 0.6395 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6316 - accuracy: 0.7019 - val_loss: 0.6328 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6344 - accuracy: 0.6963 - val_loss: 0.6274 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6294 - accuracy: 0.6969 - val_loss: 0.6238 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6095 - accuracy: 0.7175 - val_loss: 0.6193 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6075 - accuracy: 0.7156 - val_loss: 0.6156 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6124 - accuracy: 0.7094 - val_loss: 0.6110 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5990 - accuracy: 0.7169 - val_loss: 0.6080 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6219 - accuracy: 0.6775 - val_loss: 0.6098 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6035 - accuracy: 0.7025 - val_loss: 0.5991 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6019 - accuracy: 0.7000 - val_loss: 0.5944 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5864 - accuracy: 0.7181 - val_loss: 0.5918 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5850 - accuracy: 0.7156 - val_loss: 0.5917 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5840 - accuracy: 0.7069 - val_loss: 0.5848 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5923 - accuracy: 0.6969 - val_loss: 0.5813 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5697 - accuracy: 0.7212 - val_loss: 0.5784 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5836 - accuracy: 0.7031 - val_loss: 0.5771 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5774 - accuracy: 0.7106 - val_loss: 0.5739 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5687 - accuracy: 0.7156 - val_loss: 0.5708 - val_accuracy: 0.7029\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5970 - accuracy: 0.6925 - val_loss: 0.5695 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5913 - accuracy: 0.6906 - val_loss: 0.5745 - val_accuracy: 0.7563\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5634 - accuracy: 0.7212 - val_loss: 0.5654 - val_accuracy: 0.7156\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5705 - accuracy: 0.7169 - val_loss: 0.5640 - val_accuracy: 0.7228\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5611 - accuracy: 0.7244 - val_loss: 0.5653 - val_accuracy: 0.7020\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5567 - accuracy: 0.7331 - val_loss: 0.5605 - val_accuracy: 0.7129\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5653 - accuracy: 0.7175 - val_loss: 0.5591 - val_accuracy: 0.7228\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5545 - accuracy: 0.7337 - val_loss: 0.5701 - val_accuracy: 0.7020\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5842 - accuracy: 0.7038 - val_loss: 0.5566 - val_accuracy: 0.7373\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5565 - accuracy: 0.7275 - val_loss: 0.5612 - val_accuracy: 0.7038\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5674 - accuracy: 0.7212 - val_loss: 0.5545 - val_accuracy: 0.7301\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.0590 - accuracy: 0.4218\n",
      "training acc:  0.7212499976158142 , training loss:  0.5673617720603943 , val acc:  0.7300724387168884 , val loss:  0.5545405745506287 , test acc:  0.4217701256275177 , test loss:  1.0589662790298462\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt1/500word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most me much 1 each such different he something example used now question 've well good its am been however really being after 're case number set take since still many say who find 2 problem new those possible another try both between too things were while probably point had our did over why enough go right + answer let without value his through better actually before system data back ca 'll going 'd change down long every sure own given image able look high code 3 \\mathcal power add create thing part off second less page having seems function though likely around help lot give always best form either url$ etc means must light bit x above few she made small trying keep end least us world following % option size anything doing note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution again specific field once check similar > key simple information times far often come never single type c word her line looking text side quite list him important usually instead place works result b life run course put idea fact output open makes correct until hard understand show server group human last working assume water amount whether past pretty itself matter under site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character getting    else left below free got seem object 10 difference short full i.e. values camera online wo away available term common words provide old area needs current making lower everything lens experience although almost feel easy tried comes further book thought future useful call higher source smaller price test changes thus top results within    true input write ask created avoid unless perhaps bytes hand several \\\\ close big tell class design layer mm numbers effect air clear position    others running block access 4 days wrong e.g. against exactly anyone multiple three takes actual post fine product believe difficult map game target language search s standard story ' nothing terms n model sort yes # length larger m cause generally research sound along cases google certain company known money turn earth gives mind uses whole return ones taking history easily theory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6207 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0211s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6425 - accuracy: 0.7069 - val_loss: 0.6389 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6430 - accuracy: 0.6950 - val_loss: 0.6334 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6335 - accuracy: 0.6950 - val_loss: 0.6306 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6247 - accuracy: 0.7038 - val_loss: 0.6231 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6220 - accuracy: 0.7019 - val_loss: 0.6179 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6027 - accuracy: 0.7200 - val_loss: 0.6149 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6017 - accuracy: 0.7163 - val_loss: 0.6085 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6100 - accuracy: 0.6981 - val_loss: 0.6096 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6099 - accuracy: 0.6931 - val_loss: 0.5991 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6052 - accuracy: 0.6994 - val_loss: 0.5972 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6032 - accuracy: 0.6981 - val_loss: 0.5919 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5844 - accuracy: 0.7156 - val_loss: 0.5912 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5923 - accuracy: 0.7044 - val_loss: 0.5893 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5792 - accuracy: 0.7200 - val_loss: 0.5816 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5871 - accuracy: 0.6988 - val_loss: 0.5781 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5739 - accuracy: 0.7144 - val_loss: 0.5814 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5869 - accuracy: 0.6981 - val_loss: 0.5761 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5809 - accuracy: 0.7044 - val_loss: 0.5693 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5834 - accuracy: 0.7019 - val_loss: 0.5682 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5462 - accuracy: 0.7331 - val_loss: 0.5669 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5643 - accuracy: 0.7175 - val_loss: 0.5638 - val_accuracy: 0.7029\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5647 - accuracy: 0.7088 - val_loss: 0.5603 - val_accuracy: 0.7219\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5621 - accuracy: 0.7237 - val_loss: 0.5661 - val_accuracy: 0.7020\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5743 - accuracy: 0.7125 - val_loss: 0.5579 - val_accuracy: 0.7373\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5708 - accuracy: 0.7225 - val_loss: 0.5585 - val_accuracy: 0.7065\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5748 - accuracy: 0.7094 - val_loss: 0.5559 - val_accuracy: 0.7527\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5631 - accuracy: 0.7294 - val_loss: 0.5610 - val_accuracy: 0.7038\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5475 - accuracy: 0.7369 - val_loss: 0.5553 - val_accuracy: 0.7165\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5824 - accuracy: 0.7138 - val_loss: 0.5527 - val_accuracy: 0.7310\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5621 - accuracy: 0.7319 - val_loss: 0.5518 - val_accuracy: 0.7355\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.0993 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_test_batch_end` time: 0.0067s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.0876 - accuracy: 0.4220\n",
      "training acc:  0.7318750023841858 , training loss:  0.5620684623718262 , val acc:  0.7355072498321533 , val loss:  0.5517718195915222 , test acc:  0.4219798743724823 , test loss:  1.0875657796859741\n",
      "\n",
      "opt2\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt2/50word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6276 - accuracy: 0.7075 - val_loss: 0.6287 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6156 - accuracy: 0.7131 - val_loss: 0.6260 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6320 - accuracy: 0.6938 - val_loss: 0.6203 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6229 - accuracy: 0.7006 - val_loss: 0.6174 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6178 - accuracy: 0.7013 - val_loss: 0.6166 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5978 - accuracy: 0.7231 - val_loss: 0.6198 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6254 - accuracy: 0.6938 - val_loss: 0.6094 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5957 - accuracy: 0.7206 - val_loss: 0.6074 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5997 - accuracy: 0.7175 - val_loss: 0.6078 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5929 - accuracy: 0.7169 - val_loss: 0.6042 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5975 - accuracy: 0.7106 - val_loss: 0.6065 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6069 - accuracy: 0.6894 - val_loss: 0.5977 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6083 - accuracy: 0.6900 - val_loss: 0.5936 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5923 - accuracy: 0.7050 - val_loss: 0.5974 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5900 - accuracy: 0.7044 - val_loss: 0.5905 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5806 - accuracy: 0.7163 - val_loss: 0.5876 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5968 - accuracy: 0.6931 - val_loss: 0.5833 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5685 - accuracy: 0.7231 - val_loss: 0.5864 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5767 - accuracy: 0.7169 - val_loss: 0.5816 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5903 - accuracy: 0.6906 - val_loss: 0.5904 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5804 - accuracy: 0.7031 - val_loss: 0.5790 - val_accuracy: 0.7020\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5944 - accuracy: 0.6862 - val_loss: 0.5740 - val_accuracy: 0.7020\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5841 - accuracy: 0.6981 - val_loss: 0.5723 - val_accuracy: 0.7020\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5780 - accuracy: 0.7006 - val_loss: 0.5704 - val_accuracy: 0.7020\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5774 - accuracy: 0.6950 - val_loss: 0.5684 - val_accuracy: 0.7020\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5643 - accuracy: 0.7063 - val_loss: 0.5663 - val_accuracy: 0.7020\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5652 - accuracy: 0.7131 - val_loss: 0.5660 - val_accuracy: 0.7020\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5733 - accuracy: 0.7050 - val_loss: 0.5754 - val_accuracy: 0.7020\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5438 - accuracy: 0.7375 - val_loss: 0.5614 - val_accuracy: 0.7020\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5683 - accuracy: 0.7050 - val_loss: 0.5602 - val_accuracy: 0.7020\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.0998 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0139s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.0677 - accuracy: 0.4123\n",
      "training acc:  0.7049999833106995 , training loss:  0.5682918429374695 , val acc:  0.7019927501678467 , val loss:  0.5601881742477417 , test acc:  0.4123322069644928 , test loss:  1.0676907300949097\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt2/100word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know =\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.9158 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0116s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6479 - accuracy: 0.7244 - val_loss: 0.6448 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6600 - accuracy: 0.6725 - val_loss: 0.6386 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6515 - accuracy: 0.6837 - val_loss: 0.6321 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6363 - accuracy: 0.6938 - val_loss: 0.6299 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6380 - accuracy: 0.6894 - val_loss: 0.6253 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6245 - accuracy: 0.7031 - val_loss: 0.6221 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6024 - accuracy: 0.7262 - val_loss: 0.6214 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6310 - accuracy: 0.6856 - val_loss: 0.6177 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6067 - accuracy: 0.7163 - val_loss: 0.6146 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6281 - accuracy: 0.6856 - val_loss: 0.6109 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6128 - accuracy: 0.7025 - val_loss: 0.6089 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5949 - accuracy: 0.7194 - val_loss: 0.6137 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6198 - accuracy: 0.6869 - val_loss: 0.6030 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5950 - accuracy: 0.7138 - val_loss: 0.6015 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5816 - accuracy: 0.7262 - val_loss: 0.5982 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5971 - accuracy: 0.7069 - val_loss: 0.5950 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6130 - accuracy: 0.6787 - val_loss: 0.5924 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5938 - accuracy: 0.7069 - val_loss: 0.5904 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6009 - accuracy: 0.6900 - val_loss: 0.5888 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5866 - accuracy: 0.7088 - val_loss: 0.5864 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5981 - accuracy: 0.6900 - val_loss: 0.5829 - val_accuracy: 0.7020\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5758 - accuracy: 0.7131 - val_loss: 0.5821 - val_accuracy: 0.7020\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6029 - accuracy: 0.6806 - val_loss: 0.5787 - val_accuracy: 0.7020\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5855 - accuracy: 0.6975 - val_loss: 0.5807 - val_accuracy: 0.7020\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5728 - accuracy: 0.7069 - val_loss: 0.5746 - val_accuracy: 0.7020\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5644 - accuracy: 0.7212 - val_loss: 0.5806 - val_accuracy: 0.7020\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5544 - accuracy: 0.7275 - val_loss: 0.5770 - val_accuracy: 0.7020\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5813 - accuracy: 0.6925 - val_loss: 0.5679 - val_accuracy: 0.7020\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5886 - accuracy: 0.6844 - val_loss: 0.5655 - val_accuracy: 0.7020\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5630 - accuracy: 0.7081 - val_loss: 0.5636 - val_accuracy: 0.7029\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.0065 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0162s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9763 - accuracy: 0.4125\n",
      "training acc:  0.7081249952316284 , training loss:  0.5630207657814026 , val acc:  0.7028985619544983 , val loss:  0.5635940432548523 , test acc:  0.4125419557094574 , test loss:  0.9763055443763733\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt2/150word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most me much 1 each such different he something example used now question 've well good its am been however really being after 're case number set take since still many say who find 2 problem new those possible another try both between too things were while probably point\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6921 - accuracy: 0.5962 - val_loss: 0.6226 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6190 - accuracy: 0.7063 - val_loss: 0.6203 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6021 - accuracy: 0.7237 - val_loss: 0.6179 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6155 - accuracy: 0.7038 - val_loss: 0.6155 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6340 - accuracy: 0.6744 - val_loss: 0.6155 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6089 - accuracy: 0.7106 - val_loss: 0.6179 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6018 - accuracy: 0.7144 - val_loss: 0.6075 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5989 - accuracy: 0.7144 - val_loss: 0.6058 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5924 - accuracy: 0.7200 - val_loss: 0.6030 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5788 - accuracy: 0.7331 - val_loss: 0.6078 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5962 - accuracy: 0.7106 - val_loss: 0.5987 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5936 - accuracy: 0.7081 - val_loss: 0.5968 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6004 - accuracy: 0.7000 - val_loss: 0.5928 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5828 - accuracy: 0.7188 - val_loss: 0.5935 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5962 - accuracy: 0.6981 - val_loss: 0.5881 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5964 - accuracy: 0.6981 - val_loss: 0.5850 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5757 - accuracy: 0.7181 - val_loss: 0.5850 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5989 - accuracy: 0.6862 - val_loss: 0.5812 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5653 - accuracy: 0.7256 - val_loss: 0.5829 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5901 - accuracy: 0.6925 - val_loss: 0.5760 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5743 - accuracy: 0.7031 - val_loss: 0.5823 - val_accuracy: 0.7120\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5659 - accuracy: 0.7262 - val_loss: 0.5719 - val_accuracy: 0.7020\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5614 - accuracy: 0.7156 - val_loss: 0.5758 - val_accuracy: 0.7020\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5570 - accuracy: 0.7231 - val_loss: 0.5727 - val_accuracy: 0.7020\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5770 - accuracy: 0.6969 - val_loss: 0.5662 - val_accuracy: 0.7020\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5896 - accuracy: 0.6956 - val_loss: 0.5641 - val_accuracy: 0.7029\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5693 - accuracy: 0.7031 - val_loss: 0.5621 - val_accuracy: 0.7020\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5633 - accuracy: 0.7063 - val_loss: 0.5598 - val_accuracy: 0.7029\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5714 - accuracy: 0.7019 - val_loss: 0.5579 - val_accuracy: 0.7029\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5621 - accuracy: 0.7063 - val_loss: 0.5571 - val_accuracy: 0.7020\n",
      "  1/596 [..............................] - ETA: 8s - loss: 1.1180 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0140s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.0782 - accuracy: 0.4124\n",
      "training acc:  0.706250011920929 , training loss:  0.5621389746665955 , val acc:  0.7019927501678467 , val loss:  0.5571327805519104 , test acc:  0.4124370813369751 , test loss:  1.0782395601272583\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt2/200word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most me much 1 each such different he something example used now question 've well good its am been however really being after 're case number set take since still many say who find 2 problem new those possible another try both between too things were while probably point had our did over why enough go right + answer let without value his through better actually before system data back ca 'll going 'd change down long every sure own given image able look high code 3 \\mathcal power add create thing part off second less page having seems\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 1.6658 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_end` time: 0.0112s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 1.1441 - accuracy: 0.3862 - val_loss: 0.6249 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6135 - accuracy: 0.7038 - val_loss: 0.6123 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6067 - accuracy: 0.7094 - val_loss: 0.6107 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6100 - accuracy: 0.7013 - val_loss: 0.6160 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5912 - accuracy: 0.7237 - val_loss: 0.6068 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6134 - accuracy: 0.6931 - val_loss: 0.6081 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6178 - accuracy: 0.6850 - val_loss: 0.6031 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6133 - accuracy: 0.6913 - val_loss: 0.6002 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5935 - accuracy: 0.7131 - val_loss: 0.5979 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5975 - accuracy: 0.7044 - val_loss: 0.5983 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6036 - accuracy: 0.6969 - val_loss: 0.5935 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5729 - accuracy: 0.7325 - val_loss: 0.5942 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5706 - accuracy: 0.7294 - val_loss: 0.6068 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5860 - accuracy: 0.7131 - val_loss: 0.5895 - val_accuracy: 0.7020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5864 - accuracy: 0.7125 - val_loss: 0.5848 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6026 - accuracy: 0.6825 - val_loss: 0.5822 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5964 - accuracy: 0.6906 - val_loss: 0.5805 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5736 - accuracy: 0.7156 - val_loss: 0.5789 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5862 - accuracy: 0.7025 - val_loss: 0.5773 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5800 - accuracy: 0.7025 - val_loss: 0.5738 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5759 - accuracy: 0.7069 - val_loss: 0.5737 - val_accuracy: 0.7020\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5715 - accuracy: 0.7106 - val_loss: 0.5700 - val_accuracy: 0.7020\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5826 - accuracy: 0.6900 - val_loss: 0.5679 - val_accuracy: 0.7020\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5613 - accuracy: 0.7150 - val_loss: 0.5699 - val_accuracy: 0.7020\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5730 - accuracy: 0.7031 - val_loss: 0.5647 - val_accuracy: 0.7020\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5489 - accuracy: 0.7225 - val_loss: 0.5682 - val_accuracy: 0.7020\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5731 - accuracy: 0.6975 - val_loss: 0.5598 - val_accuracy: 0.7038\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5716 - accuracy: 0.7000 - val_loss: 0.5579 - val_accuracy: 0.7029\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5616 - accuracy: 0.7106 - val_loss: 0.5579 - val_accuracy: 0.7020\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5653 - accuracy: 0.7069 - val_loss: 0.5595 - val_accuracy: 0.7020\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.1962 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0132s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.1371 - accuracy: 0.4123\n",
      "training acc:  0.7068750262260437 , training loss:  0.5653388500213623 , val acc:  0.7019927501678467 , val loss:  0.559529721736908 , test acc:  0.4123322069644928 , test loss:  1.1371395587921143\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt2/250word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most me much 1 each such different he something example used now question 've well good its am been however really being after 're case number set take since still many say who find 2 problem new those possible another try both between too things were while probably point had our did over why enough go right + answer let without value his through better actually before system data back ca 'll going 'd change down long every sure own given image able look high code 3 \\mathcal power add create thing part off second less page having seems function though likely around help lot give always best form either url$ etc means must light bit x above few she made small trying keep end least us world following % option size anything doing note maybe done start day based space 0 found file order real reason said already\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6907 - accuracy: 0.7163 - val_loss: 0.6406 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6480 - accuracy: 0.6869 - val_loss: 0.6421 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6353 - accuracy: 0.7025 - val_loss: 0.6328 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6055 - accuracy: 0.7337 - val_loss: 0.6290 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6249 - accuracy: 0.7056 - val_loss: 0.6249 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6222 - accuracy: 0.7050 - val_loss: 0.6222 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6245 - accuracy: 0.7013 - val_loss: 0.6182 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6036 - accuracy: 0.7212 - val_loss: 0.6172 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6092 - accuracy: 0.7100 - val_loss: 0.6199 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6096 - accuracy: 0.7038 - val_loss: 0.6066 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6211 - accuracy: 0.6850 - val_loss: 0.6046 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5972 - accuracy: 0.7163 - val_loss: 0.6009 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5993 - accuracy: 0.7063 - val_loss: 0.5959 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5936 - accuracy: 0.7088 - val_loss: 0.5936 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5912 - accuracy: 0.7106 - val_loss: 0.5910 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5848 - accuracy: 0.7100 - val_loss: 0.5851 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5984 - accuracy: 0.6938 - val_loss: 0.5874 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5954 - accuracy: 0.6956 - val_loss: 0.5816 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5956 - accuracy: 0.6988 - val_loss: 0.5781 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5628 - accuracy: 0.7175 - val_loss: 0.5896 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5742 - accuracy: 0.7056 - val_loss: 0.5831 - val_accuracy: 0.7020\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5523 - accuracy: 0.7269 - val_loss: 0.5699 - val_accuracy: 0.7020\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5565 - accuracy: 0.7231 - val_loss: 0.5655 - val_accuracy: 0.7020\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5909 - accuracy: 0.6894 - val_loss: 0.5637 - val_accuracy: 0.7210\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5607 - accuracy: 0.7144 - val_loss: 0.5597 - val_accuracy: 0.7029\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5603 - accuracy: 0.7081 - val_loss: 0.5571 - val_accuracy: 0.7129\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5548 - accuracy: 0.7306 - val_loss: 0.5548 - val_accuracy: 0.7129\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5668 - accuracy: 0.7056 - val_loss: 0.5571 - val_accuracy: 0.7627\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5353 - accuracy: 0.7394 - val_loss: 0.5520 - val_accuracy: 0.7038\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5713 - accuracy: 0.7100 - val_loss: 0.5543 - val_accuracy: 0.7690\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 1.0229 - accuracy: 0.4289\n",
      "training acc:  0.7099999785423279 , training loss:  0.5713000893592834 , val acc:  0.76902174949646 , val loss:  0.5542809963226318 , test acc:  0.4289010167121887 , test loss:  1.0228689908981323\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt2/300word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most me much 1 each such different he something example used now question 've well good its am been however really being after 're case number set take since still many say who find 2 problem new those possible another try both between too things were while probably point had our did over why enough go right + answer let without value his through better actually before system data back ca 'll going 'd change down long every sure own given image able look high code 3 \\mathcal power add create thing part off second less page having seems function though likely around help lot give always best form either url$ etc means must light bit x above few she made small trying keep end least us world following % option size anything doing note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution again specific field once check similar > key simple information times far often come never single type c word her line looking text side quite list him important usually instead place works\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6202 - accuracy: 0.7200 - val_loss: 0.6288 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6274 - accuracy: 0.7031 - val_loss: 0.6247 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6054 - accuracy: 0.7269 - val_loss: 0.6216 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6107 - accuracy: 0.7156 - val_loss: 0.6287 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6142 - accuracy: 0.7094 - val_loss: 0.6139 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6195 - accuracy: 0.6981 - val_loss: 0.6118 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6135 - accuracy: 0.6981 - val_loss: 0.6101 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6098 - accuracy: 0.7038 - val_loss: 0.6050 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5982 - accuracy: 0.7125 - val_loss: 0.6012 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5983 - accuracy: 0.7113 - val_loss: 0.6039 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5930 - accuracy: 0.7100 - val_loss: 0.5962 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5941 - accuracy: 0.7025 - val_loss: 0.5933 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5646 - accuracy: 0.7362 - val_loss: 0.5904 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5964 - accuracy: 0.6913 - val_loss: 0.5881 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5961 - accuracy: 0.6925 - val_loss: 0.5833 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5871 - accuracy: 0.7025 - val_loss: 0.5786 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5663 - accuracy: 0.7237 - val_loss: 0.5763 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5613 - accuracy: 0.7225 - val_loss: 0.5733 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5710 - accuracy: 0.7075 - val_loss: 0.5710 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.5662 - accuracy: 0.7131 - val_loss: 0.5755 - val_accuracy: 0.7391\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5562 - accuracy: 0.7312 - val_loss: 0.5749 - val_accuracy: 0.7020\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5626 - accuracy: 0.7250 - val_loss: 0.5672 - val_accuracy: 0.7346\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5806 - accuracy: 0.6906 - val_loss: 0.5635 - val_accuracy: 0.7020\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5606 - accuracy: 0.7119 - val_loss: 0.5609 - val_accuracy: 0.7337\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5580 - accuracy: 0.7194 - val_loss: 0.5586 - val_accuracy: 0.7020\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5531 - accuracy: 0.7200 - val_loss: 0.5549 - val_accuracy: 0.7219\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5529 - accuracy: 0.7131 - val_loss: 0.5551 - val_accuracy: 0.7491\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5566 - accuracy: 0.7287 - val_loss: 0.5538 - val_accuracy: 0.7029\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5439 - accuracy: 0.7294 - val_loss: 0.5500 - val_accuracy: 0.7418\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5485 - accuracy: 0.7306 - val_loss: 0.5492 - val_accuracy: 0.7536\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.0725 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0166s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.0778 - accuracy: 0.4286\n",
      "training acc:  0.7306249737739563 , training loss:  0.54852294921875 , val acc:  0.7536231875419617 , val loss:  0.5491548776626587 , test acc:  0.4285864233970642 , test loss:  1.0777513980865479\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt2/350word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most me much 1 each such different he something example used now question 've well good its am been however really being after 're case number set take since still many say who find 2 problem new those possible another try both between too things were while probably point had our did over why enough go right + answer let without value his through better actually before system data back ca 'll going 'd change down long every sure own given image able look high code 3 \\mathcal power add create thing part off second less page having seems function though likely around help lot give always best form either url$ etc means must light bit x above few she made small trying keep end least us world following % option size anything doing note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution again specific field once check similar > key simple information times far often come never single type c word her line looking text side quite list him important usually instead place works result b life run course put idea fact output open makes correct until hard understand show server group human last working assume water amount whether past pretty itself matter under site paper state god color user ai control range kind next < method due error name great sense version low\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6865 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0106s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6377 - accuracy: 0.6963 - val_loss: 0.6281 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6286 - accuracy: 0.6994 - val_loss: 0.6277 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6159 - accuracy: 0.7106 - val_loss: 0.6273 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6235 - accuracy: 0.7000 - val_loss: 0.6141 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6078 - accuracy: 0.7119 - val_loss: 0.6099 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6267 - accuracy: 0.6787 - val_loss: 0.6053 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.6103 - accuracy: 0.7006 - val_loss: 0.6017 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6089 - accuracy: 0.6956 - val_loss: 0.5968 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5837 - accuracy: 0.7237 - val_loss: 0.5934 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6079 - accuracy: 0.6925 - val_loss: 0.6040 - val_accuracy: 0.7029\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6085 - accuracy: 0.6931 - val_loss: 0.5863 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5824 - accuracy: 0.7150 - val_loss: 0.5872 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5836 - accuracy: 0.7031 - val_loss: 0.5801 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5768 - accuracy: 0.7088 - val_loss: 0.5797 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5996 - accuracy: 0.6913 - val_loss: 0.5753 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5589 - accuracy: 0.7294 - val_loss: 0.5858 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5845 - accuracy: 0.7025 - val_loss: 0.5716 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5727 - accuracy: 0.7050 - val_loss: 0.5727 - val_accuracy: 0.7373\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5825 - accuracy: 0.7019 - val_loss: 0.5639 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5690 - accuracy: 0.7188 - val_loss: 0.5648 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5681 - accuracy: 0.7075 - val_loss: 0.5650 - val_accuracy: 0.7509\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5703 - accuracy: 0.7088 - val_loss: 0.5592 - val_accuracy: 0.7020\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5654 - accuracy: 0.7237 - val_loss: 0.5557 - val_accuracy: 0.7029\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5725 - accuracy: 0.7019 - val_loss: 0.5537 - val_accuracy: 0.7120\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5683 - accuracy: 0.7094 - val_loss: 0.5532 - val_accuracy: 0.7029\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5316 - accuracy: 0.7406 - val_loss: 0.5528 - val_accuracy: 0.7029\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5744 - accuracy: 0.7150 - val_loss: 0.5514 - val_accuracy: 0.7038\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5648 - accuracy: 0.7175 - val_loss: 0.5478 - val_accuracy: 0.7455\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5250 - accuracy: 0.7494 - val_loss: 0.5479 - val_accuracy: 0.7147\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5384 - accuracy: 0.7312 - val_loss: 0.5458 - val_accuracy: 0.7219\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.2017 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0123s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.1687 - accuracy: 0.4187\n",
      "training acc:  0.731249988079071 , training loss:  0.5384341478347778 , val acc:  0.7219203114509583 , val loss:  0.5457762479782104 , test acc:  0.4187290370464325 , test loss:  1.1686768531799316\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt2/400word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most me much 1 each such different he something example used now question 've well good its am been however really being after 're case number set take since still many say who find 2 problem new those possible another try both between too things were while probably point had our did over why enough go right + answer let without value his through better actually before system data back ca 'll going 'd change down long every sure own given image able look high code 3 \\mathcal power add create thing part off second less page having seems function though likely around help lot give always best form either url$ etc means must light bit x above few she made small trying keep end least us world following % option size anything doing note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution again specific field once check similar > key simple information times far often come never single type c word her line looking text side quite list him important usually instead place works result b life run course put idea fact output open makes correct until hard understand show server group human last working assume water amount whether past pretty itself matter under site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character getting    else left below free got seem object 10 difference short full i.e. values camera online wo away available term common words provide old area needs current making lower everything lens experience although almost feel easy tried comes further book\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 1s - loss: 0.8563 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_train_batch_end` time: 0.0164s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6589 - accuracy: 0.6381 - val_loss: 0.6245 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6379 - accuracy: 0.6837 - val_loss: 0.6250 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.6357 - accuracy: 0.6775 - val_loss: 0.6201 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6179 - accuracy: 0.7025 - val_loss: 0.6257 - val_accuracy: 0.7020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6039 - accuracy: 0.7156 - val_loss: 0.6142 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6085 - accuracy: 0.7038 - val_loss: 0.6094 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6149 - accuracy: 0.6888 - val_loss: 0.6009 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6061 - accuracy: 0.6988 - val_loss: 0.5977 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5996 - accuracy: 0.7013 - val_loss: 0.5966 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6074 - accuracy: 0.6894 - val_loss: 0.5917 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5993 - accuracy: 0.6981 - val_loss: 0.5888 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5894 - accuracy: 0.7050 - val_loss: 0.5844 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6043 - accuracy: 0.6812 - val_loss: 0.5841 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5808 - accuracy: 0.7138 - val_loss: 0.5837 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5648 - accuracy: 0.7275 - val_loss: 0.5777 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.5908 - accuracy: 0.6894 - val_loss: 0.5748 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5617 - accuracy: 0.7219 - val_loss: 0.5753 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5678 - accuracy: 0.7113 - val_loss: 0.5778 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5718 - accuracy: 0.7044 - val_loss: 0.5655 - val_accuracy: 0.7029\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5542 - accuracy: 0.7181 - val_loss: 0.5663 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5848 - accuracy: 0.7044 - val_loss: 0.5651 - val_accuracy: 0.7509\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5747 - accuracy: 0.7169 - val_loss: 0.5593 - val_accuracy: 0.7264\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5674 - accuracy: 0.7106 - val_loss: 0.5568 - val_accuracy: 0.7029\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5495 - accuracy: 0.7337 - val_loss: 0.5565 - val_accuracy: 0.7029\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5706 - accuracy: 0.7156 - val_loss: 0.5540 - val_accuracy: 0.7029\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5546 - accuracy: 0.7356 - val_loss: 0.5587 - val_accuracy: 0.7020\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5455 - accuracy: 0.7287 - val_loss: 0.5493 - val_accuracy: 0.7264\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5492 - accuracy: 0.7287 - val_loss: 0.5477 - val_accuracy: 0.7310\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5462 - accuracy: 0.7350 - val_loss: 0.5470 - val_accuracy: 0.7228\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5660 - accuracy: 0.7188 - val_loss: 0.5482 - val_accuracy: 0.7129\n",
      "  1/596 [..............................] - ETA: 0s - loss: 1.2095 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0200s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.1873 - accuracy: 0.4152\n",
      "training acc:  0.71875 , training loss:  0.5659757852554321 , val acc:  0.7128623127937317 , val loss:  0.548209011554718 , test acc:  0.4151635766029358 , test loss:  1.1872509717941284\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt2/450word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most me much 1 each such different he something example used now question 've well good its am been however really being after 're case number set take since still many say who find 2 problem new those possible another try both between too things were while probably point had our did over why enough go right + answer let without value his through better actually before system data back ca 'll going 'd change down long every sure own given image able look high code 3 \\mathcal power add create thing part off second less page having seems function though likely around help lot give always best form either url$ etc means must light bit x above few she made small trying keep end least us world following % option size anything doing note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution again specific field once check similar > key simple information times far often come never single type c word her line looking text side quite list him important usually instead place works result b life run course put idea fact output open makes correct until hard understand show server group human last working assume water amount whether past pretty itself matter under site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character getting    else left below free got seem object 10 difference short full i.e. values camera online wo away available term common words provide old area needs current making lower everything lens experience although almost feel easy tried comes further book thought future useful call higher source smaller price test changes thus top results within    true input write ask created avoid unless perhaps bytes hand several \\\\ close big tell class design layer mm numbers effect air clear position    others running block access 4 days wrong e.g. against exactly\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.8058 - accuracy: 0.5331 - val_loss: 0.6217 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6007 - accuracy: 0.7269 - val_loss: 0.6226 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6285 - accuracy: 0.6881 - val_loss: 0.6172 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6115 - accuracy: 0.7100 - val_loss: 0.6173 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6110 - accuracy: 0.7069 - val_loss: 0.6102 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6226 - accuracy: 0.6875 - val_loss: 0.6088 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6014 - accuracy: 0.7125 - val_loss: 0.6054 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6010 - accuracy: 0.7031 - val_loss: 0.6145 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6081 - accuracy: 0.7013 - val_loss: 0.5980 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6056 - accuracy: 0.6919 - val_loss: 0.5939 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5959 - accuracy: 0.6988 - val_loss: 0.5907 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5881 - accuracy: 0.7038 - val_loss: 0.6148 - val_accuracy: 0.7745\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5992 - accuracy: 0.6994 - val_loss: 0.5852 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5842 - accuracy: 0.7100 - val_loss: 0.5806 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5920 - accuracy: 0.6975 - val_loss: 0.5787 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5727 - accuracy: 0.7119 - val_loss: 0.5762 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5630 - accuracy: 0.7344 - val_loss: 0.5726 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5427 - accuracy: 0.7406 - val_loss: 0.5766 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5571 - accuracy: 0.7200 - val_loss: 0.5668 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5707 - accuracy: 0.7031 - val_loss: 0.5642 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5619 - accuracy: 0.7100 - val_loss: 0.5618 - val_accuracy: 0.7020\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5579 - accuracy: 0.7125 - val_loss: 0.5591 - val_accuracy: 0.7029\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5706 - accuracy: 0.7119 - val_loss: 0.5570 - val_accuracy: 0.7065\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5894 - accuracy: 0.6913 - val_loss: 0.5584 - val_accuracy: 0.7500\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5655 - accuracy: 0.7175 - val_loss: 0.5647 - val_accuracy: 0.7745\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5594 - accuracy: 0.7206 - val_loss: 0.5517 - val_accuracy: 0.7364\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5523 - accuracy: 0.7356 - val_loss: 0.5551 - val_accuracy: 0.7029\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5500 - accuracy: 0.7181 - val_loss: 0.5488 - val_accuracy: 0.7518\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5373 - accuracy: 0.7375 - val_loss: 0.5488 - val_accuracy: 0.7138\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5430 - accuracy: 0.7387 - val_loss: 0.5462 - val_accuracy: 0.7255\n",
      "  1/596 [..............................] - ETA: 3s - loss: 1.1858 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0141s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.1585 - accuracy: 0.4195\n",
      "training acc:  0.7387499809265137 , training loss:  0.5429794192314148 , val acc:  0.7255434989929199 , val loss:  0.5461909174919128 , test acc:  0.4194630980491638 , test loss:  1.1584974527359009\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt2/500word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in i that you it $ for be this   with as are on have if not or can but 's would your n't an do so from will there they at by one my which we what more all some when was then like / has use about any also just could get only time   no how other than them out up need does their should way make where same work - even because very using want 'm these think first into people here two may see know = might most me much 1 each such different he something example used now question 've well good its am been however really being after 're case number set take since still many say who find 2 problem new those possible another try both between too things were while probably point had our did over why enough go right + answer let without value his through better actually before system data back ca 'll going 'd change down long every sure own given image able look high code 3 \\mathcal power add create thing part off second less page having seems function though likely around help lot give always best form either url$ etc means must light bit x above few she made small trying keep end least us world following % option size anything doing note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution again specific field once check similar > key simple information times far often come never single type c word her line looking text side quite list him important usually instead place works result b life run course put idea fact output open makes correct until hard understand show server group human last working assume water amount whether past pretty itself matter under site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character getting    else left below free got seem object 10 difference short full i.e. values camera online wo away available term common words provide old area needs current making lower everything lens experience although almost feel easy tried comes further book thought future useful call higher source smaller price test changes thus top results within    true input write ask created avoid unless perhaps bytes hand several \\\\ close big tell class design layer mm numbers effect air clear position    others running block access 4 days wrong e.g. against exactly anyone multiple three takes actual post fine product believe difficult map game target language search s standard story ' nothing terms n model sort yes # length larger m cause generally research sound along cases google certain company known money turn earth gives mind uses whole return ones taking history\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6983 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6418 - accuracy: 0.6819 - val_loss: 0.6276 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6212 - accuracy: 0.7069 - val_loss: 0.6234 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6051 - accuracy: 0.7269 - val_loss: 0.6200 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6130 - accuracy: 0.7094 - val_loss: 0.6162 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6063 - accuracy: 0.7144 - val_loss: 0.6136 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6187 - accuracy: 0.6969 - val_loss: 0.6096 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6185 - accuracy: 0.6906 - val_loss: 0.6062 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.5946 - accuracy: 0.7175 - val_loss: 0.6028 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5978 - accuracy: 0.7100 - val_loss: 0.5996 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5941 - accuracy: 0.7131 - val_loss: 0.5974 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5877 - accuracy: 0.7169 - val_loss: 0.5965 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5951 - accuracy: 0.7019 - val_loss: 0.5913 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5822 - accuracy: 0.7138 - val_loss: 0.5919 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5796 - accuracy: 0.7144 - val_loss: 0.5854 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5804 - accuracy: 0.7088 - val_loss: 0.5843 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5699 - accuracy: 0.7237 - val_loss: 0.5799 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5867 - accuracy: 0.6994 - val_loss: 0.5780 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5889 - accuracy: 0.6888 - val_loss: 0.5757 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5891 - accuracy: 0.6825 - val_loss: 0.5732 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5686 - accuracy: 0.7081 - val_loss: 0.5701 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5795 - accuracy: 0.7050 - val_loss: 0.5679 - val_accuracy: 0.7020\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5665 - accuracy: 0.7088 - val_loss: 0.5661 - val_accuracy: 0.7020\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5676 - accuracy: 0.7056 - val_loss: 0.5639 - val_accuracy: 0.7038\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5669 - accuracy: 0.7031 - val_loss: 0.5623 - val_accuracy: 0.7020\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5536 - accuracy: 0.7163 - val_loss: 0.5599 - val_accuracy: 0.7020\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5771 - accuracy: 0.7056 - val_loss: 0.5615 - val_accuracy: 0.7482\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.5560 - accuracy: 0.7131 - val_loss: 0.5557 - val_accuracy: 0.7129\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5591 - accuracy: 0.7131 - val_loss: 0.5541 - val_accuracy: 0.7101\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5472 - accuracy: 0.7231 - val_loss: 0.5523 - val_accuracy: 0.7228\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5545 - accuracy: 0.7250 - val_loss: 0.5534 - val_accuracy: 0.7029\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.1704 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0161s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 1.1191 - accuracy: 0.4125\n",
      "training acc:  0.7250000238418579 , training loss:  0.5544506311416626 , val acc:  0.7028985619544983 , val loss:  0.5534490346908569 , test acc:  0.4125419557094574 , test loss:  1.1190673112869263\n",
      "\n",
      "opt3\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt3/50word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6502 - accuracy: 0.6787 - val_loss: 0.6297 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6187 - accuracy: 0.6988 - val_loss: 0.6035 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6014 - accuracy: 0.6963 - val_loss: 0.5875 - val_accuracy: 0.7065\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5975 - accuracy: 0.6856 - val_loss: 0.5770 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5859 - accuracy: 0.6762 - val_loss: 0.5711 - val_accuracy: 0.7065\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5873 - accuracy: 0.6800 - val_loss: 0.5675 - val_accuracy: 0.7065\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5768 - accuracy: 0.6913 - val_loss: 0.5666 - val_accuracy: 0.7065\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5797 - accuracy: 0.6963 - val_loss: 0.5640 - val_accuracy: 0.7210\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5677 - accuracy: 0.7050 - val_loss: 0.5626 - val_accuracy: 0.7283\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5678 - accuracy: 0.6963 - val_loss: 0.5615 - val_accuracy: 0.7301\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5716 - accuracy: 0.7056 - val_loss: 0.5620 - val_accuracy: 0.7319\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5476 - accuracy: 0.7281 - val_loss: 0.5602 - val_accuracy: 0.7292\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5555 - accuracy: 0.7231 - val_loss: 0.5602 - val_accuracy: 0.7292\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5664 - accuracy: 0.7075 - val_loss: 0.5645 - val_accuracy: 0.7065\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5529 - accuracy: 0.7075 - val_loss: 0.5599 - val_accuracy: 0.7283\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5513 - accuracy: 0.7175 - val_loss: 0.5618 - val_accuracy: 0.7228\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5632 - accuracy: 0.7044 - val_loss: 0.5596 - val_accuracy: 0.7292\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5657 - accuracy: 0.7075 - val_loss: 0.5595 - val_accuracy: 0.7301\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5659 - accuracy: 0.7050 - val_loss: 0.5657 - val_accuracy: 0.7056\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5697 - accuracy: 0.7106 - val_loss: 0.5596 - val_accuracy: 0.7301\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5372 - accuracy: 0.7281 - val_loss: 0.5631 - val_accuracy: 0.7138\n",
      "  1/596 [..............................] - ETA: 4s - loss: 1.2366 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0146s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9675 - accuracy: 0.4280\n",
      "training acc:  0.7281249761581421 , training loss:  0.5371802449226379 , val acc:  0.7137681245803833 , val loss:  0.5630583167076111 , test acc:  0.4279572069644928 , test loss:  0.9674938917160034\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt3/100word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7239 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0196s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6587 - accuracy: 0.7056 - val_loss: 0.6427 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6279 - accuracy: 0.7038 - val_loss: 0.6138 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5911 - accuracy: 0.7237 - val_loss: 0.5939 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5833 - accuracy: 0.7081 - val_loss: 0.5780 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5770 - accuracy: 0.6969 - val_loss: 0.5698 - val_accuracy: 0.7047\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5898 - accuracy: 0.6894 - val_loss: 0.5682 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5650 - accuracy: 0.6994 - val_loss: 0.5610 - val_accuracy: 0.7237\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5574 - accuracy: 0.7188 - val_loss: 0.5605 - val_accuracy: 0.7183\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5642 - accuracy: 0.7119 - val_loss: 0.5567 - val_accuracy: 0.7274\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5880 - accuracy: 0.6844 - val_loss: 0.5567 - val_accuracy: 0.7246\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5722 - accuracy: 0.7000 - val_loss: 0.5562 - val_accuracy: 0.7255\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5473 - accuracy: 0.7200 - val_loss: 0.5566 - val_accuracy: 0.7210\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5676 - accuracy: 0.7056 - val_loss: 0.5551 - val_accuracy: 0.7337\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5635 - accuracy: 0.7031 - val_loss: 0.5553 - val_accuracy: 0.7210\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5772 - accuracy: 0.7038 - val_loss: 0.5571 - val_accuracy: 0.7246\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5616 - accuracy: 0.7019 - val_loss: 0.5546 - val_accuracy: 0.7255\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5525 - accuracy: 0.7181 - val_loss: 0.5566 - val_accuracy: 0.7228\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5561 - accuracy: 0.7125 - val_loss: 0.5543 - val_accuracy: 0.7246\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5740 - accuracy: 0.7013 - val_loss: 0.5539 - val_accuracy: 0.7292\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5636 - accuracy: 0.7275 - val_loss: 0.5566 - val_accuracy: 0.7237\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5793 - accuracy: 0.7000 - val_loss: 0.5543 - val_accuracy: 0.7255\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5734 - accuracy: 0.7044 - val_loss: 0.5556 - val_accuracy: 0.7192\n",
      "  1/596 [..............................] - ETA: 4s - loss: 1.2694 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0141s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9377 - accuracy: 0.4315\n",
      "training acc:  0.7043750286102295 , training loss:  0.5733621716499329 , val acc:  0.7192028760910034 , val loss:  0.5555853247642517 , test acc:  0.4315226376056671 , test loss:  0.9376567602157593\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt3/150word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less page seems function though likely around help lot give always best form either url$ etc means must light bit x made small trying keep end least us world following % option size anything note maybe done start day based space 0 found file order real reason said already years read\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6816 - accuracy: 0.7469 - val_loss: 0.6826 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6539 - accuracy: 0.6944 - val_loss: 0.6336 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6153 - accuracy: 0.7156 - val_loss: 0.6139 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6128 - accuracy: 0.6856 - val_loss: 0.5938 - val_accuracy: 0.7029\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5747 - accuracy: 0.7256 - val_loss: 0.5857 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5999 - accuracy: 0.6769 - val_loss: 0.5770 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5695 - accuracy: 0.7119 - val_loss: 0.5725 - val_accuracy: 0.7011\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5796 - accuracy: 0.6981 - val_loss: 0.5654 - val_accuracy: 0.7192\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5668 - accuracy: 0.7094 - val_loss: 0.5627 - val_accuracy: 0.7246\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5734 - accuracy: 0.6938 - val_loss: 0.5626 - val_accuracy: 0.7210\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5633 - accuracy: 0.6975 - val_loss: 0.5598 - val_accuracy: 0.7174\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5512 - accuracy: 0.7287 - val_loss: 0.5583 - val_accuracy: 0.7219\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5385 - accuracy: 0.7387 - val_loss: 0.5571 - val_accuracy: 0.7255\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5730 - accuracy: 0.7069 - val_loss: 0.5590 - val_accuracy: 0.7174\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5344 - accuracy: 0.7381 - val_loss: 0.5580 - val_accuracy: 0.7174\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5630 - accuracy: 0.7100 - val_loss: 0.5563 - val_accuracy: 0.7409\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5731 - accuracy: 0.7013 - val_loss: 0.5576 - val_accuracy: 0.7183\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5312 - accuracy: 0.7375 - val_loss: 0.5608 - val_accuracy: 0.7156\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5799 - accuracy: 0.6950 - val_loss: 0.5554 - val_accuracy: 0.7264\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5514 - accuracy: 0.7188 - val_loss: 0.5597 - val_accuracy: 0.7192\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5801 - accuracy: 0.6800 - val_loss: 0.5586 - val_accuracy: 0.7219\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5783 - accuracy: 0.6963 - val_loss: 0.5553 - val_accuracy: 0.7310\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5623 - accuracy: 0.7169 - val_loss: 0.5566 - val_accuracy: 0.7192\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5617 - accuracy: 0.7050 - val_loss: 0.5551 - val_accuracy: 0.7201\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5472 - accuracy: 0.7219 - val_loss: 0.5554 - val_accuracy: 0.7156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5745 - accuracy: 0.7075 - val_loss: 0.5553 - val_accuracy: 0.7156\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5690 - accuracy: 0.7081 - val_loss: 0.5553 - val_accuracy: 0.7418\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.2191 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0009s vs `on_test_batch_end` time: 0.0121s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9152 - accuracy: 0.4468\n",
      "training acc:  0.7081249952316284 , training loss:  0.5689631700515747 , val acc:  0.741847813129425 , val loss:  0.5552989840507507 , test acc:  0.4468330442905426 , test loss:  0.9152445793151855\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt3/200word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less page seems function though likely around help lot give always best form either url$ etc means must light bit x made small trying keep end least us world following % option size anything note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution specific field check similar > key simple information times far often come never single type c word line looking text side quite list important usually instead place works result b life run course put\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6536 - accuracy: 0.7100 - val_loss: 0.6404 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6335 - accuracy: 0.6844 - val_loss: 0.6111 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6054 - accuracy: 0.7088 - val_loss: 0.5923 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5759 - accuracy: 0.7250 - val_loss: 0.5802 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5743 - accuracy: 0.7050 - val_loss: 0.5698 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5742 - accuracy: 0.7119 - val_loss: 0.5624 - val_accuracy: 0.7219\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5844 - accuracy: 0.6825 - val_loss: 0.5614 - val_accuracy: 0.7183\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5660 - accuracy: 0.7050 - val_loss: 0.5579 - val_accuracy: 0.7246\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5621 - accuracy: 0.6994 - val_loss: 0.5602 - val_accuracy: 0.7138\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5419 - accuracy: 0.7244 - val_loss: 0.5580 - val_accuracy: 0.7210\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5936 - accuracy: 0.6850 - val_loss: 0.5554 - val_accuracy: 0.7228\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5478 - accuracy: 0.7212 - val_loss: 0.5545 - val_accuracy: 0.7237\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5321 - accuracy: 0.7406 - val_loss: 0.5533 - val_accuracy: 0.7428\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5434 - accuracy: 0.7231 - val_loss: 0.5563 - val_accuracy: 0.7246\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5636 - accuracy: 0.7113 - val_loss: 0.5547 - val_accuracy: 0.7255\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5605 - accuracy: 0.7138 - val_loss: 0.5583 - val_accuracy: 0.7192\n",
      "  1/596 [..............................] - ETA: 3s - loss: 1.3064 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0188s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9800 - accuracy: 0.4227\n",
      "training acc:  0.7137500047683716 , training loss:  0.5604947209358215 , val acc:  0.7192028760910034 , val loss:  0.5582872033119202 , test acc:  0.4227139353752136 , test loss:  0.9800373315811157\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt3/250word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less page seems function though likely around help lot give always best form either url$ etc means must light bit x made small trying keep end least us world following % option size anything note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution specific field check similar > key simple information times far often come never single type c word line looking text side quite list important usually instead place works result b life run course put idea fact output open makes correct hard understand show server group human last working assume water amount whether past pretty matter site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7641 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0158s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6521 - accuracy: 0.7175 - val_loss: 0.6348 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6155 - accuracy: 0.7119 - val_loss: 0.6031 - val_accuracy: 0.7047\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6100 - accuracy: 0.6862 - val_loss: 0.5869 - val_accuracy: 0.7219\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5782 - accuracy: 0.7113 - val_loss: 0.5722 - val_accuracy: 0.7165\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5788 - accuracy: 0.6975 - val_loss: 0.5659 - val_accuracy: 0.7219\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5568 - accuracy: 0.7219 - val_loss: 0.5626 - val_accuracy: 0.7174\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5662 - accuracy: 0.7144 - val_loss: 0.5582 - val_accuracy: 0.7418\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5895 - accuracy: 0.6862 - val_loss: 0.5582 - val_accuracy: 0.7255\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5753 - accuracy: 0.6931 - val_loss: 0.5560 - val_accuracy: 0.7255\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5636 - accuracy: 0.7100 - val_loss: 0.5565 - val_accuracy: 0.7255\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5973 - accuracy: 0.6837 - val_loss: 0.5579 - val_accuracy: 0.7219\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5746 - accuracy: 0.7013 - val_loss: 0.5609 - val_accuracy: 0.7129\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.2222 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_test_batch_end` time: 0.0135s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9238 - accuracy: 0.4186\n",
      "training acc:  0.7012500166893005 , training loss:  0.574626088142395 , val acc:  0.7128623127937317 , val loss:  0.5609441995620728 , test acc:  0.4186241626739502 , test loss:  0.9237880706787109\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt3/300word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less page seems function though likely around help lot give always best form either url$ etc means must light bit x made small trying keep end least us world following % option size anything note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution specific field check similar > key simple information times far often come never single type c word line looking text side quite list important usually instead place works result b life run course put idea fact output open makes correct hard understand show server group human last working assume water amount whether past pretty matter site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character getting    else left free got seem object 10 difference short full i.e. values camera online wo away available term common words provide old area needs current making lower everything lens experience although almost feel easy tried comes book thought future useful call higher source smaller price test changes thus\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6360 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0107s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6431 - accuracy: 0.7325 - val_loss: 0.6522 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6282 - accuracy: 0.6944 - val_loss: 0.6073 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5962 - accuracy: 0.7150 - val_loss: 0.5892 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6055 - accuracy: 0.6812 - val_loss: 0.5819 - val_accuracy: 0.7029\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5915 - accuracy: 0.6881 - val_loss: 0.5755 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5711 - accuracy: 0.7044 - val_loss: 0.5690 - val_accuracy: 0.7056\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5532 - accuracy: 0.7244 - val_loss: 0.5616 - val_accuracy: 0.7237\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5495 - accuracy: 0.7231 - val_loss: 0.5604 - val_accuracy: 0.7201\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5682 - accuracy: 0.7025 - val_loss: 0.5585 - val_accuracy: 0.7237\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5594 - accuracy: 0.7188 - val_loss: 0.5572 - val_accuracy: 0.7382\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5798 - accuracy: 0.7000 - val_loss: 0.5606 - val_accuracy: 0.7156\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5653 - accuracy: 0.6956 - val_loss: 0.5586 - val_accuracy: 0.7201\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5538 - accuracy: 0.7237 - val_loss: 0.5561 - val_accuracy: 0.7246\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5489 - accuracy: 0.7250 - val_loss: 0.5558 - val_accuracy: 0.7237\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5705 - accuracy: 0.7000 - val_loss: 0.5547 - val_accuracy: 0.7328\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5788 - accuracy: 0.7031 - val_loss: 0.5614 - val_accuracy: 0.7129\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5717 - accuracy: 0.6988 - val_loss: 0.5590 - val_accuracy: 0.7174\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5877 - accuracy: 0.6906 - val_loss: 0.5550 - val_accuracy: 0.7319\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.1873 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0133s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9009 - accuracy: 0.4409\n",
      "training acc:  0.690625011920929 , training loss:  0.5877284407615662 , val acc:  0.7318840622901917 , val loss:  0.5549695491790771 , test acc:  0.4408557116985321 , test loss:  0.900861382484436\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt3/350word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less page seems function though likely around help lot give always best form either url$ etc means must light bit x made small trying keep end least us world following % option size anything note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution specific field check similar > key simple information times far often come never single type c word line looking text side quite list important usually instead place works result b life run course put idea fact output open makes correct hard understand show server group human last working assume water amount whether past pretty matter site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character getting    else left free got seem object 10 difference short full i.e. values camera online wo away available term common words provide old area needs current making lower everything lens experience although almost feel easy tried comes book thought future useful call higher source smaller price test changes thus top results within    true input write ask created avoid unless perhaps bytes hand several \\\\ close big tell class design layer mm numbers effect air clear position    others running block access 4 days wrong e.g. exactly anyone multiple three takes actual post fine product believe difficult map game\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6611 - accuracy: 0.7056 - val_loss: 0.6359 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6160 - accuracy: 0.7106 - val_loss: 0.6052 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6163 - accuracy: 0.6781 - val_loss: 0.5888 - val_accuracy: 0.7011\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5772 - accuracy: 0.7144 - val_loss: 0.5763 - val_accuracy: 0.7056\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5937 - accuracy: 0.6950 - val_loss: 0.5765 - val_accuracy: 0.7536\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5791 - accuracy: 0.7069 - val_loss: 0.5641 - val_accuracy: 0.7337\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5697 - accuracy: 0.7119 - val_loss: 0.5618 - val_accuracy: 0.7219\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6133 - accuracy: 0.6631 - val_loss: 0.5617 - val_accuracy: 0.7219\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5496 - accuracy: 0.7244 - val_loss: 0.5623 - val_accuracy: 0.7156\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5906 - accuracy: 0.6819 - val_loss: 0.5589 - val_accuracy: 0.7219\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5607 - accuracy: 0.7063 - val_loss: 0.5576 - val_accuracy: 0.7228\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5922 - accuracy: 0.6837 - val_loss: 0.5583 - val_accuracy: 0.7418\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5809 - accuracy: 0.7050 - val_loss: 0.5584 - val_accuracy: 0.7201\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5699 - accuracy: 0.7031 - val_loss: 0.5565 - val_accuracy: 0.7219\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5547 - accuracy: 0.7113 - val_loss: 0.5620 - val_accuracy: 0.7138\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5619 - accuracy: 0.7144 - val_loss: 0.5665 - val_accuracy: 0.7038\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5622 - accuracy: 0.7075 - val_loss: 0.5571 - val_accuracy: 0.7228\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.2390 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0145s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.9318 - accuracy: 0.4271\n",
      "training acc:  0.7074999809265137 , training loss:  0.5621675252914429 , val acc:  0.7228260636329651 , val loss:  0.5570536255836487 , test acc:  0.42711830139160156 , test loss:  0.9317625164985657\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt3/400word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less page seems function though likely around help lot give always best form either url$ etc means must light bit x made small trying keep end least us world following % option size anything note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution specific field check similar > key simple information times far often come never single type c word line looking text side quite list important usually instead place works result b life run course put idea fact output open makes correct hard understand show server group human last working assume water amount whether past pretty matter site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character getting    else left free got seem object 10 difference short full i.e. values camera online wo away available term common words provide old area needs current making lower everything lens experience although almost feel easy tried comes book thought future useful call higher source smaller price test changes thus top results within    true input write ask created avoid unless perhaps bytes hand several \\\\ close big tell class design layer mm numbers effect air clear position    others running block access 4 days wrong e.g. exactly anyone multiple three takes actual post fine product believe difficult map game target language search standard story ' nothing terms n model sort yes # length larger cause generally research sound along cases google certain company known money turn earth gives mind uses whole return ones taking history easily theory   computer allow normal writing thanks problems become address n$ support account\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.5212 - accuracy: 0.8750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0262s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6724 - accuracy: 0.7188 - val_loss: 0.6505 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6304 - accuracy: 0.7044 - val_loss: 0.6159 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6177 - accuracy: 0.6800 - val_loss: 0.5991 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5743 - accuracy: 0.7225 - val_loss: 0.5772 - val_accuracy: 0.7002\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5758 - accuracy: 0.7000 - val_loss: 0.5687 - val_accuracy: 0.7111\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5640 - accuracy: 0.7094 - val_loss: 0.5670 - val_accuracy: 0.7011\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5604 - accuracy: 0.7119 - val_loss: 0.5593 - val_accuracy: 0.7237\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5538 - accuracy: 0.7194 - val_loss: 0.5568 - val_accuracy: 0.7355\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5477 - accuracy: 0.7188 - val_loss: 0.5560 - val_accuracy: 0.7246\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5784 - accuracy: 0.6850 - val_loss: 0.5549 - val_accuracy: 0.7246\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5604 - accuracy: 0.7206 - val_loss: 0.5564 - val_accuracy: 0.7228\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5555 - accuracy: 0.7131 - val_loss: 0.5546 - val_accuracy: 0.7246\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5467 - accuracy: 0.7175 - val_loss: 0.5545 - val_accuracy: 0.7228\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5730 - accuracy: 0.6925 - val_loss: 0.5532 - val_accuracy: 0.7310\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5888 - accuracy: 0.6913 - val_loss: 0.5538 - val_accuracy: 0.7346\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5714 - accuracy: 0.7088 - val_loss: 0.5540 - val_accuracy: 0.7301\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5615 - accuracy: 0.7081 - val_loss: 0.5534 - val_accuracy: 0.7319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/596 [..............................] - ETA: 5s - loss: 1.2025 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0147s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.9158 - accuracy: 0.4392\n",
      "training acc:  0.7081249952316284 , training loss:  0.5615354776382446 , val acc:  0.7318840622901917 , val loss:  0.5533792972564697 , test acc:  0.43917784094810486 , test loss:  0.9157645106315613\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt3/450word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less page seems function though likely around help lot give always best form either url$ etc means must light bit x made small trying keep end least us world following % option size anything note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution specific field check similar > key simple information times far often come never single type c word line looking text side quite list important usually instead place works result b life run course put idea fact output open makes correct hard understand show server group human last working assume water amount whether past pretty matter site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character getting    else left free got seem object 10 difference short full i.e. values camera online wo away available term common words provide old area needs current making lower everything lens experience although almost feel easy tried comes book thought future useful call higher source smaller price test changes thus top results within    true input write ask created avoid unless perhaps bytes hand several \\\\ close big tell class design layer mm numbers effect air clear position    others running block access 4 days wrong e.g. exactly anyone multiple three takes actual post fine product believe difficult map game target language search standard story ' nothing terms n model sort yes # length larger cause generally research sound along cases google certain company known money turn earth gives mind uses whole return ones taking history easily theory   computer allow normal writing thanks problems become address n$ support account per step happen inside added seen says e bad sometimes therefore setting structure play 100 content network english 6 children meaning food functions gets move related head fire rate main slightly context reading situation force update mode necessary build view body looks examples flash local options points stop job link\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6982 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0132s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6647 - accuracy: 0.7069 - val_loss: 0.6569 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6311 - accuracy: 0.7169 - val_loss: 0.6292 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6183 - accuracy: 0.7006 - val_loss: 0.6041 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6062 - accuracy: 0.6881 - val_loss: 0.5886 - val_accuracy: 0.7002\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5786 - accuracy: 0.7088 - val_loss: 0.5772 - val_accuracy: 0.6984\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5847 - accuracy: 0.6919 - val_loss: 0.5695 - val_accuracy: 0.7111\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5751 - accuracy: 0.7081 - val_loss: 0.5649 - val_accuracy: 0.7346\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5708 - accuracy: 0.7150 - val_loss: 0.5602 - val_accuracy: 0.7246\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5567 - accuracy: 0.7113 - val_loss: 0.5644 - val_accuracy: 0.7056\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5414 - accuracy: 0.7437 - val_loss: 0.5558 - val_accuracy: 0.7274\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5605 - accuracy: 0.7175 - val_loss: 0.5548 - val_accuracy: 0.7346\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5714 - accuracy: 0.7081 - val_loss: 0.5550 - val_accuracy: 0.7274\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5626 - accuracy: 0.7056 - val_loss: 0.5594 - val_accuracy: 0.7156\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5648 - accuracy: 0.7125 - val_loss: 0.5542 - val_accuracy: 0.7292\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5526 - accuracy: 0.7250 - val_loss: 0.5542 - val_accuracy: 0.7246\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5721 - accuracy: 0.7113 - val_loss: 0.5555 - val_accuracy: 0.7246\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5565 - accuracy: 0.7175 - val_loss: 0.5591 - val_accuracy: 0.7147\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5788 - accuracy: 0.6975 - val_loss: 0.5559 - val_accuracy: 0.7228\n",
      "  1/596 [..............................] - ETA: 4s - loss: 1.2386 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0158s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9407 - accuracy: 0.4260\n",
      "training acc:  0.6974999904632568 , training loss:  0.5788132548332214 , val acc:  0.7228260636329651 , val loss:  0.5558977723121643 , test acc:  0.4259647727012634 , test loss:  0.9407258629798889\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt3/500word_list.txt\n",
      "WORD INDEX STR:  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less page seems function though likely around help lot give always best form either url$ etc means must light bit x made small trying keep end least us world following % option size anything note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution specific field check similar > key simple information times far often come never single type c word line looking text side quite list important usually instead place works result b life run course put idea fact output open makes correct hard understand show server group human last working assume water amount whether past pretty matter site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character getting    else left free got seem object 10 difference short full i.e. values camera online wo away available term common words provide old area needs current making lower everything lens experience although almost feel easy tried comes book thought future useful call higher source smaller price test changes thus top results within    true input write ask created avoid unless perhaps bytes hand several \\\\ close big tell class design layer mm numbers effect air clear position    others running block access 4 days wrong e.g. exactly anyone multiple three takes actual post fine product believe difficult map game target language search standard story ' nothing terms n model sort yes # length larger cause generally research sound along cases google certain company known money turn earth gives mind uses whole return ones taking history easily theory   computer allow normal writing thanks problems become address n$ support account per step happen inside added seen says e bad sometimes therefore setting structure play 100 content network english 6 children meaning food functions gets move related head fire rate main slightly context reading situation force update mode necessary build view body looks examples flash local options points stop job link knowledge choose rules remember yet exact depends approach random entire planet select tool apply characters parts assuming guess rule speed whatever instance half directly \\ noise outside total algorithm mentioned conditions reasons worth quality later save major risk background needed pressure please client personal images humans limit questions perfect _\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6887 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0258s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6442 - accuracy: 0.7131 - val_loss: 0.6251 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6189 - accuracy: 0.6875 - val_loss: 0.5932 - val_accuracy: 0.7029\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5747 - accuracy: 0.7194 - val_loss: 0.5882 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5508 - accuracy: 0.7394 - val_loss: 0.5768 - val_accuracy: 0.7038\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5621 - accuracy: 0.7144 - val_loss: 0.5610 - val_accuracy: 0.7210\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5906 - accuracy: 0.6744 - val_loss: 0.5613 - val_accuracy: 0.7219\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6005 - accuracy: 0.6669 - val_loss: 0.5604 - val_accuracy: 0.7228\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5613 - accuracy: 0.7119 - val_loss: 0.5569 - val_accuracy: 0.7246\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5723 - accuracy: 0.7019 - val_loss: 0.5590 - val_accuracy: 0.7192\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5643 - accuracy: 0.7050 - val_loss: 0.5661 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5738 - accuracy: 0.7038 - val_loss: 0.5607 - val_accuracy: 0.7138\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.2381 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0133s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9399 - accuracy: 0.4189\n",
      "training acc:  0.7037500143051147 , training loss:  0.5737599730491638 , val acc:  0.7137681245803833 , val loss:  0.5606887340545654 , test acc:  0.4189387559890747 , test loss:  0.9399285912513733\n",
      "\n",
      "opt4\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt4/50word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.7116 - accuracy: 0.5650 - val_loss: 0.6287 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6364 - accuracy: 0.6925 - val_loss: 0.6236 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6192 - accuracy: 0.7056 - val_loss: 0.6197 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6225 - accuracy: 0.7025 - val_loss: 0.6171 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6083 - accuracy: 0.7150 - val_loss: 0.6143 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6139 - accuracy: 0.7056 - val_loss: 0.6137 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5972 - accuracy: 0.7200 - val_loss: 0.6153 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6106 - accuracy: 0.7025 - val_loss: 0.6064 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6169 - accuracy: 0.6894 - val_loss: 0.6052 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6007 - accuracy: 0.7075 - val_loss: 0.6024 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5930 - accuracy: 0.7125 - val_loss: 0.5982 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5985 - accuracy: 0.7025 - val_loss: 0.5962 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6074 - accuracy: 0.6862 - val_loss: 0.5933 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5999 - accuracy: 0.7019 - val_loss: 0.5907 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5950 - accuracy: 0.7019 - val_loss: 0.5887 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5918 - accuracy: 0.7025 - val_loss: 0.5869 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6007 - accuracy: 0.6894 - val_loss: 0.5842 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5629 - accuracy: 0.7325 - val_loss: 0.5848 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5772 - accuracy: 0.7138 - val_loss: 0.5803 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5859 - accuracy: 0.6956 - val_loss: 0.5798 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5840 - accuracy: 0.6956 - val_loss: 0.5769 - val_accuracy: 0.7020\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5879 - accuracy: 0.6881 - val_loss: 0.5750 - val_accuracy: 0.7020\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5687 - accuracy: 0.7113 - val_loss: 0.5810 - val_accuracy: 0.7020\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5645 - accuracy: 0.7256 - val_loss: 0.5741 - val_accuracy: 0.7020\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5557 - accuracy: 0.7256 - val_loss: 0.5706 - val_accuracy: 0.7020\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5543 - accuracy: 0.7231 - val_loss: 0.5755 - val_accuracy: 0.7020\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5773 - accuracy: 0.6988 - val_loss: 0.5665 - val_accuracy: 0.7020\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5627 - accuracy: 0.7044 - val_loss: 0.5644 - val_accuracy: 0.7020\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5593 - accuracy: 0.7100 - val_loss: 0.5707 - val_accuracy: 0.7020\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5703 - accuracy: 0.6944 - val_loss: 0.5629 - val_accuracy: 0.7020\n",
      "  1/596 [..............................] - ETA: 4s - loss: 1.0742 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0134s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9730 - accuracy: 0.4123\n",
      "training acc:  0.6943749785423279 , training loss:  0.5702918767929077 , val acc:  0.7019927501678467 , val loss:  0.5628741979598999 , test acc:  0.4123322069644928 , test loss:  0.9730130434036255\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt4/100word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 1.3979 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0058s vs `on_train_batch_end` time: 0.0161s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.7287 - accuracy: 0.5744 - val_loss: 0.6233 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6120 - accuracy: 0.7163 - val_loss: 0.6213 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6134 - accuracy: 0.7069 - val_loss: 0.6239 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6272 - accuracy: 0.6869 - val_loss: 0.6098 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6203 - accuracy: 0.6894 - val_loss: 0.6053 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5980 - accuracy: 0.7106 - val_loss: 0.6008 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6010 - accuracy: 0.7069 - val_loss: 0.5967 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6020 - accuracy: 0.7006 - val_loss: 0.5944 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6028 - accuracy: 0.6969 - val_loss: 0.5911 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6075 - accuracy: 0.6831 - val_loss: 0.5877 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5835 - accuracy: 0.7088 - val_loss: 0.5872 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5986 - accuracy: 0.6881 - val_loss: 0.5807 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5743 - accuracy: 0.7138 - val_loss: 0.5777 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5769 - accuracy: 0.7100 - val_loss: 0.5793 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5659 - accuracy: 0.7244 - val_loss: 0.5745 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5773 - accuracy: 0.7069 - val_loss: 0.5737 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5691 - accuracy: 0.7056 - val_loss: 0.5701 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5934 - accuracy: 0.6894 - val_loss: 0.5706 - val_accuracy: 0.7029\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5557 - accuracy: 0.7362 - val_loss: 0.5688 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5595 - accuracy: 0.7175 - val_loss: 0.5632 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5775 - accuracy: 0.6950 - val_loss: 0.5642 - val_accuracy: 0.7029\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5560 - accuracy: 0.7188 - val_loss: 0.5598 - val_accuracy: 0.7020\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5648 - accuracy: 0.6969 - val_loss: 0.5587 - val_accuracy: 0.7029\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5539 - accuracy: 0.7181 - val_loss: 0.5600 - val_accuracy: 0.7020\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5701 - accuracy: 0.7013 - val_loss: 0.5687 - val_accuracy: 0.7020\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5405 - accuracy: 0.7294 - val_loss: 0.5718 - val_accuracy: 0.7020\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.4517 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0132s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.1880 - accuracy: 0.4123\n",
      "training acc:  0.7293750047683716 , training loss:  0.5405104160308838 , val acc:  0.7019927501678467 , val loss:  0.5718398094177246 , test acc:  0.4123322069644928 , test loss:  1.1879982948303223\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt4/150word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less page seems function though likely around help lot give always best form either url$ etc means must light bit x made small trying keep end least us world following % option size anything note maybe done start day based space 0 found file order real reason said already\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 1.5730 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_end` time: 0.0104s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.7008 - accuracy: 0.6875 - val_loss: 0.6456 - val_accuracy: 0.7011\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6321 - accuracy: 0.7175 - val_loss: 0.6392 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6373 - accuracy: 0.7063 - val_loss: 0.6362 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6117 - accuracy: 0.7256 - val_loss: 0.6278 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6210 - accuracy: 0.7081 - val_loss: 0.6228 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6159 - accuracy: 0.7106 - val_loss: 0.6189 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6216 - accuracy: 0.7013 - val_loss: 0.6148 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6043 - accuracy: 0.7163 - val_loss: 0.6150 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6090 - accuracy: 0.7044 - val_loss: 0.6053 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6072 - accuracy: 0.6950 - val_loss: 0.6084 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5942 - accuracy: 0.7163 - val_loss: 0.6009 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5755 - accuracy: 0.7331 - val_loss: 0.6131 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5817 - accuracy: 0.7237 - val_loss: 0.5915 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5875 - accuracy: 0.7063 - val_loss: 0.5857 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5805 - accuracy: 0.7094 - val_loss: 0.5880 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5855 - accuracy: 0.7025 - val_loss: 0.5818 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5888 - accuracy: 0.6888 - val_loss: 0.5749 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5696 - accuracy: 0.7181 - val_loss: 0.5738 - val_accuracy: 0.7020\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5924 - accuracy: 0.6737 - val_loss: 0.5718 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5788 - accuracy: 0.6925 - val_loss: 0.5670 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5428 - accuracy: 0.7337 - val_loss: 0.5758 - val_accuracy: 0.7020\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5616 - accuracy: 0.7163 - val_loss: 0.5617 - val_accuracy: 0.7020\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5636 - accuracy: 0.7119 - val_loss: 0.5642 - val_accuracy: 0.7020\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5457 - accuracy: 0.7250 - val_loss: 0.5576 - val_accuracy: 0.7029\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5696 - accuracy: 0.6950 - val_loss: 0.5556 - val_accuracy: 0.7029\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5455 - accuracy: 0.7269 - val_loss: 0.5582 - val_accuracy: 0.7020\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5614 - accuracy: 0.7156 - val_loss: 0.5524 - val_accuracy: 0.7020\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5549 - accuracy: 0.7225 - val_loss: 0.5501 - val_accuracy: 0.7038\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5476 - accuracy: 0.7156 - val_loss: 0.5564 - val_accuracy: 0.7600\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5463 - accuracy: 0.7188 - val_loss: 0.5488 - val_accuracy: 0.7029\n",
      "  1/596 [..............................] - ETA: 4s - loss: 1.3450 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0143s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.0991 - accuracy: 0.4122\n",
      "training acc:  0.71875 , training loss:  0.5462794899940491 , val acc:  0.7028985619544983 , val loss:  0.5487851500511169 , test acc:  0.4122273623943329 , test loss:  1.0991355180740356\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt4/200word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less page seems function though likely around help lot give always best form either url$ etc means must light bit x made small trying keep end least us world following % option size anything note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution specific field check similar > key simple information times far often come never single type c word line looking text side quite list important usually instead place works result b life run\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6522 - accuracy: 0.6862 - val_loss: 0.6319 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6059 - accuracy: 0.7337 - val_loss: 0.6333 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6039 - accuracy: 0.7294 - val_loss: 0.6207 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6158 - accuracy: 0.7056 - val_loss: 0.6162 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5929 - accuracy: 0.7275 - val_loss: 0.6115 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6006 - accuracy: 0.7156 - val_loss: 0.6030 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6003 - accuracy: 0.7075 - val_loss: 0.6006 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6014 - accuracy: 0.6994 - val_loss: 0.6031 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5996 - accuracy: 0.6963 - val_loss: 0.5909 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6005 - accuracy: 0.6831 - val_loss: 0.5924 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5681 - accuracy: 0.7325 - val_loss: 0.5856 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5747 - accuracy: 0.7131 - val_loss: 0.5773 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5686 - accuracy: 0.7206 - val_loss: 0.5737 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5734 - accuracy: 0.7056 - val_loss: 0.5830 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5442 - accuracy: 0.7406 - val_loss: 0.5686 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5695 - accuracy: 0.7100 - val_loss: 0.5680 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5725 - accuracy: 0.6975 - val_loss: 0.5635 - val_accuracy: 0.7029\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5694 - accuracy: 0.7000 - val_loss: 0.5648 - val_accuracy: 0.7391\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5511 - accuracy: 0.7237 - val_loss: 0.5610 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5619 - accuracy: 0.7106 - val_loss: 0.5552 - val_accuracy: 0.7029\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5380 - accuracy: 0.7163 - val_loss: 0.5568 - val_accuracy: 0.7020\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5606 - accuracy: 0.7194 - val_loss: 0.5586 - val_accuracy: 0.7020\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5702 - accuracy: 0.7063 - val_loss: 0.5499 - val_accuracy: 0.7074\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5382 - accuracy: 0.7231 - val_loss: 0.5489 - val_accuracy: 0.7391\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5495 - accuracy: 0.7231 - val_loss: 0.5478 - val_accuracy: 0.7029\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5638 - accuracy: 0.7175 - val_loss: 0.5471 - val_accuracy: 0.7029\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5592 - accuracy: 0.7156 - val_loss: 0.5440 - val_accuracy: 0.7274\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5563 - accuracy: 0.7231 - val_loss: 0.5440 - val_accuracy: 0.7074\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5317 - accuracy: 0.7344 - val_loss: 0.5435 - val_accuracy: 0.7083\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5417 - accuracy: 0.7269 - val_loss: 0.5411 - val_accuracy: 0.7409\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.3865 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0145s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 15s 24ms/step - loss: 1.1091 - accuracy: 0.4250\n",
      "training acc:  0.7268750071525574 , training loss:  0.5416716933250427 , val acc:  0.7409420013427734 , val loss:  0.5410671830177307 , test acc:  0.4250209629535675 , test loss:  1.1091187000274658\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt4/250word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less page seems function though likely around help lot give always best form either url$ etc means must light bit x made small trying keep end least us world following % option size anything note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution specific field check similar > key simple information times far often come never single type c word line looking text side quite list important usually instead place works result b life run course put idea fact output open makes correct hard understand show server group human last working assume water amount whether past pretty matter site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.6450 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0094s vs `on_train_batch_end` time: 0.0291s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6379 - accuracy: 0.7038 - val_loss: 0.6367 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6305 - accuracy: 0.7081 - val_loss: 0.6305 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6265 - accuracy: 0.7050 - val_loss: 0.6258 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6335 - accuracy: 0.6900 - val_loss: 0.6217 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6308 - accuracy: 0.6831 - val_loss: 0.6141 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6142 - accuracy: 0.7025 - val_loss: 0.6095 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6017 - accuracy: 0.7125 - val_loss: 0.6061 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6141 - accuracy: 0.6906 - val_loss: 0.5999 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6036 - accuracy: 0.6950 - val_loss: 0.5952 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6164 - accuracy: 0.6675 - val_loss: 0.5962 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5802 - accuracy: 0.7200 - val_loss: 0.5858 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5790 - accuracy: 0.7125 - val_loss: 0.5814 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5796 - accuracy: 0.7088 - val_loss: 0.5832 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5766 - accuracy: 0.7056 - val_loss: 0.5737 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5940 - accuracy: 0.6806 - val_loss: 0.5700 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5623 - accuracy: 0.7150 - val_loss: 0.5664 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5641 - accuracy: 0.7069 - val_loss: 0.5687 - val_accuracy: 0.7346\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5461 - accuracy: 0.7331 - val_loss: 0.5653 - val_accuracy: 0.7446\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5668 - accuracy: 0.7031 - val_loss: 0.5617 - val_accuracy: 0.7428\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5328 - accuracy: 0.7437 - val_loss: 0.5594 - val_accuracy: 0.7020\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5497 - accuracy: 0.7113 - val_loss: 0.5536 - val_accuracy: 0.7029\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5538 - accuracy: 0.7131 - val_loss: 0.5556 - val_accuracy: 0.7473\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5482 - accuracy: 0.7262 - val_loss: 0.5493 - val_accuracy: 0.7038\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5335 - accuracy: 0.7275 - val_loss: 0.5491 - val_accuracy: 0.7029\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5301 - accuracy: 0.7281 - val_loss: 0.5453 - val_accuracy: 0.7391\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5619 - accuracy: 0.7188 - val_loss: 0.5446 - val_accuracy: 0.7147\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5519 - accuracy: 0.7275 - val_loss: 0.5448 - val_accuracy: 0.7111\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5366 - accuracy: 0.7406 - val_loss: 0.5468 - val_accuracy: 0.7029\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5368 - accuracy: 0.7231 - val_loss: 0.5408 - val_accuracy: 0.7418\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5446 - accuracy: 0.7350 - val_loss: 0.5410 - val_accuracy: 0.7274\n",
      "  1/596 [..............................] - ETA: 3s - loss: 1.4326 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_test_batch_end` time: 0.0210s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.1356 - accuracy: 0.4204\n",
      "training acc:  0.7350000143051147 , training loss:  0.5445845127105713 , val acc:  0.7273550629615784 , val loss:  0.5409572124481201 , test acc:  0.42040687799453735 , test loss:  1.1356223821640015\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt4/300word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less page seems function though likely around help lot give always best form either url$ etc means must light bit x made small trying keep end least us world following % option size anything note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution specific field check similar > key simple information times far often come never single type c word line looking text side quite list important usually instead place works result b life run course put idea fact output open makes correct hard understand show server group human last working assume water amount whether past pretty matter site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character getting    else left free got seem object 10 difference short full i.e. values camera online wo away available term common words provide old area needs current making lower everything lens experience although almost feel easy tried comes book thought future useful call higher source smaller price test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.7158 - accuracy: 0.6562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0332s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6604 - accuracy: 0.6812 - val_loss: 0.6367 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6365 - accuracy: 0.6956 - val_loss: 0.6307 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6280 - accuracy: 0.7050 - val_loss: 0.6248 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6299 - accuracy: 0.6956 - val_loss: 0.6192 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6289 - accuracy: 0.6881 - val_loss: 0.6151 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6252 - accuracy: 0.6862 - val_loss: 0.6079 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6153 - accuracy: 0.6938 - val_loss: 0.6083 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5883 - accuracy: 0.7237 - val_loss: 0.5985 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6042 - accuracy: 0.6963 - val_loss: 0.5939 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6066 - accuracy: 0.6837 - val_loss: 0.5918 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6019 - accuracy: 0.6888 - val_loss: 0.5853 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5961 - accuracy: 0.6888 - val_loss: 0.5798 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5621 - accuracy: 0.7244 - val_loss: 0.5794 - val_accuracy: 0.7029\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5821 - accuracy: 0.7044 - val_loss: 0.5716 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5740 - accuracy: 0.6994 - val_loss: 0.5675 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5815 - accuracy: 0.6894 - val_loss: 0.5674 - val_accuracy: 0.7183\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5616 - accuracy: 0.7113 - val_loss: 0.5658 - val_accuracy: 0.7391\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5658 - accuracy: 0.7175 - val_loss: 0.5575 - val_accuracy: 0.7029\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5657 - accuracy: 0.7025 - val_loss: 0.5549 - val_accuracy: 0.7111\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5659 - accuracy: 0.7019 - val_loss: 0.5579 - val_accuracy: 0.7482\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5505 - accuracy: 0.7312 - val_loss: 0.5514 - val_accuracy: 0.7029\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5386 - accuracy: 0.7312 - val_loss: 0.5479 - val_accuracy: 0.7364\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5349 - accuracy: 0.7369 - val_loss: 0.5471 - val_accuracy: 0.7418\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5489 - accuracy: 0.7306 - val_loss: 0.5487 - val_accuracy: 0.7572\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5578 - accuracy: 0.7200 - val_loss: 0.5430 - val_accuracy: 0.7400\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5330 - accuracy: 0.7487 - val_loss: 0.5529 - val_accuracy: 0.7029\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5465 - accuracy: 0.7250 - val_loss: 0.5408 - val_accuracy: 0.7446\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5524 - accuracy: 0.7325 - val_loss: 0.5400 - val_accuracy: 0.7473\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5203 - accuracy: 0.7606 - val_loss: 0.5492 - val_accuracy: 0.7047\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5279 - accuracy: 0.7469 - val_loss: 0.5379 - val_accuracy: 0.7428\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.4042 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0131s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.1229 - accuracy: 0.4298\n",
      "training acc:  0.746874988079071 , training loss:  0.5278828740119934 , val acc:  0.7427536249160767 , val loss:  0.5378707647323608 , test acc:  0.42984479665756226 , test loss:  1.122888445854187\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt4/350word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less page seems function though likely around help lot give always best form either url$ etc means must light bit x made small trying keep end least us world following % option size anything note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution specific field check similar > key simple information times far often come never single type c word line looking text side quite list important usually instead place works result b life run course put idea fact output open makes correct hard understand show server group human last working assume water amount whether past pretty matter site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character getting    else left free got seem object 10 difference short full i.e. values camera online wo away available term common words provide old area needs current making lower everything lens experience although almost feel easy tried comes book thought future useful call higher source smaller price test changes thus top results within    true input write ask created avoid unless perhaps bytes hand several \\\\ close big tell class design layer mm numbers effect air clear position    others running block access 4 days wrong e.g. exactly anyone multiple three takes actual post fine product believe difficult\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.8515 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0165s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6253 - accuracy: 0.7219 - val_loss: 0.6381 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6347 - accuracy: 0.7013 - val_loss: 0.6304 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6177 - accuracy: 0.7163 - val_loss: 0.6259 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6251 - accuracy: 0.6938 - val_loss: 0.6202 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5903 - accuracy: 0.7350 - val_loss: 0.6148 - val_accuracy: 0.7020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6097 - accuracy: 0.6994 - val_loss: 0.6088 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6063 - accuracy: 0.6950 - val_loss: 0.6014 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5944 - accuracy: 0.7100 - val_loss: 0.5936 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5905 - accuracy: 0.7038 - val_loss: 0.5875 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5752 - accuracy: 0.7231 - val_loss: 0.5835 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5924 - accuracy: 0.6900 - val_loss: 0.5792 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5794 - accuracy: 0.7063 - val_loss: 0.5750 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5909 - accuracy: 0.6819 - val_loss: 0.5735 - val_accuracy: 0.7029\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5701 - accuracy: 0.7100 - val_loss: 0.5690 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5673 - accuracy: 0.7144 - val_loss: 0.5646 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5615 - accuracy: 0.7100 - val_loss: 0.5655 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5429 - accuracy: 0.7244 - val_loss: 0.5605 - val_accuracy: 0.7255\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5523 - accuracy: 0.7094 - val_loss: 0.5565 - val_accuracy: 0.7201\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5673 - accuracy: 0.7050 - val_loss: 0.5574 - val_accuracy: 0.7020\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5746 - accuracy: 0.7113 - val_loss: 0.5541 - val_accuracy: 0.7029\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5294 - accuracy: 0.7381 - val_loss: 0.5554 - val_accuracy: 0.7020\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5522 - accuracy: 0.7188 - val_loss: 0.5488 - val_accuracy: 0.7301\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5595 - accuracy: 0.7300 - val_loss: 0.5493 - val_accuracy: 0.7491\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5545 - accuracy: 0.7194 - val_loss: 0.5514 - val_accuracy: 0.7518\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5665 - accuracy: 0.7250 - val_loss: 0.5449 - val_accuracy: 0.7409\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5295 - accuracy: 0.7469 - val_loss: 0.5476 - val_accuracy: 0.7065\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5567 - accuracy: 0.7269 - val_loss: 0.5455 - val_accuracy: 0.7156\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5498 - accuracy: 0.7375 - val_loss: 0.5439 - val_accuracy: 0.7500\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5519 - accuracy: 0.7387 - val_loss: 0.5450 - val_accuracy: 0.7192\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5442 - accuracy: 0.7344 - val_loss: 0.5408 - val_accuracy: 0.7437\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.4106 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0160s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.1540 - accuracy: 0.4273\n",
      "training acc:  0.734375 , training loss:  0.5441896319389343 , val acc:  0.7436594367027283 , val loss:  0.5408129096031189 , test acc:  0.4273280203342438 , test loss:  1.1539785861968994\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt4/400word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less page seems function though likely around help lot give always best form either url$ etc means must light bit x made small trying keep end least us world following % option size anything note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution specific field check similar > key simple information times far often come never single type c word line looking text side quite list important usually instead place works result b life run course put idea fact output open makes correct hard understand show server group human last working assume water amount whether past pretty matter site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character getting    else left free got seem object 10 difference short full i.e. values camera online wo away available term common words provide old area needs current making lower everything lens experience although almost feel easy tried comes book thought future useful call higher source smaller price test changes thus top results within    true input write ask created avoid unless perhaps bytes hand several \\\\ close big tell class design layer mm numbers effect air clear position    others running block access 4 days wrong e.g. exactly anyone multiple three takes actual post fine product believe difficult map game target language search standard story ' nothing terms n model sort yes # length larger cause generally research sound along cases google certain company known money turn earth gives mind uses whole return ones taking history easily theory   computer allow normal writing thanks problems become address n$\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7058 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 0.0104s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6517 - accuracy: 0.6931 - val_loss: 0.6386 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6327 - accuracy: 0.7056 - val_loss: 0.6322 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6207 - accuracy: 0.7100 - val_loss: 0.6302 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6270 - accuracy: 0.6969 - val_loss: 0.6218 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6188 - accuracy: 0.6975 - val_loss: 0.6107 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6061 - accuracy: 0.7100 - val_loss: 0.6157 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6183 - accuracy: 0.6856 - val_loss: 0.6009 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5848 - accuracy: 0.7194 - val_loss: 0.5928 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5827 - accuracy: 0.7163 - val_loss: 0.5896 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5886 - accuracy: 0.7063 - val_loss: 0.5828 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5637 - accuracy: 0.7294 - val_loss: 0.5778 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5629 - accuracy: 0.7181 - val_loss: 0.5730 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5840 - accuracy: 0.6913 - val_loss: 0.5702 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5731 - accuracy: 0.6837 - val_loss: 0.5716 - val_accuracy: 0.7391\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5718 - accuracy: 0.7113 - val_loss: 0.5628 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5544 - accuracy: 0.7144 - val_loss: 0.5608 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5543 - accuracy: 0.7150 - val_loss: 0.5589 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5677 - accuracy: 0.7006 - val_loss: 0.5540 - val_accuracy: 0.7264\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5552 - accuracy: 0.7244 - val_loss: 0.5565 - val_accuracy: 0.7518\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5366 - accuracy: 0.7356 - val_loss: 0.5499 - val_accuracy: 0.7065\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5549 - accuracy: 0.7312 - val_loss: 0.5515 - val_accuracy: 0.7029\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5555 - accuracy: 0.7244 - val_loss: 0.5509 - val_accuracy: 0.7029\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5558 - accuracy: 0.7362 - val_loss: 0.5470 - val_accuracy: 0.7509\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5547 - accuracy: 0.7319 - val_loss: 0.5436 - val_accuracy: 0.7373\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5510 - accuracy: 0.7306 - val_loss: 0.5423 - val_accuracy: 0.7391\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5380 - accuracy: 0.7481 - val_loss: 0.5439 - val_accuracy: 0.7527\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5292 - accuracy: 0.7369 - val_loss: 0.5414 - val_accuracy: 0.7491\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5327 - accuracy: 0.7588 - val_loss: 0.5397 - val_accuracy: 0.7400\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5555 - accuracy: 0.7350 - val_loss: 0.5381 - val_accuracy: 0.7455\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5484 - accuracy: 0.7381 - val_loss: 0.5378 - val_accuracy: 0.7500\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.3745 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_test_batch_end` time: 0.0227s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.1164 - accuracy: 0.4296\n",
      "training acc:  0.7381250262260437 , training loss:  0.5483862161636353 , val acc:  0.75 , val loss:  0.537841260433197 , test acc:  0.42963507771492004 , test loss:  1.1163873672485352\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt4/450word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less page seems function though likely around help lot give always best form either url$ etc means must light bit x made small trying keep end least us world following % option size anything note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution specific field check similar > key simple information times far often come never single type c word line looking text side quite list important usually instead place works result b life run course put idea fact output open makes correct hard understand show server group human last working assume water amount whether past pretty matter site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character getting    else left free got seem object 10 difference short full i.e. values camera online wo away available term common words provide old area needs current making lower everything lens experience although almost feel easy tried comes book thought future useful call higher source smaller price test changes thus top results within    true input write ask created avoid unless perhaps bytes hand several \\\\ close big tell class design layer mm numbers effect air clear position    others running block access 4 days wrong e.g. exactly anyone multiple three takes actual post fine product believe difficult map game target language search standard story ' nothing terms n model sort yes # length larger cause generally research sound along cases google certain company known money turn earth gives mind uses whole return ones taking history easily theory   computer allow normal writing thanks problems become address n$ support account per step happen inside added seen says e bad sometimes therefore setting structure play 100 content network english 6 children meaning food functions gets move related head fire rate main slightly context reading situation force update mode necessary build view body looks examples flash local options points stop\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 1.7468 - accuracy: 0.0625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_train_batch_end` time: 0.0133s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.8046 - accuracy: 0.5175 - val_loss: 0.6251 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6109 - accuracy: 0.7194 - val_loss: 0.6209 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6164 - accuracy: 0.7069 - val_loss: 0.6159 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6206 - accuracy: 0.6950 - val_loss: 0.6119 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6109 - accuracy: 0.7038 - val_loss: 0.6072 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6094 - accuracy: 0.7000 - val_loss: 0.6023 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5977 - accuracy: 0.7144 - val_loss: 0.5990 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6112 - accuracy: 0.6856 - val_loss: 0.5937 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5812 - accuracy: 0.7225 - val_loss: 0.5896 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5803 - accuracy: 0.7150 - val_loss: 0.5875 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6058 - accuracy: 0.6719 - val_loss: 0.5814 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5874 - accuracy: 0.6956 - val_loss: 0.5787 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5614 - accuracy: 0.7231 - val_loss: 0.5729 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5554 - accuracy: 0.7262 - val_loss: 0.5734 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5599 - accuracy: 0.7200 - val_loss: 0.5679 - val_accuracy: 0.7020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5649 - accuracy: 0.7150 - val_loss: 0.5642 - val_accuracy: 0.7020\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5511 - accuracy: 0.7156 - val_loss: 0.5599 - val_accuracy: 0.7020\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5571 - accuracy: 0.7156 - val_loss: 0.5572 - val_accuracy: 0.7029\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5661 - accuracy: 0.7044 - val_loss: 0.5568 - val_accuracy: 0.7382\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5564 - accuracy: 0.7237 - val_loss: 0.5532 - val_accuracy: 0.7237\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5348 - accuracy: 0.7450 - val_loss: 0.5530 - val_accuracy: 0.7029\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5705 - accuracy: 0.7081 - val_loss: 0.5531 - val_accuracy: 0.7518\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5463 - accuracy: 0.7381 - val_loss: 0.5594 - val_accuracy: 0.7464\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5501 - accuracy: 0.7306 - val_loss: 0.5450 - val_accuracy: 0.7283\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5395 - accuracy: 0.7381 - val_loss: 0.5472 - val_accuracy: 0.7065\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5531 - accuracy: 0.7269 - val_loss: 0.5421 - val_accuracy: 0.7428\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5608 - accuracy: 0.7244 - val_loss: 0.5427 - val_accuracy: 0.7509\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5407 - accuracy: 0.7344 - val_loss: 0.5405 - val_accuracy: 0.7482\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5197 - accuracy: 0.7500 - val_loss: 0.5435 - val_accuracy: 0.7156\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5360 - accuracy: 0.7444 - val_loss: 0.5443 - val_accuracy: 0.7147\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.2018 - accuracy: 0.4167\n",
      "training acc:  0.7443749904632568 , training loss:  0.5360114574432373 , val acc:  0.7146739363670349 , val loss:  0.5443083047866821 , test acc:  0.41673657298088074 , test loss:  1.2017656564712524\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_600/opt4/500word_list.txt\n",
      "WORD INDEX STR: . ,  $   's would n't one like / use also could get time   need way make work - even using want 'm think first people two may see know = might much 1 different something example used question 've well good however really 're case number set take since still many say find 2 problem new possible another try things probably point enough go right + answer let without value better actually system data back ca 'll going 'd change long every sure given image able look high code 3 \\mathcal power add create thing part second less page seems function though likely around help lot give always best form either url$ etc means must light bit x made small trying keep end least us world following % option size anything note maybe done start day based space 0 found file order real reason said already years read person \\to & large non process edit rather little    mean issue someone level simply solution specific field check similar > key simple information times far often come never single type c word line looking text side quite list important usually instead place works result b life run course put idea fact output open makes correct hard understand show server group human last working assume water amount whether past pretty matter site paper state god color user ai control range kind next < method due error name great sense version low 5 called particular year general consider original energy character getting    else left free got seem object 10 difference short full i.e. values camera online wo away available term common words provide old area needs current making lower everything lens experience although almost feel easy tried comes book thought future useful call higher source smaller price test changes thus top results within    true input write ask created avoid unless perhaps bytes hand several \\\\ close big tell class design layer mm numbers effect air clear position    others running block access 4 days wrong e.g. exactly anyone multiple three takes actual post fine product believe difficult map game target language search standard story ' nothing terms n model sort yes # length larger cause generally research sound along cases google certain company known money turn earth gives mind uses whole return ones taking history easily theory   computer allow normal writing thanks problems become address n$ support account per step happen inside added seen says e bad sometimes therefore setting structure play 100 content network english 6 children meaning food functions gets move related head fire rate main slightly context reading situation force update mode necessary build view body looks examples flash local options points stop job link knowledge choose rules remember yet exact depends approach random entire planet select tool apply characters parts assuming guess rule speed whatever instance half directly \\ noise outside total algorithm mentioned conditions reasons worth quality later save major risk background needed pressure please client personal images humans limit questions\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.8487 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0155s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6440 - accuracy: 0.6637 - val_loss: 0.6323 - val_accuracy: 0.7020\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6514 - accuracy: 0.6712 - val_loss: 0.6251 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6347 - accuracy: 0.6862 - val_loss: 0.6185 - val_accuracy: 0.7020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6267 - accuracy: 0.6862 - val_loss: 0.6157 - val_accuracy: 0.7020\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6106 - accuracy: 0.7019 - val_loss: 0.6095 - val_accuracy: 0.7020\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6013 - accuracy: 0.7100 - val_loss: 0.6017 - val_accuracy: 0.7020\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6055 - accuracy: 0.6988 - val_loss: 0.5959 - val_accuracy: 0.7020\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6015 - accuracy: 0.6969 - val_loss: 0.5908 - val_accuracy: 0.7020\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5898 - accuracy: 0.6969 - val_loss: 0.6003 - val_accuracy: 0.7020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5959 - accuracy: 0.6862 - val_loss: 0.5805 - val_accuracy: 0.7020\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5813 - accuracy: 0.6944 - val_loss: 0.5746 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5877 - accuracy: 0.6950 - val_loss: 0.5706 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5719 - accuracy: 0.7069 - val_loss: 0.5677 - val_accuracy: 0.7020\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5463 - accuracy: 0.7287 - val_loss: 0.5629 - val_accuracy: 0.7020\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5653 - accuracy: 0.7050 - val_loss: 0.5599 - val_accuracy: 0.7029\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5678 - accuracy: 0.7050 - val_loss: 0.5575 - val_accuracy: 0.7120\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5536 - accuracy: 0.7156 - val_loss: 0.5547 - val_accuracy: 0.7065\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5666 - accuracy: 0.7131 - val_loss: 0.5547 - val_accuracy: 0.7455\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5346 - accuracy: 0.7369 - val_loss: 0.5501 - val_accuracy: 0.7120\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5423 - accuracy: 0.7319 - val_loss: 0.5480 - val_accuracy: 0.7237\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5617 - accuracy: 0.7113 - val_loss: 0.5471 - val_accuracy: 0.7446\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5344 - accuracy: 0.7400 - val_loss: 0.5464 - val_accuracy: 0.7165\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5555 - accuracy: 0.7281 - val_loss: 0.5435 - val_accuracy: 0.7418\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5449 - accuracy: 0.7294 - val_loss: 0.5423 - val_accuracy: 0.7455\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5558 - accuracy: 0.7312 - val_loss: 0.5414 - val_accuracy: 0.7409\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5226 - accuracy: 0.7525 - val_loss: 0.5402 - val_accuracy: 0.7446\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5553 - accuracy: 0.7369 - val_loss: 0.5403 - val_accuracy: 0.7563\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5143 - accuracy: 0.7719 - val_loss: 0.5416 - val_accuracy: 0.7264\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5305 - accuracy: 0.7613 - val_loss: 0.5384 - val_accuracy: 0.7418\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5403 - accuracy: 0.7450 - val_loss: 0.5403 - val_accuracy: 0.7337\n",
      "  1/596 [..............................] - ETA: 3s - loss: 1.5320 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0132s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.2249 - accuracy: 0.4216\n",
      "training acc:  0.7450000047683716 , training loss:  0.5402573943138123 , val acc:  0.7336956262588501 , val loss:  0.5403035283088684 , test acc:  0.4215604066848755 , test loss:  1.2249304056167603\n",
      "\n",
      "Started evaluation for: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_800.csv\n",
      "Loading training file from path:  /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_800.csv\n",
      "(3793, 6)\n",
      "Loading testing file from path:  /home/sukanya/PhD/Datasets/PAN SCD/pan19-style-change-detection/processed/test.csv\n",
      "(9538, 5)\n",
      "opt1\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt1/50word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.7231 - accuracy: 0.5875 - val_loss: 0.6240 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6387 - accuracy: 0.6800 - val_loss: 0.6207 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6333 - accuracy: 0.6806 - val_loss: 0.6205 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6132 - accuracy: 0.7056 - val_loss: 0.6142 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6174 - accuracy: 0.6994 - val_loss: 0.6109 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6221 - accuracy: 0.6913 - val_loss: 0.6083 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6194 - accuracy: 0.6844 - val_loss: 0.6032 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6052 - accuracy: 0.7031 - val_loss: 0.6021 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6097 - accuracy: 0.6925 - val_loss: 0.6024 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.6116 - accuracy: 0.6875 - val_loss: 0.5932 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5927 - accuracy: 0.7088 - val_loss: 0.5896 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5953 - accuracy: 0.6994 - val_loss: 0.5862 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5934 - accuracy: 0.7019 - val_loss: 0.5824 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5821 - accuracy: 0.7206 - val_loss: 0.5796 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5818 - accuracy: 0.7081 - val_loss: 0.5765 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5907 - accuracy: 0.6969 - val_loss: 0.5745 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5855 - accuracy: 0.7006 - val_loss: 0.5705 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5793 - accuracy: 0.7000 - val_loss: 0.5677 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5707 - accuracy: 0.7069 - val_loss: 0.5649 - val_accuracy: 0.7021\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5588 - accuracy: 0.7144 - val_loss: 0.5615 - val_accuracy: 0.7021\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5886 - accuracy: 0.6869 - val_loss: 0.5619 - val_accuracy: 0.7048\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5706 - accuracy: 0.7050 - val_loss: 0.5571 - val_accuracy: 0.7021\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5745 - accuracy: 0.7013 - val_loss: 0.5589 - val_accuracy: 0.7021\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5783 - accuracy: 0.6969 - val_loss: 0.5526 - val_accuracy: 0.7021\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5723 - accuracy: 0.7050 - val_loss: 0.5504 - val_accuracy: 0.7021\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5626 - accuracy: 0.6975 - val_loss: 0.5505 - val_accuracy: 0.7234\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5535 - accuracy: 0.7125 - val_loss: 0.5464 - val_accuracy: 0.7101\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5569 - accuracy: 0.7194 - val_loss: 0.5441 - val_accuracy: 0.7061\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5557 - accuracy: 0.7088 - val_loss: 0.5451 - val_accuracy: 0.7021\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5586 - accuracy: 0.7056 - val_loss: 0.5490 - val_accuracy: 0.7713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/596 [..............................] - ETA: 7s - loss: 1.0291 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0135s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.0779 - accuracy: 0.4238\n",
      "training acc:  0.7056249976158142 , training loss:  0.5585939288139343 , val acc:  0.771276593208313 , val loss:  0.5489546656608582 , test acc:  0.42376258969306946 , test loss:  1.077937126159668\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt1/100word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7492 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0056s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6443 - accuracy: 0.6612 - val_loss: 0.6296 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6206 - accuracy: 0.7063 - val_loss: 0.6322 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6077 - accuracy: 0.7194 - val_loss: 0.6201 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6231 - accuracy: 0.6888 - val_loss: 0.6277 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6145 - accuracy: 0.7038 - val_loss: 0.6089 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6110 - accuracy: 0.6988 - val_loss: 0.6100 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6161 - accuracy: 0.6850 - val_loss: 0.6013 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6022 - accuracy: 0.6988 - val_loss: 0.5928 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5817 - accuracy: 0.7212 - val_loss: 0.5883 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5963 - accuracy: 0.6969 - val_loss: 0.5822 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5695 - accuracy: 0.7244 - val_loss: 0.5777 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5661 - accuracy: 0.7244 - val_loss: 0.5718 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5815 - accuracy: 0.6969 - val_loss: 0.5664 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5770 - accuracy: 0.6994 - val_loss: 0.5627 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5649 - accuracy: 0.7075 - val_loss: 0.5570 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5571 - accuracy: 0.7194 - val_loss: 0.5530 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5681 - accuracy: 0.7106 - val_loss: 0.5486 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5785 - accuracy: 0.6950 - val_loss: 0.5461 - val_accuracy: 0.7048\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5576 - accuracy: 0.7200 - val_loss: 0.5431 - val_accuracy: 0.7035\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5568 - accuracy: 0.7200 - val_loss: 0.5404 - val_accuracy: 0.7048\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5617 - accuracy: 0.7038 - val_loss: 0.5399 - val_accuracy: 0.7527\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5508 - accuracy: 0.7250 - val_loss: 0.5506 - val_accuracy: 0.7021\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5578 - accuracy: 0.7069 - val_loss: 0.5318 - val_accuracy: 0.7181\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5420 - accuracy: 0.7244 - val_loss: 0.5296 - val_accuracy: 0.7460\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5575 - accuracy: 0.7075 - val_loss: 0.5274 - val_accuracy: 0.7407\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5657 - accuracy: 0.7256 - val_loss: 0.5268 - val_accuracy: 0.7234\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5472 - accuracy: 0.7269 - val_loss: 0.5246 - val_accuracy: 0.7566\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5579 - accuracy: 0.7200 - val_loss: 0.5238 - val_accuracy: 0.7620\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5464 - accuracy: 0.7325 - val_loss: 0.5267 - val_accuracy: 0.7048\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5541 - accuracy: 0.7206 - val_loss: 0.5197 - val_accuracy: 0.7527\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.2150 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_test_batch_end` time: 0.0140s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.2491 - accuracy: 0.4226\n",
      "training acc:  0.7206249833106995 , training loss:  0.5540658831596375 , val acc:  0.7526595592498779 , val loss:  0.5196555256843567 , test acc:  0.4226090610027313 , test loss:  1.2491300106048584\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt1/150word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know most me such 1 he each something different example now question used well its 've however being good really been am after 're number case set still take since many say who problem try new things 2 those find our probably possible another between both were point had while over\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 2.1487 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0210s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8620 - accuracy: 0.4956 - val_loss: 0.6202 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6110 - accuracy: 0.7113 - val_loss: 0.6170 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6163 - accuracy: 0.7031 - val_loss: 0.6137 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6103 - accuracy: 0.7050 - val_loss: 0.6099 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5939 - accuracy: 0.7225 - val_loss: 0.6074 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5881 - accuracy: 0.7269 - val_loss: 0.6088 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6114 - accuracy: 0.6919 - val_loss: 0.5976 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6045 - accuracy: 0.7019 - val_loss: 0.5933 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5746 - accuracy: 0.7306 - val_loss: 0.5897 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5818 - accuracy: 0.7163 - val_loss: 0.5847 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5958 - accuracy: 0.6913 - val_loss: 0.5796 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5764 - accuracy: 0.7150 - val_loss: 0.5763 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5801 - accuracy: 0.7031 - val_loss: 0.5827 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5862 - accuracy: 0.6919 - val_loss: 0.5705 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5808 - accuracy: 0.7050 - val_loss: 0.5760 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5788 - accuracy: 0.6913 - val_loss: 0.5599 - val_accuracy: 0.7035\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5719 - accuracy: 0.7031 - val_loss: 0.5574 - val_accuracy: 0.7061\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5674 - accuracy: 0.7056 - val_loss: 0.5559 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5775 - accuracy: 0.6906 - val_loss: 0.5514 - val_accuracy: 0.7021\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5718 - accuracy: 0.6975 - val_loss: 0.5453 - val_accuracy: 0.7048\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5546 - accuracy: 0.7194 - val_loss: 0.5424 - val_accuracy: 0.7141\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5617 - accuracy: 0.7150 - val_loss: 0.5394 - val_accuracy: 0.7048\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5587 - accuracy: 0.7088 - val_loss: 0.5384 - val_accuracy: 0.7035\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5590 - accuracy: 0.7106 - val_loss: 0.5340 - val_accuracy: 0.7154\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5428 - accuracy: 0.7356 - val_loss: 0.5342 - val_accuracy: 0.7048\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5640 - accuracy: 0.7275 - val_loss: 0.5292 - val_accuracy: 0.7367\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5576 - accuracy: 0.7250 - val_loss: 0.5274 - val_accuracy: 0.7327\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5420 - accuracy: 0.7219 - val_loss: 0.5277 - val_accuracy: 0.7141\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5461 - accuracy: 0.7244 - val_loss: 0.5240 - val_accuracy: 0.7367\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5512 - accuracy: 0.7287 - val_loss: 0.5224 - val_accuracy: 0.7367\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.2102 - accuracy: 0.4215\n",
      "training acc:  0.7287499904632568 , training loss:  0.5511771440505981 , val acc:  0.7367021441459656 , val loss:  0.522408127784729 , test acc:  0.4214555323123932 , test loss:  1.2101919651031494\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt1/200word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know most me such 1 he each something different example now question used well its 've however being good really been am after 're number case set still take since many say who problem try new things 2 those find our probably possible another between both were point had while over too answer enough his value + why did right without let go system better actually through every data going power \\mathcal back long down 'd given before 'll able code change ca own sure look image 3 create part less high thing form off function second likely best either add\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.7914 - accuracy: 0.5337 - val_loss: 0.6256 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6239 - accuracy: 0.7006 - val_loss: 0.6221 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6325 - accuracy: 0.6862 - val_loss: 0.6185 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6243 - accuracy: 0.6919 - val_loss: 0.6146 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6110 - accuracy: 0.7050 - val_loss: 0.6106 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6067 - accuracy: 0.7088 - val_loss: 0.6070 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6157 - accuracy: 0.6894 - val_loss: 0.6016 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5918 - accuracy: 0.7169 - val_loss: 0.6006 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6045 - accuracy: 0.6988 - val_loss: 0.5967 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5980 - accuracy: 0.7000 - val_loss: 0.5883 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5927 - accuracy: 0.7019 - val_loss: 0.5831 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5833 - accuracy: 0.7088 - val_loss: 0.5778 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5763 - accuracy: 0.7163 - val_loss: 0.5754 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5901 - accuracy: 0.6888 - val_loss: 0.5688 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5844 - accuracy: 0.6956 - val_loss: 0.5644 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5876 - accuracy: 0.6869 - val_loss: 0.5604 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5754 - accuracy: 0.6956 - val_loss: 0.5574 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5767 - accuracy: 0.6981 - val_loss: 0.5518 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5616 - accuracy: 0.7094 - val_loss: 0.5485 - val_accuracy: 0.7021\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5445 - accuracy: 0.7206 - val_loss: 0.5513 - val_accuracy: 0.7021\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5613 - accuracy: 0.7163 - val_loss: 0.5410 - val_accuracy: 0.7048\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5623 - accuracy: 0.7044 - val_loss: 0.5376 - val_accuracy: 0.7048\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5586 - accuracy: 0.7138 - val_loss: 0.5348 - val_accuracy: 0.7048\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5553 - accuracy: 0.7244 - val_loss: 0.5320 - val_accuracy: 0.7074\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5648 - accuracy: 0.7219 - val_loss: 0.5301 - val_accuracy: 0.7620\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5471 - accuracy: 0.7244 - val_loss: 0.5263 - val_accuracy: 0.7646\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5423 - accuracy: 0.7337 - val_loss: 0.5232 - val_accuracy: 0.7367\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5232 - accuracy: 0.7531 - val_loss: 0.5221 - val_accuracy: 0.7354\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5470 - accuracy: 0.7325 - val_loss: 0.5199 - val_accuracy: 0.7394\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5487 - accuracy: 0.7462 - val_loss: 0.5193 - val_accuracy: 0.7367\n",
      "  1/596 [..............................] - ETA: 8s - loss: 1.2579 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0127s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.2481 - accuracy: 0.4198\n",
      "training acc:  0.7462499737739563 , training loss:  0.5487406849861145 , val acc:  0.7367021441459656 , val loss:  0.5193036198616028 , test acc:  0.41977769136428833 , test loss:  1.248141884803772\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt1/250word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know most me such 1 he each something different example now question used well its 've however being good really been am after 're number case set still take since many say who problem try new things 2 those find our probably possible another between both were point had while over too answer enough his value + why did right without let go system better actually through every data going power \\mathcal back long down 'd given before 'll able code change ca own sure look image 3 create part less high thing form off function second likely best either add must etc seems give having she around help lot always option made means though small us page world light keep bit space anything real url$ above trying end years least large following note few non order \\to & doing maybe x person done day start    size reason said process\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6565 - accuracy: 0.7006 - val_loss: 0.6442 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6305 - accuracy: 0.7125 - val_loss: 0.6373 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6370 - accuracy: 0.6950 - val_loss: 0.6308 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6161 - accuracy: 0.7181 - val_loss: 0.6245 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6322 - accuracy: 0.6869 - val_loss: 0.6194 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6036 - accuracy: 0.7188 - val_loss: 0.6176 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6087 - accuracy: 0.7088 - val_loss: 0.6101 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6223 - accuracy: 0.6850 - val_loss: 0.6050 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5994 - accuracy: 0.7106 - val_loss: 0.6000 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6076 - accuracy: 0.6919 - val_loss: 0.5958 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5922 - accuracy: 0.7088 - val_loss: 0.5910 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5882 - accuracy: 0.7094 - val_loss: 0.5854 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6046 - accuracy: 0.6831 - val_loss: 0.5797 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5871 - accuracy: 0.7019 - val_loss: 0.5762 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5684 - accuracy: 0.7206 - val_loss: 0.5714 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5947 - accuracy: 0.6844 - val_loss: 0.5748 - val_accuracy: 0.7181\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5679 - accuracy: 0.7119 - val_loss: 0.5626 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5634 - accuracy: 0.7144 - val_loss: 0.5579 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5594 - accuracy: 0.7138 - val_loss: 0.5574 - val_accuracy: 0.7021\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5508 - accuracy: 0.7212 - val_loss: 0.5497 - val_accuracy: 0.7035\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5677 - accuracy: 0.6938 - val_loss: 0.5465 - val_accuracy: 0.7074\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5762 - accuracy: 0.6969 - val_loss: 0.5449 - val_accuracy: 0.7394\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5671 - accuracy: 0.7075 - val_loss: 0.5412 - val_accuracy: 0.7380\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5662 - accuracy: 0.7100 - val_loss: 0.5368 - val_accuracy: 0.7061\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5717 - accuracy: 0.7031 - val_loss: 0.5403 - val_accuracy: 0.7779\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5419 - accuracy: 0.7244 - val_loss: 0.5323 - val_accuracy: 0.7101\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5535 - accuracy: 0.7225 - val_loss: 0.5294 - val_accuracy: 0.7181\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5647 - accuracy: 0.7231 - val_loss: 0.5271 - val_accuracy: 0.7473\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5336 - accuracy: 0.7444 - val_loss: 0.5261 - val_accuracy: 0.7181\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5483 - accuracy: 0.7275 - val_loss: 0.5229 - val_accuracy: 0.7646\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.1344 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0141s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.1563 - accuracy: 0.4232\n",
      "training acc:  0.7275000214576721 , training loss:  0.5483009815216064 , val acc:  0.7646276354789734 , val loss:  0.5229161977767944 , test acc:  0.42323824763298035 , test loss:  1.1562700271606445\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt1/300word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know most me such 1 he each something different example now question used well its 've however being good really been am after 're number case set still take since many say who problem try new things 2 those find our probably possible another between both were point had while over too answer enough his value + why did right without let go system better actually through every data going power \\mathcal back long down 'd given before 'll able code change ca own sure look image 3 create part less high thing form off function second likely best either add must etc seems give having she around help lot always option made means though small us page world light keep bit space anything real url$ above trying end years least large following note few non order \\to & doing maybe x person done day start    size reason said process % already level read 0 > based someone once simply rather her life found issue times information specific type solution little check again similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.7592 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0081s vs `on_train_batch_end` time: 0.0238s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6578 - accuracy: 0.6306 - val_loss: 0.6316 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6325 - accuracy: 0.6925 - val_loss: 0.6265 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6248 - accuracy: 0.7006 - val_loss: 0.6248 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6187 - accuracy: 0.7025 - val_loss: 0.6170 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6202 - accuracy: 0.6963 - val_loss: 0.6125 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6054 - accuracy: 0.7100 - val_loss: 0.6083 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6127 - accuracy: 0.6925 - val_loss: 0.6014 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6020 - accuracy: 0.7025 - val_loss: 0.5976 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6035 - accuracy: 0.6963 - val_loss: 0.5902 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5913 - accuracy: 0.7069 - val_loss: 0.5857 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5855 - accuracy: 0.7063 - val_loss: 0.5798 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5743 - accuracy: 0.7156 - val_loss: 0.5751 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5785 - accuracy: 0.7063 - val_loss: 0.5715 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5827 - accuracy: 0.7000 - val_loss: 0.5655 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5890 - accuracy: 0.6812 - val_loss: 0.5623 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5736 - accuracy: 0.7019 - val_loss: 0.5577 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5581 - accuracy: 0.7181 - val_loss: 0.5551 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5611 - accuracy: 0.7081 - val_loss: 0.5523 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5669 - accuracy: 0.7025 - val_loss: 0.5452 - val_accuracy: 0.7035\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5578 - accuracy: 0.7131 - val_loss: 0.5414 - val_accuracy: 0.7048\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5546 - accuracy: 0.7181 - val_loss: 0.5486 - val_accuracy: 0.7021\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5640 - accuracy: 0.7131 - val_loss: 0.5382 - val_accuracy: 0.7035\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5599 - accuracy: 0.7125 - val_loss: 0.5350 - val_accuracy: 0.7048\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5478 - accuracy: 0.7181 - val_loss: 0.5300 - val_accuracy: 0.7380\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5415 - accuracy: 0.7312 - val_loss: 0.5324 - val_accuracy: 0.7048\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5556 - accuracy: 0.7262 - val_loss: 0.5362 - val_accuracy: 0.7035\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5463 - accuracy: 0.7312 - val_loss: 0.5234 - val_accuracy: 0.7367\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5457 - accuracy: 0.7387 - val_loss: 0.5216 - val_accuracy: 0.7354\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5534 - accuracy: 0.7262 - val_loss: 0.5223 - val_accuracy: 0.7221\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5438 - accuracy: 0.7294 - val_loss: 0.5174 - val_accuracy: 0.7606\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.1698 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0184s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 1.1900 - accuracy: 0.4228\n",
      "training acc:  0.7293750047683716 , training loss:  0.543764591217041 , val acc:  0.7606382966041565 , val loss:  0.517353355884552 , test acc:  0.42281877994537354 , test loss:  1.1900489330291748\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt1/350word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know most me such 1 he each something different example now question used well its 've however being good really been am after 're number case set still take since many say who problem try new things 2 those find our probably possible another between both were point had while over too answer enough his value + why did right without let go system better actually through every data going power \\mathcal back long down 'd given before 'll able code change ca own sure look image 3 create part less high thing form off function second likely best either add must etc seems give having she around help lot always option made means though small us page world light keep bit space anything real url$ above trying end years least large following note few non order \\to & doing maybe x person done day start    size reason said process % already level read 0 > based someone once simply rather her life found issue times information specific type solution little check again similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line until put < c idea water god working group list state assume past under whether run matter place him last amount server great paper correct energy control itself open next method range due pretty sense i.e. called character color getting particular consider site 5 kind\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6903 - accuracy: 0.5987 - val_loss: 0.6285 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6310 - accuracy: 0.6938 - val_loss: 0.6245 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6273 - accuracy: 0.6938 - val_loss: 0.6171 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6185 - accuracy: 0.6975 - val_loss: 0.6103 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6110 - accuracy: 0.7013 - val_loss: 0.6043 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5921 - accuracy: 0.7200 - val_loss: 0.6042 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5976 - accuracy: 0.7038 - val_loss: 0.5913 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5821 - accuracy: 0.7156 - val_loss: 0.5842 - val_accuracy: 0.7021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5854 - accuracy: 0.7106 - val_loss: 0.5785 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5869 - accuracy: 0.7013 - val_loss: 0.5744 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5883 - accuracy: 0.7013 - val_loss: 0.5665 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5774 - accuracy: 0.6994 - val_loss: 0.5635 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5744 - accuracy: 0.6994 - val_loss: 0.5556 - val_accuracy: 0.7035\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5621 - accuracy: 0.7175 - val_loss: 0.5521 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5671 - accuracy: 0.7050 - val_loss: 0.5519 - val_accuracy: 0.7699\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5433 - accuracy: 0.7350 - val_loss: 0.5673 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5569 - accuracy: 0.7094 - val_loss: 0.5426 - val_accuracy: 0.7713\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5565 - accuracy: 0.7206 - val_loss: 0.5355 - val_accuracy: 0.7048\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5654 - accuracy: 0.7125 - val_loss: 0.5332 - val_accuracy: 0.7739\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5646 - accuracy: 0.7312 - val_loss: 0.5268 - val_accuracy: 0.7434\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5439 - accuracy: 0.7212 - val_loss: 0.5242 - val_accuracy: 0.7660\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5519 - accuracy: 0.7394 - val_loss: 0.5258 - val_accuracy: 0.7141\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5656 - accuracy: 0.7244 - val_loss: 0.5198 - val_accuracy: 0.7660\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5545 - accuracy: 0.7381 - val_loss: 0.5182 - val_accuracy: 0.7407\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5452 - accuracy: 0.7344 - val_loss: 0.5154 - val_accuracy: 0.7646\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5371 - accuracy: 0.7462 - val_loss: 0.5138 - val_accuracy: 0.7620\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5215 - accuracy: 0.7594 - val_loss: 0.5139 - val_accuracy: 0.7407\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5215 - accuracy: 0.7594 - val_loss: 0.5117 - val_accuracy: 0.7513\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5421 - accuracy: 0.7506 - val_loss: 0.5094 - val_accuracy: 0.7620\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5315 - accuracy: 0.7575 - val_loss: 0.5070 - val_accuracy: 0.7686\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.2851 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0150s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.2957 - accuracy: 0.4241\n",
      "training acc:  0.7574999928474426 , training loss:  0.5314987897872925 , val acc:  0.7686170339584351 , val loss:  0.5069977045059204 , test acc:  0.42407718300819397 , test loss:  1.2957308292388916\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt1/400word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know most me such 1 he each something different example now question used well its 've however being good really been am after 're number case set still take since many say who problem try new things 2 those find our probably possible another between both were point had while over too answer enough his value + why did right without let go system better actually through every data going power \\mathcal back long down 'd given before 'll able code change ca own sure look image 3 create part less high thing form off function second likely best either add must etc seems give having she around help lot always option made means though small us page world light keep bit space anything real url$ above trying end years least large following note few non order \\to & doing maybe x person done day start    size reason said process % already level read 0 > based someone once simply rather her life found issue times information specific type solution little check again similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line until put < c idea water god working group list state assume past under whether run matter place him last amount server great paper correct energy control itself open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year original object below higher online bytes making useful left difference current needs area user values free price future everything lens test provide away common almost results thus seem input further 10 call lower position class camera easy although name book got old target\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7098 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0026s vs `on_train_batch_end` time: 0.0206s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6455 - accuracy: 0.6931 - val_loss: 0.6383 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6304 - accuracy: 0.7056 - val_loss: 0.6314 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6424 - accuracy: 0.6806 - val_loss: 0.6257 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6143 - accuracy: 0.7119 - val_loss: 0.6239 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5998 - accuracy: 0.7225 - val_loss: 0.6135 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6057 - accuracy: 0.7094 - val_loss: 0.6116 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6154 - accuracy: 0.6906 - val_loss: 0.6030 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6020 - accuracy: 0.7019 - val_loss: 0.5957 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5770 - accuracy: 0.7269 - val_loss: 0.5903 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5906 - accuracy: 0.7000 - val_loss: 0.5846 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5915 - accuracy: 0.6981 - val_loss: 0.5781 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5868 - accuracy: 0.6994 - val_loss: 0.5709 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5809 - accuracy: 0.6981 - val_loss: 0.5654 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5788 - accuracy: 0.7031 - val_loss: 0.5649 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5771 - accuracy: 0.6950 - val_loss: 0.5560 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5688 - accuracy: 0.7019 - val_loss: 0.5537 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5483 - accuracy: 0.7188 - val_loss: 0.5470 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5570 - accuracy: 0.7150 - val_loss: 0.5534 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5583 - accuracy: 0.7125 - val_loss: 0.5387 - val_accuracy: 0.7048\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5546 - accuracy: 0.7194 - val_loss: 0.5348 - val_accuracy: 0.7141\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5481 - accuracy: 0.7350 - val_loss: 0.5330 - val_accuracy: 0.7074\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5621 - accuracy: 0.7144 - val_loss: 0.5300 - val_accuracy: 0.7141\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5566 - accuracy: 0.7231 - val_loss: 0.5255 - val_accuracy: 0.7460\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5483 - accuracy: 0.7394 - val_loss: 0.5245 - val_accuracy: 0.7699\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5468 - accuracy: 0.7419 - val_loss: 0.5217 - val_accuracy: 0.7699\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5301 - accuracy: 0.7412 - val_loss: 0.5187 - val_accuracy: 0.7580\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5466 - accuracy: 0.7425 - val_loss: 0.5182 - val_accuracy: 0.7380\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5362 - accuracy: 0.7544 - val_loss: 0.5143 - val_accuracy: 0.7673\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5367 - accuracy: 0.7494 - val_loss: 0.5128 - val_accuracy: 0.7819\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5478 - accuracy: 0.7400 - val_loss: 0.5118 - val_accuracy: 0.7793\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.1770 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0132s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.2278 - accuracy: 0.4258\n",
      "training acc:  0.7400000095367432 , training loss:  0.5478013157844543 , val acc:  0.7792553305625916 , val loss:  0.5118091702461243 , test acc:  0.42575502395629883 , test loss:  1.2278119325637817\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt1/450word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know most me such 1 he each something different example now question used well its 've however being good really been am after 're number case set still take since many say who problem try new things 2 those find our probably possible another between both were point had while over too answer enough his value + why did right without let go system better actually through every data going power \\mathcal back long down 'd given before 'll able code change ca own sure look image 3 create part less high thing form off function second likely best either add must etc seems give having she around help lot always option made means though small us page world light keep bit space anything real url$ above trying end years least large following note few non order \\to & doing maybe x person done day start    size reason said process % already level read 0 > based someone once simply rather her life found issue times information specific type solution little check again similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line until put < c idea water god working group list state assume past under whether run matter place him last amount server great paper correct energy control itself open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year original object below higher online bytes making useful left difference current needs area user values free price future everything lens test provide away common almost results thus seem input further 10 call lower position class camera easy although name book got old target error within write comes true unless clear    words low mm full feel smaller wo available top source thought experience others big map avoid perhaps several against ask close access effect tried numbers e.g. changes tell actual created multiple difficult game three length takes larger standard 4 layer design story\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.8929 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0079s vs `on_train_batch_end` time: 0.0365s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6372 - accuracy: 0.6644 - val_loss: 0.6395 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6288 - accuracy: 0.7000 - val_loss: 0.6221 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6302 - accuracy: 0.6850 - val_loss: 0.6146 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6093 - accuracy: 0.7088 - val_loss: 0.6077 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6006 - accuracy: 0.7081 - val_loss: 0.6038 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5998 - accuracy: 0.7069 - val_loss: 0.5943 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5927 - accuracy: 0.7075 - val_loss: 0.5866 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5842 - accuracy: 0.7144 - val_loss: 0.5806 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5899 - accuracy: 0.6969 - val_loss: 0.5749 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5733 - accuracy: 0.7163 - val_loss: 0.5706 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5851 - accuracy: 0.6919 - val_loss: 0.5638 - val_accuracy: 0.7048\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5849 - accuracy: 0.6856 - val_loss: 0.5567 - val_accuracy: 0.7035\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5796 - accuracy: 0.6931 - val_loss: 0.5516 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5549 - accuracy: 0.7125 - val_loss: 0.5472 - val_accuracy: 0.7035\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5602 - accuracy: 0.7119 - val_loss: 0.5419 - val_accuracy: 0.7247\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5614 - accuracy: 0.7250 - val_loss: 0.5383 - val_accuracy: 0.7074\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5603 - accuracy: 0.7163 - val_loss: 0.5344 - val_accuracy: 0.7327\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5485 - accuracy: 0.7306 - val_loss: 0.5302 - val_accuracy: 0.7340\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5551 - accuracy: 0.7231 - val_loss: 0.5291 - val_accuracy: 0.7726\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5520 - accuracy: 0.7362 - val_loss: 0.5261 - val_accuracy: 0.7194\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5531 - accuracy: 0.7237 - val_loss: 0.5236 - val_accuracy: 0.7301\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5425 - accuracy: 0.7425 - val_loss: 0.5212 - val_accuracy: 0.7340\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5505 - accuracy: 0.7375 - val_loss: 0.5172 - val_accuracy: 0.7553\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5294 - accuracy: 0.7487 - val_loss: 0.5149 - val_accuracy: 0.7766\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5384 - accuracy: 0.7425 - val_loss: 0.5223 - val_accuracy: 0.7673\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5478 - accuracy: 0.7406 - val_loss: 0.5145 - val_accuracy: 0.7367\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5319 - accuracy: 0.7588 - val_loss: 0.5120 - val_accuracy: 0.7779\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5410 - accuracy: 0.7500 - val_loss: 0.5088 - val_accuracy: 0.7633\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5202 - accuracy: 0.7656 - val_loss: 0.5091 - val_accuracy: 0.7753\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5268 - accuracy: 0.7563 - val_loss: 0.5073 - val_accuracy: 0.7566\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.3348 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0145s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.3404 - accuracy: 0.4223\n",
      "training acc:  0.7562500238418579 , training loss:  0.5267627835273743 , val acc:  0.7566489577293396 , val loss:  0.5072741508483887 , test acc:  0.4222944676876068 , test loss:  1.3403602838516235\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt1/500word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know most me such 1 he each something different example now question used well its 've however being good really been am after 're number case set still take since many say who problem try new things 2 those find our probably possible another between both were point had while over too answer enough his value + why did right without let go system better actually through every data going power \\mathcal back long down 'd given before 'll able code change ca own sure look image 3 create part less high thing form off function second likely best either add must etc seems give having she around help lot always option made means though small us page world light keep bit space anything real url$ above trying end years least large following note few non order \\to & doing maybe x person done day start    size reason said process % already level read 0 > based someone once simply rather her life found issue times information specific type solution little check again similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line until put < c idea water god working group list state assume past under whether run matter place him last amount server great paper correct energy control itself open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year original object below higher online bytes making useful left difference current needs area user values free price future everything lens test provide away common almost results thus seem input further 10 call lower position class camera easy although name book got old target error within write comes true unless clear    words low mm full feel smaller wo available top source thought experience others big map avoid perhaps several against ask close access effect tried numbers e.g. changes tell actual created multiple difficult game three length takes larger standard 4 layer design story earth believe anyone exactly theory \\\\ days wrong terms fine history air mind hand sound language model ' #    turn along sort writing generally yes post become   taking easily nothing known money search product running gives happen per support company n planet block cases m slightly allow uses\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6929 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_train_batch_end` time: 0.0105s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6275 - accuracy: 0.7081 - val_loss: 0.6406 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6344 - accuracy: 0.6894 - val_loss: 0.6198 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6096 - accuracy: 0.7106 - val_loss: 0.6181 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6179 - accuracy: 0.6944 - val_loss: 0.6009 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6031 - accuracy: 0.6994 - val_loss: 0.5933 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6111 - accuracy: 0.6787 - val_loss: 0.5852 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5830 - accuracy: 0.7150 - val_loss: 0.5810 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5947 - accuracy: 0.6900 - val_loss: 0.5777 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5653 - accuracy: 0.7219 - val_loss: 0.5809 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5815 - accuracy: 0.6988 - val_loss: 0.5637 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5680 - accuracy: 0.7081 - val_loss: 0.5555 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5598 - accuracy: 0.7131 - val_loss: 0.5498 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5785 - accuracy: 0.6988 - val_loss: 0.5443 - val_accuracy: 0.7048\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5721 - accuracy: 0.7069 - val_loss: 0.5408 - val_accuracy: 0.7234\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5507 - accuracy: 0.7281 - val_loss: 0.5428 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5548 - accuracy: 0.7212 - val_loss: 0.5386 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5433 - accuracy: 0.7294 - val_loss: 0.5358 - val_accuracy: 0.7806\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5415 - accuracy: 0.7287 - val_loss: 0.5325 - val_accuracy: 0.7035\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5460 - accuracy: 0.7337 - val_loss: 0.5280 - val_accuracy: 0.7074\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5586 - accuracy: 0.7188 - val_loss: 0.5264 - val_accuracy: 0.7088\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5476 - accuracy: 0.7319 - val_loss: 0.5192 - val_accuracy: 0.7407\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5414 - accuracy: 0.7462 - val_loss: 0.5171 - val_accuracy: 0.7367\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5454 - accuracy: 0.7431 - val_loss: 0.5140 - val_accuracy: 0.7646\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5297 - accuracy: 0.7369 - val_loss: 0.5121 - val_accuracy: 0.7739\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5567 - accuracy: 0.7325 - val_loss: 0.5109 - val_accuracy: 0.7686\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5361 - accuracy: 0.7437 - val_loss: 0.5092 - val_accuracy: 0.7713\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5209 - accuracy: 0.7638 - val_loss: 0.5120 - val_accuracy: 0.7340\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5351 - accuracy: 0.7444 - val_loss: 0.5134 - val_accuracy: 0.7340\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5264 - accuracy: 0.7625 - val_loss: 0.5055 - val_accuracy: 0.7699\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5286 - accuracy: 0.7494 - val_loss: 0.5040 - val_accuracy: 0.7753\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.3143 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0149s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.3824 - accuracy: 0.4245\n",
      "training acc:  0.7493749856948853 , training loss:  0.5286245942115784 , val acc:  0.7752659320831299 , val loss:  0.5040264129638672 , test acc:  0.4244966506958008 , test loss:  1.3823670148849487\n",
      "\n",
      "opt2\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt2/50word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.8647 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_train_batch_end` time: 0.0104s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6579 - accuracy: 0.6956 - val_loss: 0.6352 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6398 - accuracy: 0.6925 - val_loss: 0.6321 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6199 - accuracy: 0.7156 - val_loss: 0.6324 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6289 - accuracy: 0.6975 - val_loss: 0.6301 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6340 - accuracy: 0.6894 - val_loss: 0.6220 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6194 - accuracy: 0.7069 - val_loss: 0.6231 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6217 - accuracy: 0.7013 - val_loss: 0.6167 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6207 - accuracy: 0.6963 - val_loss: 0.6126 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6145 - accuracy: 0.7013 - val_loss: 0.6104 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5880 - accuracy: 0.7300 - val_loss: 0.6171 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6039 - accuracy: 0.7094 - val_loss: 0.6046 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6173 - accuracy: 0.6881 - val_loss: 0.6074 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6107 - accuracy: 0.6925 - val_loss: 0.5976 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5944 - accuracy: 0.7088 - val_loss: 0.5946 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5864 - accuracy: 0.7138 - val_loss: 0.5905 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6055 - accuracy: 0.6837 - val_loss: 0.5935 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5968 - accuracy: 0.6938 - val_loss: 0.5874 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5869 - accuracy: 0.7031 - val_loss: 0.5798 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5811 - accuracy: 0.7081 - val_loss: 0.5825 - val_accuracy: 0.7021\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5903 - accuracy: 0.6875 - val_loss: 0.5730 - val_accuracy: 0.7021\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5636 - accuracy: 0.7237 - val_loss: 0.5702 - val_accuracy: 0.7021\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5883 - accuracy: 0.6906 - val_loss: 0.5668 - val_accuracy: 0.7021\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5655 - accuracy: 0.7131 - val_loss: 0.5647 - val_accuracy: 0.7021\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5677 - accuracy: 0.7063 - val_loss: 0.5621 - val_accuracy: 0.7021\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5732 - accuracy: 0.6944 - val_loss: 0.5591 - val_accuracy: 0.7021\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5677 - accuracy: 0.7000 - val_loss: 0.5540 - val_accuracy: 0.7021\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5667 - accuracy: 0.7044 - val_loss: 0.5535 - val_accuracy: 0.7021\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5618 - accuracy: 0.7100 - val_loss: 0.5510 - val_accuracy: 0.7021\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5617 - accuracy: 0.7000 - val_loss: 0.5564 - val_accuracy: 0.7021\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5749 - accuracy: 0.6956 - val_loss: 0.5628 - val_accuracy: 0.7939\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.9563 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0162s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.9931 - accuracy: 0.4279\n",
      "training acc:  0.6956250071525574 , training loss:  0.5749286413192749 , val acc:  0.7938829660415649 , val loss:  0.5627692341804504 , test acc:  0.4278523623943329 , test loss:  0.9931050539016724\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt2/100word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here =\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6464 - accuracy: 0.6594 - val_loss: 0.6297 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6437 - accuracy: 0.6794 - val_loss: 0.6258 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6187 - accuracy: 0.7100 - val_loss: 0.6227 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6294 - accuracy: 0.6925 - val_loss: 0.6182 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6219 - accuracy: 0.6988 - val_loss: 0.6169 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6225 - accuracy: 0.6938 - val_loss: 0.6111 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6031 - accuracy: 0.7131 - val_loss: 0.6110 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5933 - accuracy: 0.7219 - val_loss: 0.6047 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6088 - accuracy: 0.6931 - val_loss: 0.5996 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6026 - accuracy: 0.6975 - val_loss: 0.5943 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5939 - accuracy: 0.7063 - val_loss: 0.5919 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5922 - accuracy: 0.7038 - val_loss: 0.5882 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5960 - accuracy: 0.6956 - val_loss: 0.5815 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5972 - accuracy: 0.6900 - val_loss: 0.5793 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5809 - accuracy: 0.7075 - val_loss: 0.5747 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5639 - accuracy: 0.7206 - val_loss: 0.5725 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5589 - accuracy: 0.7169 - val_loss: 0.5736 - val_accuracy: 0.7101\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5651 - accuracy: 0.7038 - val_loss: 0.5586 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5686 - accuracy: 0.6969 - val_loss: 0.5596 - val_accuracy: 0.7021\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5662 - accuracy: 0.6975 - val_loss: 0.5503 - val_accuracy: 0.7021\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5539 - accuracy: 0.7038 - val_loss: 0.5451 - val_accuracy: 0.7021\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5492 - accuracy: 0.7106 - val_loss: 0.5425 - val_accuracy: 0.7101\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.5511 - accuracy: 0.7113 - val_loss: 0.5448 - val_accuracy: 0.7021\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5463 - accuracy: 0.7106 - val_loss: 0.5385 - val_accuracy: 0.7646\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5422 - accuracy: 0.7325 - val_loss: 0.5346 - val_accuracy: 0.7021\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5418 - accuracy: 0.7194 - val_loss: 0.5361 - val_accuracy: 0.7021\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5286 - accuracy: 0.7312 - val_loss: 0.5309 - val_accuracy: 0.7021\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5357 - accuracy: 0.7250 - val_loss: 0.5231 - val_accuracy: 0.7726\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5323 - accuracy: 0.7356 - val_loss: 0.5238 - val_accuracy: 0.7035\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5328 - accuracy: 0.7356 - val_loss: 0.5155 - val_accuracy: 0.7513\n",
      "  1/596 [..............................] - ETA: 0s - loss: 1.1975 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0133s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 1.2114 - accuracy: 0.4227\n",
      "training acc:  0.7356250286102295 , training loss:  0.5327926874160767 , val acc:  0.751329779624939 , val loss:  0.5154687762260437 , test acc:  0.4227139353752136 , test loss:  1.2113808393478394\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt2/150word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know most me such 1 he each something different example now question used well its 've however being good really been am after 're number case set still take since many say who problem try new things 2 those find our probably possible another between both were point had\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6604 - accuracy: 0.7063 - val_loss: 0.6445 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6415 - accuracy: 0.6975 - val_loss: 0.6363 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6200 - accuracy: 0.7156 - val_loss: 0.6325 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6295 - accuracy: 0.6988 - val_loss: 0.6254 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6374 - accuracy: 0.6812 - val_loss: 0.6219 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6372 - accuracy: 0.6775 - val_loss: 0.6182 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6331 - accuracy: 0.6775 - val_loss: 0.6137 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6123 - accuracy: 0.7019 - val_loss: 0.6074 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6109 - accuracy: 0.6981 - val_loss: 0.6027 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6060 - accuracy: 0.7013 - val_loss: 0.5993 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6096 - accuracy: 0.6906 - val_loss: 0.5940 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6042 - accuracy: 0.6919 - val_loss: 0.5902 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6011 - accuracy: 0.6931 - val_loss: 0.5853 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5971 - accuracy: 0.6963 - val_loss: 0.5812 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5918 - accuracy: 0.6906 - val_loss: 0.5769 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5810 - accuracy: 0.7013 - val_loss: 0.5726 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5735 - accuracy: 0.7100 - val_loss: 0.5687 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5723 - accuracy: 0.7019 - val_loss: 0.5634 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5723 - accuracy: 0.7000 - val_loss: 0.5604 - val_accuracy: 0.7021\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5698 - accuracy: 0.7006 - val_loss: 0.5552 - val_accuracy: 0.7021\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5557 - accuracy: 0.7150 - val_loss: 0.5514 - val_accuracy: 0.7021\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5587 - accuracy: 0.7038 - val_loss: 0.5479 - val_accuracy: 0.7021\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5454 - accuracy: 0.7237 - val_loss: 0.5522 - val_accuracy: 0.7021\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5568 - accuracy: 0.7144 - val_loss: 0.5422 - val_accuracy: 0.7021\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5496 - accuracy: 0.7113 - val_loss: 0.5380 - val_accuracy: 0.7021\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5510 - accuracy: 0.7038 - val_loss: 0.5377 - val_accuracy: 0.7699\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5528 - accuracy: 0.7194 - val_loss: 0.5303 - val_accuracy: 0.7261\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5329 - accuracy: 0.7369 - val_loss: 0.5300 - val_accuracy: 0.7660\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5472 - accuracy: 0.7237 - val_loss: 0.5315 - val_accuracy: 0.7021\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5308 - accuracy: 0.7362 - val_loss: 0.5257 - val_accuracy: 0.7035\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.2792 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0150s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 1.2334 - accuracy: 0.4125\n",
      "training acc:  0.7362499833106995 , training loss:  0.5307538509368896 , val acc:  0.7034574747085571 , val loss:  0.5256566405296326 , test acc:  0.4125419557094574 , test loss:  1.2334016561508179\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt2/200word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know most me such 1 he each something different example now question used well its 've however being good really been am after 're number case set still take since many say who problem try new things 2 those find our probably possible another between both were point had while over too answer enough his value + why did right without let go system better actually through every data going power \\mathcal back long down 'd given before 'll able code change ca own sure look image 3 create part less high thing form off function second likely best\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7098 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0081s vs `on_train_batch_end` time: 0.0239s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6372 - accuracy: 0.6831 - val_loss: 0.6301 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6182 - accuracy: 0.7119 - val_loss: 0.6259 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6202 - accuracy: 0.7081 - val_loss: 0.6228 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6210 - accuracy: 0.6963 - val_loss: 0.6221 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6107 - accuracy: 0.7119 - val_loss: 0.6167 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6201 - accuracy: 0.6944 - val_loss: 0.6121 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6191 - accuracy: 0.6900 - val_loss: 0.6055 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6033 - accuracy: 0.7056 - val_loss: 0.6005 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5985 - accuracy: 0.7069 - val_loss: 0.5958 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5931 - accuracy: 0.7113 - val_loss: 0.5917 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6001 - accuracy: 0.6925 - val_loss: 0.5864 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5836 - accuracy: 0.7131 - val_loss: 0.5826 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5899 - accuracy: 0.6900 - val_loss: 0.5766 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5868 - accuracy: 0.6944 - val_loss: 0.5726 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5736 - accuracy: 0.7088 - val_loss: 0.5665 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5613 - accuracy: 0.7150 - val_loss: 0.5606 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5630 - accuracy: 0.7044 - val_loss: 0.5556 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5637 - accuracy: 0.7063 - val_loss: 0.5507 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5547 - accuracy: 0.7069 - val_loss: 0.5460 - val_accuracy: 0.7021\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5320 - accuracy: 0.7319 - val_loss: 0.5415 - val_accuracy: 0.7021\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5498 - accuracy: 0.7156 - val_loss: 0.5383 - val_accuracy: 0.7021\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5402 - accuracy: 0.7175 - val_loss: 0.5325 - val_accuracy: 0.7061\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5580 - accuracy: 0.7163 - val_loss: 0.5342 - val_accuracy: 0.7021\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5450 - accuracy: 0.7200 - val_loss: 0.5259 - val_accuracy: 0.7114\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5274 - accuracy: 0.7206 - val_loss: 0.5219 - val_accuracy: 0.7447\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5215 - accuracy: 0.7319 - val_loss: 0.5189 - val_accuracy: 0.7660\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5386 - accuracy: 0.7456 - val_loss: 0.5155 - val_accuracy: 0.7354\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5447 - accuracy: 0.7344 - val_loss: 0.5125 - val_accuracy: 0.7646\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5216 - accuracy: 0.7506 - val_loss: 0.5097 - val_accuracy: 0.7699\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5266 - accuracy: 0.7531 - val_loss: 0.5121 - val_accuracy: 0.7194\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.3415 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0132s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 15s 24ms/step - loss: 1.2830 - accuracy: 0.4160\n",
      "training acc:  0.753125011920929 , training loss:  0.526605486869812 , val acc:  0.7194148898124695 , val loss:  0.5121486186981201 , test acc:  0.4160025119781494 , test loss:  1.2829890251159668\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt2/250word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know most me such 1 he each something different example now question used well its 've however being good really been am after 're number case set still take since many say who problem try new things 2 those find our probably possible another between both were point had while over too answer enough his value + why did right without let go system better actually through every data going power \\mathcal back long down 'd given before 'll able code change ca own sure look image 3 create part less high thing form off function second likely best either add must etc seems give having she around help lot always option made means though small us page world light keep bit space anything real url$ above trying end years least large following note few non order \\to & doing maybe x person done day start    size reason\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.8535 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0027s vs `on_train_batch_end` time: 0.0105s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6429 - accuracy: 0.7019 - val_loss: 0.6375 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6457 - accuracy: 0.6881 - val_loss: 0.6325 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6480 - accuracy: 0.6800 - val_loss: 0.6276 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6216 - accuracy: 0.7056 - val_loss: 0.6227 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6228 - accuracy: 0.6981 - val_loss: 0.6188 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6213 - accuracy: 0.6956 - val_loss: 0.6135 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6261 - accuracy: 0.6831 - val_loss: 0.6083 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6042 - accuracy: 0.7100 - val_loss: 0.6028 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5918 - accuracy: 0.7169 - val_loss: 0.5972 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5928 - accuracy: 0.7144 - val_loss: 0.5943 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5933 - accuracy: 0.7056 - val_loss: 0.5866 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5866 - accuracy: 0.7081 - val_loss: 0.5805 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5903 - accuracy: 0.6944 - val_loss: 0.5769 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5757 - accuracy: 0.7050 - val_loss: 0.5699 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5854 - accuracy: 0.6931 - val_loss: 0.5669 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5735 - accuracy: 0.7000 - val_loss: 0.5653 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5769 - accuracy: 0.6975 - val_loss: 0.5544 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5726 - accuracy: 0.6888 - val_loss: 0.5594 - val_accuracy: 0.7819\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5676 - accuracy: 0.7056 - val_loss: 0.5448 - val_accuracy: 0.7021\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5605 - accuracy: 0.7006 - val_loss: 0.5382 - val_accuracy: 0.7035\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5491 - accuracy: 0.7069 - val_loss: 0.5337 - val_accuracy: 0.7048\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5324 - accuracy: 0.7262 - val_loss: 0.5326 - val_accuracy: 0.7726\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5432 - accuracy: 0.7356 - val_loss: 0.5252 - val_accuracy: 0.7154\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5387 - accuracy: 0.7269 - val_loss: 0.5212 - val_accuracy: 0.7593\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5083 - accuracy: 0.7588 - val_loss: 0.5265 - val_accuracy: 0.7035\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5223 - accuracy: 0.7412 - val_loss: 0.5145 - val_accuracy: 0.7301\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5259 - accuracy: 0.7519 - val_loss: 0.5177 - val_accuracy: 0.8032\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5263 - accuracy: 0.7719 - val_loss: 0.5073 - val_accuracy: 0.7872\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5291 - accuracy: 0.7462 - val_loss: 0.5060 - val_accuracy: 0.7952\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5342 - accuracy: 0.7650 - val_loss: 0.5023 - val_accuracy: 0.7766\n",
      "  1/596 [..............................] - ETA: 8s - loss: 1.2723 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0158s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.2634 - accuracy: 0.4279\n",
      "training acc:  0.7649999856948853 , training loss:  0.5342023968696594 , val acc:  0.7765957713127136 , val loss:  0.5023384690284729 , test acc:  0.4278523623943329 , test loss:  1.2634224891662598\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt2/300word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know most me such 1 he each something different example now question used well its 've however being good really been am after 're number case set still take since many say who problem try new things 2 those find our probably possible another between both were point had while over too answer enough his value + why did right without let go system better actually through every data going power \\mathcal back long down 'd given before 'll able code change ca own sure look image 3 create part less high thing form off function second likely best either add must etc seems give having she around help lot always option made means though small us page world light keep bit space anything real url$ above trying end years least large following note few non order \\to & doing maybe x person done day start    size reason said process % already level read 0 > based someone once simply rather her life found issue times information specific type solution little check again similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7984 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 0.0225s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6429 - accuracy: 0.6944 - val_loss: 0.6357 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6272 - accuracy: 0.7113 - val_loss: 0.6302 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6217 - accuracy: 0.7113 - val_loss: 0.6264 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6244 - accuracy: 0.7019 - val_loss: 0.6207 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6277 - accuracy: 0.6931 - val_loss: 0.6172 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6141 - accuracy: 0.7044 - val_loss: 0.6123 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6081 - accuracy: 0.7050 - val_loss: 0.6072 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6085 - accuracy: 0.6975 - val_loss: 0.6094 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5952 - accuracy: 0.7119 - val_loss: 0.5963 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6037 - accuracy: 0.6938 - val_loss: 0.5911 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6021 - accuracy: 0.6869 - val_loss: 0.5861 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5952 - accuracy: 0.6969 - val_loss: 0.5818 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5938 - accuracy: 0.6881 - val_loss: 0.5746 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5758 - accuracy: 0.7063 - val_loss: 0.5696 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5828 - accuracy: 0.6875 - val_loss: 0.5660 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5690 - accuracy: 0.7044 - val_loss: 0.5615 - val_accuracy: 0.7035\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5561 - accuracy: 0.7163 - val_loss: 0.5529 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5741 - accuracy: 0.6862 - val_loss: 0.5534 - val_accuracy: 0.7340\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5587 - accuracy: 0.7069 - val_loss: 0.5437 - val_accuracy: 0.7021\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5447 - accuracy: 0.7144 - val_loss: 0.5405 - val_accuracy: 0.7301\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5534 - accuracy: 0.7200 - val_loss: 0.5340 - val_accuracy: 0.7074\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.5329 - accuracy: 0.7331 - val_loss: 0.5369 - val_accuracy: 0.7021\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5416 - accuracy: 0.7219 - val_loss: 0.5260 - val_accuracy: 0.7207\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5437 - accuracy: 0.7225 - val_loss: 0.5266 - val_accuracy: 0.7912\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5381 - accuracy: 0.7275 - val_loss: 0.5187 - val_accuracy: 0.7473\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5217 - accuracy: 0.7494 - val_loss: 0.5182 - val_accuracy: 0.7141\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5342 - accuracy: 0.7513 - val_loss: 0.5127 - val_accuracy: 0.7713\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5339 - accuracy: 0.7475 - val_loss: 0.5148 - val_accuracy: 0.8045\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5169 - accuracy: 0.7569 - val_loss: 0.5087 - val_accuracy: 0.7380\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5137 - accuracy: 0.7731 - val_loss: 0.5053 - val_accuracy: 0.7500\n",
      "  1/596 [..............................] - ETA: 4s - loss: 1.3167 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0168s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.2684 - accuracy: 0.4225\n",
      "training acc:  0.7731249928474426 , training loss:  0.5137324929237366 , val acc:  0.75 , val loss:  0.5052605271339417 , test acc:  0.422504186630249 , test loss:  1.2683812379837036\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt2/350word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know most me such 1 he each something different example now question used well its 've however being good really been am after 're number case set still take since many say who problem try new things 2 those find our probably possible another between both were point had while over too answer enough his value + why did right without let go system better actually through every data going power \\mathcal back long down 'd given before 'll able code change ca own sure look image 3 create part less high thing form off function second likely best either add must etc seems give having she around help lot always option made means though small us page world light keep bit space anything real url$ above trying end years least large following note few non order \\to & doing maybe x person done day start    size reason said process % already level read 0 > based someone once simply rather her life found issue times information specific type solution little check again similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line until put < c idea water god working group list state assume past under whether run matter place him last amount server great paper correct energy control itself open next method range due pretty sense i.e. called character color getting particular consider site\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 1s - loss: 0.7706 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0206s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6892 - accuracy: 0.6012 - val_loss: 0.6270 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6255 - accuracy: 0.7000 - val_loss: 0.6242 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6094 - accuracy: 0.7156 - val_loss: 0.6189 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6116 - accuracy: 0.7125 - val_loss: 0.6148 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6142 - accuracy: 0.7038 - val_loss: 0.6126 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6206 - accuracy: 0.6881 - val_loss: 0.6075 - val_accuracy: 0.7021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6214 - accuracy: 0.6787 - val_loss: 0.6008 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6067 - accuracy: 0.6938 - val_loss: 0.5977 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5865 - accuracy: 0.7169 - val_loss: 0.5955 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5922 - accuracy: 0.7063 - val_loss: 0.5861 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6025 - accuracy: 0.6856 - val_loss: 0.5814 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5865 - accuracy: 0.7000 - val_loss: 0.5771 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5653 - accuracy: 0.7219 - val_loss: 0.5722 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5695 - accuracy: 0.7113 - val_loss: 0.5663 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5847 - accuracy: 0.6894 - val_loss: 0.5659 - val_accuracy: 0.7035\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5670 - accuracy: 0.7100 - val_loss: 0.5575 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5623 - accuracy: 0.7056 - val_loss: 0.5549 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5531 - accuracy: 0.7119 - val_loss: 0.5483 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5610 - accuracy: 0.6988 - val_loss: 0.5437 - val_accuracy: 0.7035\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5513 - accuracy: 0.7069 - val_loss: 0.5421 - val_accuracy: 0.7301\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5496 - accuracy: 0.7069 - val_loss: 0.5388 - val_accuracy: 0.7513\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5370 - accuracy: 0.7306 - val_loss: 0.5360 - val_accuracy: 0.7021\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5444 - accuracy: 0.7119 - val_loss: 0.5286 - val_accuracy: 0.7301\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5467 - accuracy: 0.7350 - val_loss: 0.5325 - val_accuracy: 0.7021\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5477 - accuracy: 0.7219 - val_loss: 0.5248 - val_accuracy: 0.7739\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5413 - accuracy: 0.7344 - val_loss: 0.5204 - val_accuracy: 0.7154\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5416 - accuracy: 0.7312 - val_loss: 0.5282 - val_accuracy: 0.8045\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5406 - accuracy: 0.7369 - val_loss: 0.5140 - val_accuracy: 0.7540\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5364 - accuracy: 0.7450 - val_loss: 0.5116 - val_accuracy: 0.7513\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5341 - accuracy: 0.7387 - val_loss: 0.5120 - val_accuracy: 0.7952\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.2094 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0008s vs `on_test_batch_end` time: 0.0097s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 1.2449 - accuracy: 0.4289\n",
      "training acc:  0.7387499809265137 , training loss:  0.5340877771377563 , val acc:  0.7952127456665039 , val loss:  0.5120346546173096 , test acc:  0.4289010167121887 , test loss:  1.2448972463607788\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt2/400word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know most me such 1 he each something different example now question used well its 've however being good really been am after 're number case set still take since many say who problem try new things 2 those find our probably possible another between both were point had while over too answer enough his value + why did right without let go system better actually through every data going power \\mathcal back long down 'd given before 'll able code change ca own sure look image 3 create part less high thing form off function second likely best either add must etc seems give having she around help lot always option made means though small us page world light keep bit space anything real url$ above trying end years least large following note few non order \\to & doing maybe x person done day start    size reason said process % already level read 0 > based someone once simply rather her life found issue times information specific type solution little check again similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line until put < c idea water god working group list state assume past under whether run matter place him last amount server great paper correct energy control itself open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year original object below higher online bytes making useful left difference current needs area user values free price future everything lens test provide away common almost results thus seem input further 10 call lower position class camera easy although name book got\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6382 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0057s vs `on_train_batch_end` time: 0.0114s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6446 - accuracy: 0.7044 - val_loss: 0.6388 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6437 - accuracy: 0.6906 - val_loss: 0.6357 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6398 - accuracy: 0.6900 - val_loss: 0.6341 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6228 - accuracy: 0.7088 - val_loss: 0.6298 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6092 - accuracy: 0.7212 - val_loss: 0.6230 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6136 - accuracy: 0.7119 - val_loss: 0.6183 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6232 - accuracy: 0.6931 - val_loss: 0.6140 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6177 - accuracy: 0.6931 - val_loss: 0.6197 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6058 - accuracy: 0.7100 - val_loss: 0.6039 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6090 - accuracy: 0.6969 - val_loss: 0.5991 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6053 - accuracy: 0.6913 - val_loss: 0.5988 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5809 - accuracy: 0.7219 - val_loss: 0.5891 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5886 - accuracy: 0.7050 - val_loss: 0.5834 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5899 - accuracy: 0.7000 - val_loss: 0.5817 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5752 - accuracy: 0.7081 - val_loss: 0.5744 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5701 - accuracy: 0.7113 - val_loss: 0.5699 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5759 - accuracy: 0.7006 - val_loss: 0.5600 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5745 - accuracy: 0.6925 - val_loss: 0.5549 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5508 - accuracy: 0.7188 - val_loss: 0.5501 - val_accuracy: 0.7021\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5583 - accuracy: 0.7019 - val_loss: 0.5470 - val_accuracy: 0.7021\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5451 - accuracy: 0.7150 - val_loss: 0.5492 - val_accuracy: 0.7021\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5572 - accuracy: 0.7025 - val_loss: 0.5357 - val_accuracy: 0.7301\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5375 - accuracy: 0.7344 - val_loss: 0.5358 - val_accuracy: 0.7021\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5608 - accuracy: 0.7125 - val_loss: 0.5266 - val_accuracy: 0.7301\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5387 - accuracy: 0.7269 - val_loss: 0.5228 - val_accuracy: 0.7301\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5235 - accuracy: 0.7369 - val_loss: 0.5285 - val_accuracy: 0.7035\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5122 - accuracy: 0.7419 - val_loss: 0.5207 - val_accuracy: 0.7061\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5458 - accuracy: 0.7369 - val_loss: 0.5215 - val_accuracy: 0.8072\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5218 - accuracy: 0.7588 - val_loss: 0.5112 - val_accuracy: 0.7939\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5191 - accuracy: 0.7756 - val_loss: 0.5178 - val_accuracy: 0.7061\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.3787 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0152s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.2861 - accuracy: 0.4133\n",
      "training acc:  0.7756249904632568 , training loss:  0.5191365480422974 , val acc:  0.7061170339584351 , val loss:  0.5177730917930603 , test acc:  0.4132760167121887 , test loss:  1.2860960960388184\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt2/450word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know most me such 1 he each something different example now question used well its 've however being good really been am after 're number case set still take since many say who problem try new things 2 those find our probably possible another between both were point had while over too answer enough his value + why did right without let go system better actually through every data going power \\mathcal back long down 'd given before 'll able code change ca own sure look image 3 create part less high thing form off function second likely best either add must etc seems give having she around help lot always option made means though small us page world light keep bit space anything real url$ above trying end years least large following note few non order \\to & doing maybe x person done day start    size reason said process % already level read 0 > based someone once simply rather her life found issue times information specific type solution little check again similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line until put < c idea water god working group list state assume past under whether run matter place him last amount server great paper correct energy control itself open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year original object below higher online bytes making useful left difference current needs area user values free price future everything lens test provide away common almost results thus seem input further 10 call lower position class camera easy although name book got old target error within write comes true unless clear    words low mm full feel smaller wo available top source thought experience others big map avoid perhaps several against ask close access effect tried numbers e.g. changes tell actual created multiple difficult game three length takes larger standard 4 layer\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6377 - accuracy: 0.7038 - val_loss: 0.6326 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6234 - accuracy: 0.7106 - val_loss: 0.6294 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6183 - accuracy: 0.7113 - val_loss: 0.6238 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6350 - accuracy: 0.6881 - val_loss: 0.6215 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6131 - accuracy: 0.7081 - val_loss: 0.6153 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6147 - accuracy: 0.7000 - val_loss: 0.6109 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5986 - accuracy: 0.7206 - val_loss: 0.6054 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6042 - accuracy: 0.7063 - val_loss: 0.6006 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6045 - accuracy: 0.7013 - val_loss: 0.5958 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5979 - accuracy: 0.7019 - val_loss: 0.5936 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5999 - accuracy: 0.6913 - val_loss: 0.5856 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6008 - accuracy: 0.6806 - val_loss: 0.5810 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5960 - accuracy: 0.6819 - val_loss: 0.5752 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5723 - accuracy: 0.7125 - val_loss: 0.5694 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5743 - accuracy: 0.7050 - val_loss: 0.5675 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5596 - accuracy: 0.7181 - val_loss: 0.5593 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5655 - accuracy: 0.7081 - val_loss: 0.5594 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5660 - accuracy: 0.7013 - val_loss: 0.5510 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5523 - accuracy: 0.7119 - val_loss: 0.5535 - val_accuracy: 0.7021\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5580 - accuracy: 0.7106 - val_loss: 0.5417 - val_accuracy: 0.7021\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5482 - accuracy: 0.7069 - val_loss: 0.5359 - val_accuracy: 0.7048\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5334 - accuracy: 0.7381 - val_loss: 0.5333 - val_accuracy: 0.7035\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5312 - accuracy: 0.7381 - val_loss: 0.5272 - val_accuracy: 0.7301\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5237 - accuracy: 0.7456 - val_loss: 0.5264 - val_accuracy: 0.7035\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5330 - accuracy: 0.7300 - val_loss: 0.5198 - val_accuracy: 0.7287\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5328 - accuracy: 0.7437 - val_loss: 0.5175 - val_accuracy: 0.7779\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5339 - accuracy: 0.7475 - val_loss: 0.5131 - val_accuracy: 0.7487\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5262 - accuracy: 0.7519 - val_loss: 0.5109 - val_accuracy: 0.7420\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5331 - accuracy: 0.7563 - val_loss: 0.5089 - val_accuracy: 0.7979\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5278 - accuracy: 0.7563 - val_loss: 0.5050 - val_accuracy: 0.7899\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.1878 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0140s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.1963 - accuracy: 0.4288\n",
      "training acc:  0.7562500238418579 , training loss:  0.527827799320221 , val acc:  0.789893627166748 , val loss:  0.5050193667411804 , test acc:  0.4287961423397064 , test loss:  1.196304440498352\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt2/500word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that i you it $ for be this with   as are on have if not or can but would 's your an do n't so from will at they by there we one which what my more all some when was then like has / use about also any could time just only no get other than how them   out up need does their should make way where - same even work because very using want people think 'm these into first two see may might here = much know most me such 1 he each something different example now question used well its 've however being good really been am after 're number case set still take since many say who problem try new things 2 those find our probably possible another between both were point had while over too answer enough his value + why did right without let go system better actually through every data going power \\mathcal back long down 'd given before 'll able code change ca own sure look image 3 create part less high thing form off function second likely best either add must etc seems give having she around help lot always option made means though small us page world light keep bit space anything real url$ above trying end years least large following note few non order \\to & doing maybe x person done day start    size reason said process % already level read 0 > based someone once simply rather her life found issue times information specific type solution little check again similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line until put < c idea water god working group list state assume past under whether run matter place him last amount server great paper correct energy control itself open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year original object below higher online bytes making useful left difference current needs area user values free price future everything lens test provide away common almost results thus seem input further 10 call lower position class camera easy although name book got old target error within write comes true unless clear    words low mm full feel smaller wo available top source thought experience others big map avoid perhaps several against ask close access effect tried numbers e.g. changes tell actual created multiple difficult game three length takes larger standard 4 layer design story earth believe anyone exactly theory \\\\ days wrong terms fine history air mind hand sound language model ' #    turn along sort writing generally yes post become   taking easily nothing known money search product running gives happen per support company n planet block cases m slightly\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.5656 - accuracy: 0.8750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0225s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6274 - accuracy: 0.7125 - val_loss: 0.6326 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6305 - accuracy: 0.7019 - val_loss: 0.6267 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6188 - accuracy: 0.7094 - val_loss: 0.6266 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6154 - accuracy: 0.7094 - val_loss: 0.6152 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6203 - accuracy: 0.6956 - val_loss: 0.6111 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6164 - accuracy: 0.6988 - val_loss: 0.6052 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6096 - accuracy: 0.6969 - val_loss: 0.5989 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6134 - accuracy: 0.6906 - val_loss: 0.5948 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5940 - accuracy: 0.7038 - val_loss: 0.5880 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6009 - accuracy: 0.6875 - val_loss: 0.5831 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5754 - accuracy: 0.7175 - val_loss: 0.5765 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6017 - accuracy: 0.6762 - val_loss: 0.5752 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5689 - accuracy: 0.7138 - val_loss: 0.5679 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5736 - accuracy: 0.7044 - val_loss: 0.5611 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5659 - accuracy: 0.7081 - val_loss: 0.5555 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5646 - accuracy: 0.6975 - val_loss: 0.5503 - val_accuracy: 0.7035\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5612 - accuracy: 0.7094 - val_loss: 0.5455 - val_accuracy: 0.7035\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5579 - accuracy: 0.7212 - val_loss: 0.5501 - val_accuracy: 0.7846\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5576 - accuracy: 0.7175 - val_loss: 0.5359 - val_accuracy: 0.7207\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5496 - accuracy: 0.7169 - val_loss: 0.5314 - val_accuracy: 0.7061\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5391 - accuracy: 0.7319 - val_loss: 0.5269 - val_accuracy: 0.7128\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5500 - accuracy: 0.7306 - val_loss: 0.5285 - val_accuracy: 0.7035\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5415 - accuracy: 0.7306 - val_loss: 0.5191 - val_accuracy: 0.7314\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5250 - accuracy: 0.7588 - val_loss: 0.5201 - val_accuracy: 0.7074\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5166 - accuracy: 0.7456 - val_loss: 0.5131 - val_accuracy: 0.7314\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5352 - accuracy: 0.7581 - val_loss: 0.5082 - val_accuracy: 0.7633\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5245 - accuracy: 0.7575 - val_loss: 0.5054 - val_accuracy: 0.7793\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5158 - accuracy: 0.7531 - val_loss: 0.5043 - val_accuracy: 0.8032\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5338 - accuracy: 0.7588 - val_loss: 0.5017 - val_accuracy: 0.7620\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5243 - accuracy: 0.7700 - val_loss: 0.5078 - val_accuracy: 0.7301\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.4459 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0152s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.3894 - accuracy: 0.4177\n",
      "training acc:  0.7699999809265137 , training loss:  0.5243461728096008 , val acc:  0.730053186416626 , val loss:  0.5078485012054443 , test acc:  0.41768038272857666 , test loss:  1.3893721103668213\n",
      "\n",
      "opt3\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt3/50word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.8838 - accuracy: 0.2812WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0341s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6962 - accuracy: 0.6006 - val_loss: 0.6297 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6193 - accuracy: 0.7000 - val_loss: 0.6055 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5965 - accuracy: 0.7088 - val_loss: 0.5866 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 0.5739 - accuracy: 0.7138 - val_loss: 0.5721 - val_accuracy: 0.7061\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5922 - accuracy: 0.6762 - val_loss: 0.5636 - val_accuracy: 0.7035\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5627 - accuracy: 0.7050 - val_loss: 0.5590 - val_accuracy: 0.7074\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5725 - accuracy: 0.6806 - val_loss: 0.5545 - val_accuracy: 0.7314\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5479 - accuracy: 0.7200 - val_loss: 0.5549 - val_accuracy: 0.7048\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5578 - accuracy: 0.7044 - val_loss: 0.5541 - val_accuracy: 0.7074\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5653 - accuracy: 0.6881 - val_loss: 0.5474 - val_accuracy: 0.7287\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.5711 - accuracy: 0.6963 - val_loss: 0.5461 - val_accuracy: 0.7274\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5741 - accuracy: 0.6769 - val_loss: 0.5525 - val_accuracy: 0.7340\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5614 - accuracy: 0.7031 - val_loss: 0.5501 - val_accuracy: 0.7035\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5524 - accuracy: 0.6988 - val_loss: 0.5498 - val_accuracy: 0.7380\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.1476 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_test_batch_end` time: 0.0163s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9143 - accuracy: 0.4608\n",
      "training acc:  0.6987500190734863 , training loss:  0.5524234175682068 , val acc:  0.7380319237709045 , val loss:  0.5498373508453369 , test acc:  0.46078020334243774 , test loss:  0.9143484234809875\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt3/100word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6574 - accuracy: 0.6994 - val_loss: 0.6374 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6249 - accuracy: 0.6981 - val_loss: 0.6050 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5929 - accuracy: 0.7113 - val_loss: 0.5835 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.5839 - accuracy: 0.7113 - val_loss: 0.5702 - val_accuracy: 0.7234\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5806 - accuracy: 0.6837 - val_loss: 0.5614 - val_accuracy: 0.7154\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5810 - accuracy: 0.6913 - val_loss: 0.5553 - val_accuracy: 0.7221\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5847 - accuracy: 0.6800 - val_loss: 0.5532 - val_accuracy: 0.7301\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5695 - accuracy: 0.6981 - val_loss: 0.5508 - val_accuracy: 0.7128\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5652 - accuracy: 0.6963 - val_loss: 0.5461 - val_accuracy: 0.7221\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5657 - accuracy: 0.6956 - val_loss: 0.5506 - val_accuracy: 0.7035\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5637 - accuracy: 0.7150 - val_loss: 0.5447 - val_accuracy: 0.7274\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5574 - accuracy: 0.7088 - val_loss: 0.5447 - val_accuracy: 0.7234\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5598 - accuracy: 0.7006 - val_loss: 0.5439 - val_accuracy: 0.7274\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5601 - accuracy: 0.7106 - val_loss: 0.5449 - val_accuracy: 0.7168\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5576 - accuracy: 0.7144 - val_loss: 0.5471 - val_accuracy: 0.7128\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5707 - accuracy: 0.6837 - val_loss: 0.5413 - val_accuracy: 0.7247\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5651 - accuracy: 0.6931 - val_loss: 0.5419 - val_accuracy: 0.7367\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5566 - accuracy: 0.7069 - val_loss: 0.5455 - val_accuracy: 0.7128\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5741 - accuracy: 0.6888 - val_loss: 0.5402 - val_accuracy: 0.7274\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5606 - accuracy: 0.7050 - val_loss: 0.5407 - val_accuracy: 0.7247\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5550 - accuracy: 0.6994 - val_loss: 0.5476 - val_accuracy: 0.7048\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5784 - accuracy: 0.6769 - val_loss: 0.5448 - val_accuracy: 0.7128\n",
      "  1/596 [..............................] - ETA: 4s - loss: 1.3542 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_test_batch_end` time: 0.0130s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9900 - accuracy: 0.4237\n",
      "training acc:  0.6768749952316284 , training loss:  0.5783748626708984 , val acc:  0.7127659320831299 , val loss:  0.5448168516159058 , test acc:  0.42365771532058716 , test loss:  0.9900408983230591\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt3/150word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function second likely best either add must etc seems give around help lot always option made means though small us page world light keep bit space anything real url$ trying end years least large following note non order \\to & maybe x person done day start    size reason said process\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7264 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0027s vs `on_train_batch_end` time: 0.0104s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6635 - accuracy: 0.7038 - val_loss: 0.6491 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6220 - accuracy: 0.7131 - val_loss: 0.6102 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5949 - accuracy: 0.7119 - val_loss: 0.5877 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5816 - accuracy: 0.7013 - val_loss: 0.5711 - val_accuracy: 0.7048\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5749 - accuracy: 0.7150 - val_loss: 0.5613 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5828 - accuracy: 0.6862 - val_loss: 0.5532 - val_accuracy: 0.7247\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5671 - accuracy: 0.7081 - val_loss: 0.5505 - val_accuracy: 0.7487\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5860 - accuracy: 0.6744 - val_loss: 0.5496 - val_accuracy: 0.7141\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5781 - accuracy: 0.7025 - val_loss: 0.5462 - val_accuracy: 0.7301\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5536 - accuracy: 0.6944 - val_loss: 0.5554 - val_accuracy: 0.7048\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5624 - accuracy: 0.7000 - val_loss: 0.5420 - val_accuracy: 0.7314\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5452 - accuracy: 0.7188 - val_loss: 0.5404 - val_accuracy: 0.7234\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5561 - accuracy: 0.7006 - val_loss: 0.5409 - val_accuracy: 0.7261\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5735 - accuracy: 0.6950 - val_loss: 0.5400 - val_accuracy: 0.7234\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5679 - accuracy: 0.6938 - val_loss: 0.5399 - val_accuracy: 0.7261\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5801 - accuracy: 0.6756 - val_loss: 0.5400 - val_accuracy: 0.7394\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5829 - accuracy: 0.6750 - val_loss: 0.5427 - val_accuracy: 0.7487\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5623 - accuracy: 0.7031 - val_loss: 0.5395 - val_accuracy: 0.7394\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5422 - accuracy: 0.7138 - val_loss: 0.5397 - val_accuracy: 0.7261\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.5471 - accuracy: 0.7200 - val_loss: 0.5371 - val_accuracy: 0.7327\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5802 - accuracy: 0.6869 - val_loss: 0.5381 - val_accuracy: 0.7287\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.5623 - accuracy: 0.7025 - val_loss: 0.5424 - val_accuracy: 0.7168\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5540 - accuracy: 0.7013 - val_loss: 0.5384 - val_accuracy: 0.7460\n",
      "  1/596 [..............................] - ETA: 4s - loss: 1.2684 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0128s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9386 - accuracy: 0.4459\n",
      "training acc:  0.7012500166893005 , training loss:  0.5540304780006409 , val acc:  0.7460106611251831 , val loss:  0.5383960008621216 , test acc:  0.44588926434516907 , test loss:  0.9386137127876282\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt3/200word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function second likely best either add must etc seems give around help lot always option made means though small us page world light keep bit space anything real url$ trying end years least large following note non order \\to & maybe x person done day start    size reason said process % already level read 0 > based someone simply rather life found issue times information specific type solution little check similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6677 - accuracy: 0.6388 - val_loss: 0.6327 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6131 - accuracy: 0.7113 - val_loss: 0.5988 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5997 - accuracy: 0.7038 - val_loss: 0.5793 - val_accuracy: 0.7168\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5775 - accuracy: 0.7031 - val_loss: 0.5644 - val_accuracy: 0.7061\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5703 - accuracy: 0.6913 - val_loss: 0.5554 - val_accuracy: 0.7154\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5786 - accuracy: 0.6806 - val_loss: 0.5541 - val_accuracy: 0.7088\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5717 - accuracy: 0.6881 - val_loss: 0.5549 - val_accuracy: 0.7008\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5905 - accuracy: 0.6794 - val_loss: 0.5569 - val_accuracy: 0.7048\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5705 - accuracy: 0.6931 - val_loss: 0.5452 - val_accuracy: 0.7301\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5549 - accuracy: 0.7075 - val_loss: 0.5440 - val_accuracy: 0.7234\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5606 - accuracy: 0.6944 - val_loss: 0.5427 - val_accuracy: 0.7234\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5546 - accuracy: 0.7025 - val_loss: 0.5419 - val_accuracy: 0.7234\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5602 - accuracy: 0.7125 - val_loss: 0.5389 - val_accuracy: 0.7274\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5725 - accuracy: 0.7088 - val_loss: 0.5487 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5547 - accuracy: 0.6969 - val_loss: 0.5385 - val_accuracy: 0.7460\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5534 - accuracy: 0.7287 - val_loss: 0.5473 - val_accuracy: 0.7580\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5636 - accuracy: 0.7025 - val_loss: 0.5410 - val_accuracy: 0.7221\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5359 - accuracy: 0.7269 - val_loss: 0.5364 - val_accuracy: 0.7407\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5770 - accuracy: 0.6925 - val_loss: 0.5374 - val_accuracy: 0.7261\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5634 - accuracy: 0.6994 - val_loss: 0.5370 - val_accuracy: 0.7287\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5762 - accuracy: 0.6862 - val_loss: 0.5487 - val_accuracy: 0.7021\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.3404 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0150s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.0002 - accuracy: 0.4164\n",
      "training acc:  0.6862499713897705 , training loss:  0.5761660933494568 , val acc:  0.7021276354789734 , val loss:  0.5486764907836914 , test acc:  0.4164219796657562 , test loss:  1.000187873840332\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt3/250word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function second likely best either add must etc seems give around help lot always option made means though small us page world light keep bit space anything real url$ trying end years least large following note non order \\to & maybe x person done day start    size reason said process % already level read 0 > based someone simply rather life found issue times information specific type solution little check similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line put < c idea water god working group list state assume past whether run matter place last amount server great paper correct energy control open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6882 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0116s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6476 - accuracy: 0.7138 - val_loss: 0.6260 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6061 - accuracy: 0.7156 - val_loss: 0.5940 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6009 - accuracy: 0.6837 - val_loss: 0.5740 - val_accuracy: 0.7234\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.5683 - accuracy: 0.7169 - val_loss: 0.5638 - val_accuracy: 0.6968\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5859 - accuracy: 0.6925 - val_loss: 0.5533 - val_accuracy: 0.7314\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5814 - accuracy: 0.6706 - val_loss: 0.5499 - val_accuracy: 0.7287\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5516 - accuracy: 0.7269 - val_loss: 0.5443 - val_accuracy: 0.7287\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5813 - accuracy: 0.6875 - val_loss: 0.5466 - val_accuracy: 0.7141\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5627 - accuracy: 0.7069 - val_loss: 0.5421 - val_accuracy: 0.7540\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5705 - accuracy: 0.6981 - val_loss: 0.5398 - val_accuracy: 0.7314\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5551 - accuracy: 0.7125 - val_loss: 0.5476 - val_accuracy: 0.7114\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5671 - accuracy: 0.7019 - val_loss: 0.5425 - val_accuracy: 0.7633\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5814 - accuracy: 0.6794 - val_loss: 0.5394 - val_accuracy: 0.7301\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5635 - accuracy: 0.7056 - val_loss: 0.5454 - val_accuracy: 0.7141\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5637 - accuracy: 0.6956 - val_loss: 0.5470 - val_accuracy: 0.7114\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5762 - accuracy: 0.6787 - val_loss: 0.5379 - val_accuracy: 0.7314\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5505 - accuracy: 0.7163 - val_loss: 0.5381 - val_accuracy: 0.7314\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5608 - accuracy: 0.7031 - val_loss: 0.5362 - val_accuracy: 0.7340\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5632 - accuracy: 0.7019 - val_loss: 0.5378 - val_accuracy: 0.7540\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5501 - accuracy: 0.7175 - val_loss: 0.5524 - val_accuracy: 0.7021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5715 - accuracy: 0.7000 - val_loss: 0.5382 - val_accuracy: 0.7287\n",
      "  1/596 [..............................] - ETA: 0s - loss: 1.3017 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_test_batch_end` time: 0.0227s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9723 - accuracy: 0.4265\n",
      "training acc:  0.699999988079071 , training loss:  0.5715382099151611 , val acc:  0.728723406791687 , val loss:  0.5382450222969055 , test acc:  0.42648908495903015 , test loss:  0.9723100066184998\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt3/300word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function second likely best either add must etc seems give around help lot always option made means though small us page world light keep bit space anything real url$ trying end years least large following note non order \\to & maybe x person done day start    size reason said process % already level read 0 > based someone simply rather life found issue times information specific type solution little check similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line put < c idea water god working group list state assume past whether run matter place last amount server great paper correct energy control open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year original object higher online bytes making useful left difference current needs area user values free price future everything lens test provide away common almost results thus seem input 10 call lower position class camera easy although name book got old target error within write comes true unless clear    words\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7048 - accuracy: 0.0625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.0104s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6503 - accuracy: 0.7019 - val_loss: 0.6269 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6140 - accuracy: 0.6981 - val_loss: 0.5896 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5825 - accuracy: 0.7119 - val_loss: 0.5704 - val_accuracy: 0.7287\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5778 - accuracy: 0.6938 - val_loss: 0.5593 - val_accuracy: 0.6995\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5665 - accuracy: 0.6894 - val_loss: 0.5491 - val_accuracy: 0.7274\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5678 - accuracy: 0.6888 - val_loss: 0.5486 - val_accuracy: 0.7141\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5655 - accuracy: 0.6950 - val_loss: 0.5435 - val_accuracy: 0.7274\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5599 - accuracy: 0.7125 - val_loss: 0.5495 - val_accuracy: 0.7048\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5745 - accuracy: 0.6925 - val_loss: 0.5479 - val_accuracy: 0.7088\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5766 - accuracy: 0.6850 - val_loss: 0.5411 - val_accuracy: 0.7287\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5649 - accuracy: 0.7050 - val_loss: 0.5416 - val_accuracy: 0.7247\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5777 - accuracy: 0.6794 - val_loss: 0.5402 - val_accuracy: 0.7340\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5660 - accuracy: 0.6988 - val_loss: 0.5428 - val_accuracy: 0.7141\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5640 - accuracy: 0.7056 - val_loss: 0.5408 - val_accuracy: 0.7447\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5488 - accuracy: 0.7075 - val_loss: 0.5508 - val_accuracy: 0.7021\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.3623 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0149s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 1.0237 - accuracy: 0.4162\n",
      "training acc:  0.7074999809265137 , training loss:  0.5488497018814087 , val acc:  0.7021276354789734 , val loss:  0.550794243812561 , test acc:  0.416212260723114 , test loss:  1.023680567741394\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt3/350word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function second likely best either add must etc seems give around help lot always option made means though small us page world light keep bit space anything real url$ trying end years least large following note non order \\to & maybe x person done day start    size reason said process % already level read 0 > based someone simply rather life found issue times information specific type solution little check similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line put < c idea water god working group list state assume past whether run matter place last amount server great paper correct energy control open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year original object higher online bytes making useful left difference current needs area user values free price future everything lens test provide away common almost results thus seem input 10 call lower position class camera easy although name book got old target error within write comes true unless clear    words low mm full feel smaller wo available top source thought experience others big map avoid perhaps several ask close access effect tried numbers e.g. changes tell actual created multiple difficult game three length takes larger standard 4 layer design story earth believe anyone exactly theory \\\\ days wrong terms fine\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6601 - accuracy: 0.6963 - val_loss: 0.6416 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6362 - accuracy: 0.6800 - val_loss: 0.6109 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6058 - accuracy: 0.7000 - val_loss: 0.5912 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5881 - accuracy: 0.7044 - val_loss: 0.5741 - val_accuracy: 0.6995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5785 - accuracy: 0.6900 - val_loss: 0.5637 - val_accuracy: 0.7261\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5694 - accuracy: 0.7094 - val_loss: 0.5564 - val_accuracy: 0.7128\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5794 - accuracy: 0.6862 - val_loss: 0.5532 - val_accuracy: 0.7154\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5704 - accuracy: 0.6981 - val_loss: 0.5489 - val_accuracy: 0.7261\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5765 - accuracy: 0.6950 - val_loss: 0.5468 - val_accuracy: 0.7274\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5551 - accuracy: 0.7050 - val_loss: 0.5473 - val_accuracy: 0.7141\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5520 - accuracy: 0.7113 - val_loss: 0.5480 - val_accuracy: 0.7128\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5667 - accuracy: 0.6881 - val_loss: 0.5424 - val_accuracy: 0.7367\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5648 - accuracy: 0.7025 - val_loss: 0.5477 - val_accuracy: 0.7114\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5666 - accuracy: 0.6894 - val_loss: 0.5415 - val_accuracy: 0.7274\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5646 - accuracy: 0.6938 - val_loss: 0.5416 - val_accuracy: 0.7261\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5711 - accuracy: 0.6931 - val_loss: 0.5404 - val_accuracy: 0.7274\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5848 - accuracy: 0.6675 - val_loss: 0.5462 - val_accuracy: 0.7128\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5521 - accuracy: 0.7013 - val_loss: 0.5399 - val_accuracy: 0.7274\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5794 - accuracy: 0.6781 - val_loss: 0.5401 - val_accuracy: 0.7261\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5643 - accuracy: 0.6956 - val_loss: 0.5400 - val_accuracy: 0.7261\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5509 - accuracy: 0.7106 - val_loss: 0.5396 - val_accuracy: 0.7274\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5510 - accuracy: 0.7056 - val_loss: 0.5394 - val_accuracy: 0.7274\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5768 - accuracy: 0.6919 - val_loss: 0.5384 - val_accuracy: 0.7261\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5869 - accuracy: 0.6794 - val_loss: 0.5401 - val_accuracy: 0.7274\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5726 - accuracy: 0.6938 - val_loss: 0.5428 - val_accuracy: 0.7128\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5709 - accuracy: 0.6850 - val_loss: 0.5399 - val_accuracy: 0.7274\n",
      "  1/596 [..............................] - ETA: 8s - loss: 1.2441 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0135s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9501 - accuracy: 0.4311\n",
      "training acc:  0.6850000023841858 , training loss:  0.5709019303321838 , val acc:  0.727393627166748 , val loss:  0.539890468120575 , test acc:  0.4311031997203827 , test loss:  0.9501169919967651\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt3/400word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function second likely best either add must etc seems give around help lot always option made means though small us page world light keep bit space anything real url$ trying end years least large following note non order \\to & maybe x person done day start    size reason said process % already level read 0 > based someone simply rather life found issue times information specific type solution little check similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line put < c idea water god working group list state assume past whether run matter place last amount server great paper correct energy control open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year original object higher online bytes making useful left difference current needs area user values free price future everything lens test provide away common almost results thus seem input 10 call lower position class camera easy although name book got old target error within write comes true unless clear    words low mm full feel smaller wo available top source thought experience others big map avoid perhaps several ask close access effect tried numbers e.g. changes tell actual created multiple difficult game three length takes larger standard 4 layer design story earth believe anyone exactly theory \\\\ days wrong terms fine history air mind hand sound language model ' #    turn along sort writing generally yes post become   taking easily nothing known money search product running gives happen per support company n planet block cases slightly allow uses ones 100 children necessary step bad    whole certain sometimes related\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6595 - accuracy: 0.7063 - val_loss: 0.6413 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6230 - accuracy: 0.7006 - val_loss: 0.6037 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5815 - accuracy: 0.7225 - val_loss: 0.5760 - val_accuracy: 0.7008\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5939 - accuracy: 0.6806 - val_loss: 0.5637 - val_accuracy: 0.7168\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5852 - accuracy: 0.6800 - val_loss: 0.5563 - val_accuracy: 0.7128\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5574 - accuracy: 0.7113 - val_loss: 0.5538 - val_accuracy: 0.7048\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5861 - accuracy: 0.6812 - val_loss: 0.5482 - val_accuracy: 0.7207\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5717 - accuracy: 0.6925 - val_loss: 0.5450 - val_accuracy: 0.7287\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5646 - accuracy: 0.6869 - val_loss: 0.5527 - val_accuracy: 0.7008\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5623 - accuracy: 0.6913 - val_loss: 0.5552 - val_accuracy: 0.7008\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5690 - accuracy: 0.6906 - val_loss: 0.5446 - val_accuracy: 0.7207\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5743 - accuracy: 0.6819 - val_loss: 0.5427 - val_accuracy: 0.7287\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5699 - accuracy: 0.6931 - val_loss: 0.5477 - val_accuracy: 0.7074\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5805 - accuracy: 0.6775 - val_loss: 0.5416 - val_accuracy: 0.7314\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5700 - accuracy: 0.6969 - val_loss: 0.5436 - val_accuracy: 0.7181\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5688 - accuracy: 0.6919 - val_loss: 0.5419 - val_accuracy: 0.7247\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5656 - accuracy: 0.6994 - val_loss: 0.5419 - val_accuracy: 0.7247\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9629 - accuracy: 0.4251\n",
      "training acc:  0.6993749737739563 , training loss:  0.5656219124794006 , val acc:  0.7247340679168701 , val loss:  0.5419448018074036 , test acc:  0.4251258373260498 , test loss:  0.9629129767417908\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt3/450word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function second likely best either add must etc seems give around help lot always option made means though small us page world light keep bit space anything real url$ trying end years least large following note non order \\to & maybe x person done day start    size reason said process % already level read 0 > based someone simply rather life found issue times information specific type solution little check similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line put < c idea water god working group list state assume past whether run matter place last amount server great paper correct energy control open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year original object higher online bytes making useful left difference current needs area user values free price future everything lens test provide away common almost results thus seem input 10 call lower position class camera easy although name book got old target error within write comes true unless clear    words low mm full feel smaller wo available top source thought experience others big map avoid perhaps several ask close access effect tried numbers e.g. changes tell actual created multiple difficult game three length takes larger standard 4 layer design story earth believe anyone exactly theory \\\\ days wrong terms fine history air mind hand sound language model ' #    turn along sort writing generally yes post become   taking easily nothing known money search product running gives happen per support company n planet block cases slightly allow uses ones 100 children necessary step bad    whole certain sometimes related gets research says return humans build knowledge setting context cause 6 normal computer seen \\ situation main move food men characters n$ functions english inside body problems structure examples added choose rules play job ability thanks points therefore depends mode limit content meaning reading exact ground view flash major risk\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6494 - accuracy: 0.6988 - val_loss: 0.6228 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6128 - accuracy: 0.6969 - val_loss: 0.5916 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5951 - accuracy: 0.6875 - val_loss: 0.5749 - val_accuracy: 0.7074\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5822 - accuracy: 0.6862 - val_loss: 0.5613 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5862 - accuracy: 0.6781 - val_loss: 0.5532 - val_accuracy: 0.7287\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5590 - accuracy: 0.7044 - val_loss: 0.5539 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5705 - accuracy: 0.6919 - val_loss: 0.5459 - val_accuracy: 0.7261\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5636 - accuracy: 0.7000 - val_loss: 0.5478 - val_accuracy: 0.7128\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5697 - accuracy: 0.6862 - val_loss: 0.5436 - val_accuracy: 0.7301\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5771 - accuracy: 0.6831 - val_loss: 0.5469 - val_accuracy: 0.7154\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5712 - accuracy: 0.6900 - val_loss: 0.5425 - val_accuracy: 0.7287\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5588 - accuracy: 0.7000 - val_loss: 0.5431 - val_accuracy: 0.7234\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5827 - accuracy: 0.6775 - val_loss: 0.5434 - val_accuracy: 0.7380\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5764 - accuracy: 0.6881 - val_loss: 0.5429 - val_accuracy: 0.7274\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.1881 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0140s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9311 - accuracy: 0.4279\n",
      "training acc:  0.6881250143051147 , training loss:  0.5763761401176453 , val acc:  0.727393627166748 , val loss:  0.5429290533065796 , test acc:  0.4278523623943329 , test loss:  0.9311444759368896\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt3/500word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function second likely best either add must etc seems give around help lot always option made means though small us page world light keep bit space anything real url$ trying end years least large following note non order \\to & maybe x person done day start    size reason said process % already level read 0 > based someone simply rather life found issue times information specific type solution little check similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line put < c idea water god working group list state assume past whether run matter place last amount server great paper correct energy control open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year original object higher online bytes making useful left difference current needs area user values free price future everything lens test provide away common almost results thus seem input 10 call lower position class camera easy although name book got old target error within write comes true unless clear    words low mm full feel smaller wo available top source thought experience others big map avoid perhaps several ask close access effect tried numbers e.g. changes tell actual created multiple difficult game three length takes larger standard 4 layer design story earth believe anyone exactly theory \\\\ days wrong terms fine history air mind hand sound language model ' #    turn along sort writing generally yes post become   taking easily nothing known money search product running gives happen per support company n planet block cases slightly allow uses ones 100 children necessary step bad    whole certain sometimes related gets research says return humans build knowledge setting context cause 6 normal computer seen \\ situation main move food men characters n$ functions english inside body problems structure examples added choose rules play job ability thanks points therefore depends mode limit content meaning reading exact ground view flash major risk approach account force network assuming whatever solve fire total half outside stop directly conditions link google reasons noise cost update head perfect magic pressure yet options remember require distance needed rate save looks weapons completely zero wine address select background worth e algorithm software strong easier goes equation otherwise questions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6550 - accuracy: 0.6931 - val_loss: 0.6314 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6056 - accuracy: 0.7150 - val_loss: 0.5962 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5974 - accuracy: 0.6794 - val_loss: 0.5813 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5941 - accuracy: 0.6737 - val_loss: 0.5623 - val_accuracy: 0.7141\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5739 - accuracy: 0.6913 - val_loss: 0.5564 - val_accuracy: 0.7101\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5721 - accuracy: 0.6956 - val_loss: 0.5510 - val_accuracy: 0.7234\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5527 - accuracy: 0.7156 - val_loss: 0.5562 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5714 - accuracy: 0.6850 - val_loss: 0.5454 - val_accuracy: 0.7274\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5739 - accuracy: 0.6837 - val_loss: 0.5441 - val_accuracy: 0.7314\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5663 - accuracy: 0.6956 - val_loss: 0.5472 - val_accuracy: 0.7154\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5724 - accuracy: 0.6906 - val_loss: 0.5453 - val_accuracy: 0.7261\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5763 - accuracy: 0.6825 - val_loss: 0.5438 - val_accuracy: 0.7247\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5805 - accuracy: 0.6737 - val_loss: 0.5438 - val_accuracy: 0.7301\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5476 - accuracy: 0.7312 - val_loss: 0.5404 - val_accuracy: 0.7327\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5762 - accuracy: 0.6963 - val_loss: 0.5402 - val_accuracy: 0.7314\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5827 - accuracy: 0.6900 - val_loss: 0.5420 - val_accuracy: 0.7274\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5567 - accuracy: 0.7006 - val_loss: 0.5432 - val_accuracy: 0.7247\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5751 - accuracy: 0.6875 - val_loss: 0.5436 - val_accuracy: 0.7261\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.2379 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0140s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9642 - accuracy: 0.4230\n",
      "training acc:  0.6875 , training loss:  0.5750625729560852 , val acc:  0.7260638475418091 , val loss:  0.543637216091156 , test acc:  0.42302852869033813 , test loss:  0.9641681909561157\n",
      "\n",
      "opt4\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt4/50word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6587 - accuracy: 0.6544 - val_loss: 0.6283 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6127 - accuracy: 0.7175 - val_loss: 0.6225 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6116 - accuracy: 0.7138 - val_loss: 0.6198 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6283 - accuracy: 0.6919 - val_loss: 0.6138 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6181 - accuracy: 0.6944 - val_loss: 0.6078 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6048 - accuracy: 0.7056 - val_loss: 0.6053 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6145 - accuracy: 0.6913 - val_loss: 0.6012 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6130 - accuracy: 0.6844 - val_loss: 0.5939 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5806 - accuracy: 0.7231 - val_loss: 0.5918 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5926 - accuracy: 0.7038 - val_loss: 0.5862 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5999 - accuracy: 0.6856 - val_loss: 0.5822 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5862 - accuracy: 0.7013 - val_loss: 0.5764 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5719 - accuracy: 0.7100 - val_loss: 0.5751 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5669 - accuracy: 0.7131 - val_loss: 0.5684 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5678 - accuracy: 0.7125 - val_loss: 0.5647 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5816 - accuracy: 0.6750 - val_loss: 0.5612 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5763 - accuracy: 0.6862 - val_loss: 0.5597 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5506 - accuracy: 0.7169 - val_loss: 0.5549 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5715 - accuracy: 0.6894 - val_loss: 0.5515 - val_accuracy: 0.7021\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5467 - accuracy: 0.7125 - val_loss: 0.5493 - val_accuracy: 0.7021\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5612 - accuracy: 0.6969 - val_loss: 0.5481 - val_accuracy: 0.7035\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5628 - accuracy: 0.7119 - val_loss: 0.5498 - val_accuracy: 0.7021\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5519 - accuracy: 0.7094 - val_loss: 0.5424 - val_accuracy: 0.7021\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5503 - accuracy: 0.7206 - val_loss: 0.5438 - val_accuracy: 0.7021\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5436 - accuracy: 0.7212 - val_loss: 0.5375 - val_accuracy: 0.7021\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5486 - accuracy: 0.7150 - val_loss: 0.5460 - val_accuracy: 0.7021\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5437 - accuracy: 0.7081 - val_loss: 0.5345 - val_accuracy: 0.7128\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5390 - accuracy: 0.7131 - val_loss: 0.5344 - val_accuracy: 0.7340\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5518 - accuracy: 0.7050 - val_loss: 0.5310 - val_accuracy: 0.7154\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5298 - accuracy: 0.7231 - val_loss: 0.5350 - val_accuracy: 0.7021\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.4170 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_test_batch_end` time: 0.0115s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.2207 - accuracy: 0.4123\n",
      "training acc:  0.7231249809265137 , training loss:  0.5297866463661194 , val acc:  0.7021276354789734 , val loss:  0.5350494980812073 , test acc:  0.4123322069644928 , test loss:  1.220662236213684\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt4/100word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6432 - accuracy: 0.6925 - val_loss: 0.6307 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6261 - accuracy: 0.7063 - val_loss: 0.6235 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6206 - accuracy: 0.7031 - val_loss: 0.6188 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6206 - accuracy: 0.6938 - val_loss: 0.6104 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6230 - accuracy: 0.6819 - val_loss: 0.6070 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6017 - accuracy: 0.7050 - val_loss: 0.5985 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5986 - accuracy: 0.7038 - val_loss: 0.5931 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.5894 - accuracy: 0.7100 - val_loss: 0.5898 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.5966 - accuracy: 0.6925 - val_loss: 0.5832 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6003 - accuracy: 0.6781 - val_loss: 0.5772 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.5823 - accuracy: 0.7000 - val_loss: 0.5725 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5695 - accuracy: 0.7138 - val_loss: 0.5663 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5741 - accuracy: 0.6931 - val_loss: 0.5632 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5518 - accuracy: 0.7269 - val_loss: 0.5580 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5562 - accuracy: 0.7025 - val_loss: 0.5534 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5600 - accuracy: 0.6956 - val_loss: 0.5525 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5517 - accuracy: 0.7031 - val_loss: 0.5464 - val_accuracy: 0.7035\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5458 - accuracy: 0.7094 - val_loss: 0.5420 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5536 - accuracy: 0.7013 - val_loss: 0.5406 - val_accuracy: 0.7168\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5344 - accuracy: 0.7175 - val_loss: 0.5370 - val_accuracy: 0.7274\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5398 - accuracy: 0.7344 - val_loss: 0.5340 - val_accuracy: 0.7021\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5310 - accuracy: 0.7269 - val_loss: 0.5354 - val_accuracy: 0.7021\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5405 - accuracy: 0.7237 - val_loss: 0.5364 - val_accuracy: 0.7021\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5405 - accuracy: 0.7169 - val_loss: 0.5246 - val_accuracy: 0.7261\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5379 - accuracy: 0.7375 - val_loss: 0.5224 - val_accuracy: 0.7035\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5322 - accuracy: 0.7256 - val_loss: 0.5230 - val_accuracy: 0.7673\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5393 - accuracy: 0.7212 - val_loss: 0.5179 - val_accuracy: 0.7553\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.5389 - accuracy: 0.7394 - val_loss: 0.5158 - val_accuracy: 0.7207\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5206 - accuracy: 0.7600 - val_loss: 0.5161 - val_accuracy: 0.7061\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5115 - accuracy: 0.7462 - val_loss: 0.5213 - val_accuracy: 0.7035\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.6123 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0137s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.2580 - accuracy: 0.4123\n",
      "training acc:  0.7462499737739563 , training loss:  0.5114902853965759 , val acc:  0.7034574747085571 , val loss:  0.5213429927825928 , test acc:  0.4123322069644928 , test loss:  1.2580053806304932\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt4/150word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function second likely best either add must etc seems give around help lot always option made means though small us page world light keep bit space anything real url$ trying end years least large following note non order \\to & maybe x person done day start    size reason\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6618 - accuracy: 0.6388 - val_loss: 0.6301 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6284 - accuracy: 0.6981 - val_loss: 0.6239 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6073 - accuracy: 0.7200 - val_loss: 0.6270 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6118 - accuracy: 0.7119 - val_loss: 0.6167 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6259 - accuracy: 0.6831 - val_loss: 0.6099 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5987 - accuracy: 0.7138 - val_loss: 0.6034 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6070 - accuracy: 0.6975 - val_loss: 0.6016 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6000 - accuracy: 0.6963 - val_loss: 0.5897 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5945 - accuracy: 0.6969 - val_loss: 0.5839 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5890 - accuracy: 0.7025 - val_loss: 0.5789 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5918 - accuracy: 0.6950 - val_loss: 0.5750 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5922 - accuracy: 0.6831 - val_loss: 0.5692 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5600 - accuracy: 0.7231 - val_loss: 0.5672 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5706 - accuracy: 0.7019 - val_loss: 0.5607 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5689 - accuracy: 0.7069 - val_loss: 0.5554 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5739 - accuracy: 0.6894 - val_loss: 0.5529 - val_accuracy: 0.7035\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5638 - accuracy: 0.6925 - val_loss: 0.5562 - val_accuracy: 0.7566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5573 - accuracy: 0.7188 - val_loss: 0.5440 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5602 - accuracy: 0.6969 - val_loss: 0.5401 - val_accuracy: 0.7021\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5506 - accuracy: 0.7131 - val_loss: 0.5374 - val_accuracy: 0.7021\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5363 - accuracy: 0.7188 - val_loss: 0.5338 - val_accuracy: 0.7021\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5549 - accuracy: 0.7225 - val_loss: 0.5358 - val_accuracy: 0.7673\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5433 - accuracy: 0.7175 - val_loss: 0.5282 - val_accuracy: 0.7261\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5424 - accuracy: 0.7300 - val_loss: 0.5403 - val_accuracy: 0.7021\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5413 - accuracy: 0.7156 - val_loss: 0.5309 - val_accuracy: 0.7859\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.5386 - accuracy: 0.7306 - val_loss: 0.5203 - val_accuracy: 0.7394\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5216 - accuracy: 0.7469 - val_loss: 0.5218 - val_accuracy: 0.7035\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5187 - accuracy: 0.7419 - val_loss: 0.5181 - val_accuracy: 0.7699\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5333 - accuracy: 0.7344 - val_loss: 0.5241 - val_accuracy: 0.7846\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5352 - accuracy: 0.7513 - val_loss: 0.5125 - val_accuracy: 0.7660\n",
      "  1/596 [..............................] - ETA: 4s - loss: 1.4494 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0152s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.1661 - accuracy: 0.4274\n",
      "training acc:  0.7512500286102295 , training loss:  0.5351976156234741 , val acc:  0.7659574747085571 , val loss:  0.5125393867492676 , test acc:  0.4274328947067261 , test loss:  1.1661137342453003\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt4/200word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function second likely best either add must etc seems give around help lot always option made means though small us page world light keep bit space anything real url$ trying end years least large following note non order \\to & maybe x person done day start    size reason said process % already level read 0 > based someone simply rather life found issue times information specific type solution little check similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6411 - accuracy: 0.6744 - val_loss: 0.6290 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6294 - accuracy: 0.6950 - val_loss: 0.6216 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6250 - accuracy: 0.6969 - val_loss: 0.6129 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6119 - accuracy: 0.7031 - val_loss: 0.6055 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6008 - accuracy: 0.7113 - val_loss: 0.6029 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6071 - accuracy: 0.6956 - val_loss: 0.6028 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5974 - accuracy: 0.6975 - val_loss: 0.5868 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5996 - accuracy: 0.6819 - val_loss: 0.5792 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5828 - accuracy: 0.6981 - val_loss: 0.5719 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5735 - accuracy: 0.7094 - val_loss: 0.5677 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5752 - accuracy: 0.6963 - val_loss: 0.5617 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5715 - accuracy: 0.6944 - val_loss: 0.5579 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5644 - accuracy: 0.7212 - val_loss: 0.5522 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5619 - accuracy: 0.7063 - val_loss: 0.5470 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5481 - accuracy: 0.7100 - val_loss: 0.5434 - val_accuracy: 0.7035\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5751 - accuracy: 0.6931 - val_loss: 0.5392 - val_accuracy: 0.7035\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5575 - accuracy: 0.7006 - val_loss: 0.5356 - val_accuracy: 0.7035\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5386 - accuracy: 0.7106 - val_loss: 0.5331 - val_accuracy: 0.7301\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5491 - accuracy: 0.7200 - val_loss: 0.5289 - val_accuracy: 0.7287\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5359 - accuracy: 0.7256 - val_loss: 0.5256 - val_accuracy: 0.7035\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5464 - accuracy: 0.7269 - val_loss: 0.5235 - val_accuracy: 0.7301\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5512 - accuracy: 0.7194 - val_loss: 0.5276 - val_accuracy: 0.7859\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5465 - accuracy: 0.7300 - val_loss: 0.5214 - val_accuracy: 0.7620\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5305 - accuracy: 0.7325 - val_loss: 0.5163 - val_accuracy: 0.7380\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5308 - accuracy: 0.7437 - val_loss: 0.5145 - val_accuracy: 0.7540\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.5280 - accuracy: 0.7394 - val_loss: 0.5133 - val_accuracy: 0.7620\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5271 - accuracy: 0.7381 - val_loss: 0.5130 - val_accuracy: 0.7128\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5215 - accuracy: 0.7519 - val_loss: 0.5090 - val_accuracy: 0.7540\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5299 - accuracy: 0.7444 - val_loss: 0.5077 - val_accuracy: 0.7553\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5408 - accuracy: 0.7469 - val_loss: 0.5065 - val_accuracy: 0.7553\n",
      "  1/596 [..............................] - ETA: 10s - loss: 1.6920 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0131s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 14s 24ms/step - loss: 1.3259 - accuracy: 0.4253\n",
      "training acc:  0.746874988079071 , training loss:  0.5407816767692566 , val acc:  0.7553191781044006 , val loss:  0.5065369009971619 , test acc:  0.425335556268692 , test loss:  1.3258594274520874\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt4/250word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function second likely best either add must etc seems give around help lot always option made means though small us page world light keep bit space anything real url$ trying end years least large following note non order \\to & maybe x person done day start    size reason said process % already level read 0 > based someone simply rather life found issue times information specific type solution little check similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line put < c idea water god working group list state assume past whether run matter place last amount server great paper correct energy control open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.7038 - accuracy: 0.6006 - val_loss: 0.6297 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.6029 - accuracy: 0.7344 - val_loss: 0.6237 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6213 - accuracy: 0.7019 - val_loss: 0.6180 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6185 - accuracy: 0.7025 - val_loss: 0.6143 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6174 - accuracy: 0.6913 - val_loss: 0.6082 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6079 - accuracy: 0.7013 - val_loss: 0.5993 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6036 - accuracy: 0.6944 - val_loss: 0.5936 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6020 - accuracy: 0.6844 - val_loss: 0.5844 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5903 - accuracy: 0.6981 - val_loss: 0.5807 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5801 - accuracy: 0.7081 - val_loss: 0.5728 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5824 - accuracy: 0.6969 - val_loss: 0.5679 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5617 - accuracy: 0.7081 - val_loss: 0.5622 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5808 - accuracy: 0.7031 - val_loss: 0.5550 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5552 - accuracy: 0.7056 - val_loss: 0.5544 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5507 - accuracy: 0.7212 - val_loss: 0.5578 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5504 - accuracy: 0.7119 - val_loss: 0.5413 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5515 - accuracy: 0.7069 - val_loss: 0.5391 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5463 - accuracy: 0.7175 - val_loss: 0.5326 - val_accuracy: 0.7181\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5518 - accuracy: 0.7200 - val_loss: 0.5301 - val_accuracy: 0.7553\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5383 - accuracy: 0.7181 - val_loss: 0.5334 - val_accuracy: 0.7819\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5487 - accuracy: 0.7337 - val_loss: 0.5371 - val_accuracy: 0.7673\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5184 - accuracy: 0.7513 - val_loss: 0.5190 - val_accuracy: 0.7500\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5241 - accuracy: 0.7306 - val_loss: 0.5173 - val_accuracy: 0.7168\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5349 - accuracy: 0.7487 - val_loss: 0.5149 - val_accuracy: 0.7646\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5338 - accuracy: 0.7444 - val_loss: 0.5120 - val_accuracy: 0.7633\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5229 - accuracy: 0.7575 - val_loss: 0.5110 - val_accuracy: 0.7713\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5278 - accuracy: 0.7400 - val_loss: 0.5092 - val_accuracy: 0.7739\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5205 - accuracy: 0.7581 - val_loss: 0.5088 - val_accuracy: 0.7380\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5212 - accuracy: 0.7594 - val_loss: 0.5062 - val_accuracy: 0.7832ss: 0.5\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5140 - accuracy: 0.7544 - val_loss: 0.5028 - val_accuracy: 0.7686\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 1.3063 - accuracy: 0.4257\n",
      "training acc:  0.7543749809265137 , training loss:  0.514015257358551 , val acc:  0.7686170339584351 , val loss:  0.502759575843811 , test acc:  0.4256501793861389 , test loss:  1.3062571287155151\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt4/300word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function second likely best either add must etc seems give around help lot always option made means though small us page world light keep bit space anything real url$ trying end years least large following note non order \\to & maybe x person done day start    size reason said process % already level read 0 > based someone simply rather life found issue times information specific type solution little check similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line put < c idea water god working group list state assume past whether run matter place last amount server great paper correct energy control open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year original object higher online bytes making useful left difference current needs area user values free price future everything lens test provide away common almost results thus seem input 10 call lower position class camera easy although name book got old target error within write comes true unless clear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.7104 - accuracy: 0.6906 - val_loss: 0.6544 - val_accuracy: 0.6955\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6444 - accuracy: 0.6988 - val_loss: 0.6488 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6343 - accuracy: 0.7025 - val_loss: 0.6367 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6228 - accuracy: 0.7150 - val_loss: 0.6257 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6084 - accuracy: 0.7237 - val_loss: 0.6171 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6108 - accuracy: 0.7075 - val_loss: 0.6135 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6030 - accuracy: 0.7088 - val_loss: 0.6152 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6071 - accuracy: 0.6956 - val_loss: 0.5973 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5872 - accuracy: 0.7113 - val_loss: 0.5868 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5932 - accuracy: 0.6994 - val_loss: 0.5801 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5902 - accuracy: 0.6938 - val_loss: 0.5739 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5740 - accuracy: 0.7094 - val_loss: 0.5705 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5683 - accuracy: 0.7069 - val_loss: 0.5613 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5639 - accuracy: 0.6969 - val_loss: 0.5550 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5573 - accuracy: 0.7006 - val_loss: 0.5505 - val_accuracy: 0.7035\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5556 - accuracy: 0.7350 - val_loss: 0.5522 - val_accuracy: 0.7660\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5490 - accuracy: 0.7081 - val_loss: 0.5399 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5390 - accuracy: 0.7175 - val_loss: 0.5346 - val_accuracy: 0.7035\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5445 - accuracy: 0.7206 - val_loss: 0.5323 - val_accuracy: 0.7553\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5443 - accuracy: 0.7500 - val_loss: 0.5272 - val_accuracy: 0.7420\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5388 - accuracy: 0.7262 - val_loss: 0.5263 - val_accuracy: 0.7035\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5409 - accuracy: 0.7475 - val_loss: 0.5207 - val_accuracy: 0.7207\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5423 - accuracy: 0.7387 - val_loss: 0.5175 - val_accuracy: 0.7460\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5261 - accuracy: 0.7444 - val_loss: 0.5157 - val_accuracy: 0.7301\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5298 - accuracy: 0.7456 - val_loss: 0.5129 - val_accuracy: 0.7620\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5318 - accuracy: 0.7494 - val_loss: 0.5141 - val_accuracy: 0.7819\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5242 - accuracy: 0.7556 - val_loss: 0.5110 - val_accuracy: 0.7846\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5186 - accuracy: 0.7669 - val_loss: 0.5080 - val_accuracy: 0.7832\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5166 - accuracy: 0.7525 - val_loss: 0.5130 - val_accuracy: 0.7793\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5191 - accuracy: 0.7656 - val_loss: 0.5043 - val_accuracy: 0.7540\n",
      "  1/596 [..............................] - ETA: 0s - loss: 1.6136 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0150s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 1.2627 - accuracy: 0.4226\n",
      "training acc:  0.765625 , training loss:  0.5190877914428711 , val acc:  0.7539893388748169 , val loss:  0.5042991638183594 , test acc:  0.4226090610027313 , test loss:  1.2627102136611938\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt4/350word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function second likely best either add must etc seems give around help lot always option made means though small us page world light keep bit space anything real url$ trying end years least large following note non order \\to & maybe x person done day start    size reason said process % already level read 0 > based someone simply rather life found issue times information specific type solution little check similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line put < c idea water god working group list state assume past whether run matter place last amount server great paper correct energy control open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year original object higher online bytes making useful left difference current needs area user values free price future everything lens test provide away common almost results thus seem input 10 call lower position class camera easy although name book got old target error within write comes true unless clear    words low mm full feel smaller wo available top source thought experience others big map avoid perhaps several ask close access effect tried numbers e.g. changes tell actual created multiple difficult game three length takes larger standard 4 layer design story earth believe anyone exactly theory \\\\ days wrong\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.9637 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0108s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6551 - accuracy: 0.6369 - val_loss: 0.6318 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6296 - accuracy: 0.6988 - val_loss: 0.6240 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6302 - accuracy: 0.6888 - val_loss: 0.6204 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6089 - accuracy: 0.7106 - val_loss: 0.6101 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6210 - accuracy: 0.6837 - val_loss: 0.6092 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6151 - accuracy: 0.6850 - val_loss: 0.5972 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6070 - accuracy: 0.6906 - val_loss: 0.5921 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5983 - accuracy: 0.6931 - val_loss: 0.5866 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5786 - accuracy: 0.7125 - val_loss: 0.5858 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5745 - accuracy: 0.7125 - val_loss: 0.5756 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5770 - accuracy: 0.6969 - val_loss: 0.5689 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5702 - accuracy: 0.7013 - val_loss: 0.5637 - val_accuracy: 0.7021\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5554 - accuracy: 0.7200 - val_loss: 0.5567 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5537 - accuracy: 0.7150 - val_loss: 0.5512 - val_accuracy: 0.7021\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5590 - accuracy: 0.7044 - val_loss: 0.5493 - val_accuracy: 0.7101\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5466 - accuracy: 0.7138 - val_loss: 0.5425 - val_accuracy: 0.7021\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5520 - accuracy: 0.7050 - val_loss: 0.5385 - val_accuracy: 0.7035\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5503 - accuracy: 0.7163 - val_loss: 0.5371 - val_accuracy: 0.7021\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5363 - accuracy: 0.7237 - val_loss: 0.5311 - val_accuracy: 0.7128\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5377 - accuracy: 0.7331 - val_loss: 0.5283 - val_accuracy: 0.7088\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5496 - accuracy: 0.7206 - val_loss: 0.5281 - val_accuracy: 0.7035\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5335 - accuracy: 0.7269 - val_loss: 0.5219 - val_accuracy: 0.7487\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5367 - accuracy: 0.7437 - val_loss: 0.5195 - val_accuracy: 0.7261\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5095 - accuracy: 0.7475 - val_loss: 0.5223 - val_accuracy: 0.7035\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5474 - accuracy: 0.7287 - val_loss: 0.5202 - val_accuracy: 0.7846\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5088 - accuracy: 0.7625 - val_loss: 0.5158 - val_accuracy: 0.7141\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5259 - accuracy: 0.7513 - val_loss: 0.5197 - val_accuracy: 0.7035\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5412 - accuracy: 0.7431 - val_loss: 0.5141 - val_accuracy: 0.7154\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5394 - accuracy: 0.7487 - val_loss: 0.5080 - val_accuracy: 0.7580\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5200 - accuracy: 0.7569 - val_loss: 0.5106 - val_accuracy: 0.7859\n",
      "  1/596 [..............................] - ETA: 8s - loss: 1.4701 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0133s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.2117 - accuracy: 0.4320\n",
      "training acc:  0.7568749785423279 , training loss:  0.520025372505188 , val acc:  0.7859042286872864 , val loss:  0.510570228099823 , test acc:  0.4320469796657562 , test loss:  1.2117482423782349\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt4/400word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function second likely best either add must etc seems give around help lot always option made means though small us page world light keep bit space anything real url$ trying end years least large following note non order \\to & maybe x person done day start    size reason said process % already level read 0 > based someone simply rather life found issue times information specific type solution little check similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line put < c idea water god working group list state assume past whether run matter place last amount server great paper correct energy control open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year original object higher online bytes making useful left difference current needs area user values free price future everything lens test provide away common almost results thus seem input 10 call lower position class camera easy although name book got old target error within write comes true unless clear    words low mm full feel smaller wo available top source thought experience others big map avoid perhaps several ask close access effect tried numbers e.g. changes tell actual created multiple difficult game three length takes larger standard 4 layer design story earth believe anyone exactly theory \\\\ days wrong terms fine history air mind hand sound language model ' #    turn along sort writing generally yes post become   taking easily nothing known money search product running gives happen per support company n planet block cases slightly allow uses ones 100 children necessary step bad    whole certain\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6305 - accuracy: 0.6881 - val_loss: 0.6393 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6286 - accuracy: 0.7038 - val_loss: 0.6219 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6199 - accuracy: 0.7038 - val_loss: 0.6127 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6097 - accuracy: 0.7069 - val_loss: 0.6081 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5999 - accuracy: 0.7125 - val_loss: 0.5978 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5994 - accuracy: 0.7013 - val_loss: 0.5927 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5816 - accuracy: 0.7156 - val_loss: 0.5828 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5845 - accuracy: 0.6975 - val_loss: 0.5761 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5807 - accuracy: 0.6969 - val_loss: 0.5669 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5571 - accuracy: 0.7219 - val_loss: 0.5725 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5706 - accuracy: 0.7019 - val_loss: 0.5593 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5618 - accuracy: 0.7000 - val_loss: 0.5493 - val_accuracy: 0.7035\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5738 - accuracy: 0.6925 - val_loss: 0.5448 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5582 - accuracy: 0.7156 - val_loss: 0.5406 - val_accuracy: 0.7327\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5511 - accuracy: 0.7125 - val_loss: 0.5360 - val_accuracy: 0.7021\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5551 - accuracy: 0.7100 - val_loss: 0.5323 - val_accuracy: 0.7660\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5431 - accuracy: 0.7262 - val_loss: 0.5259 - val_accuracy: 0.7221\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5425 - accuracy: 0.7225 - val_loss: 0.5229 - val_accuracy: 0.7394\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5216 - accuracy: 0.7481 - val_loss: 0.5197 - val_accuracy: 0.7620\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5343 - accuracy: 0.7281 - val_loss: 0.5220 - val_accuracy: 0.7832\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5198 - accuracy: 0.7713 - val_loss: 0.5163 - val_accuracy: 0.7221\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5351 - accuracy: 0.7412 - val_loss: 0.5155 - val_accuracy: 0.7181\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5282 - accuracy: 0.7494 - val_loss: 0.5092 - val_accuracy: 0.7646\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5413 - accuracy: 0.7400 - val_loss: 0.5075 - val_accuracy: 0.7660\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5123 - accuracy: 0.7563 - val_loss: 0.5064 - val_accuracy: 0.7766\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5093 - accuracy: 0.7681 - val_loss: 0.5041 - val_accuracy: 0.7753\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5165 - accuracy: 0.7706 - val_loss: 0.5031 - val_accuracy: 0.7859\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5091 - accuracy: 0.7788 - val_loss: 0.5195 - val_accuracy: 0.7088\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5211 - accuracy: 0.7513 - val_loss: 0.5069 - val_accuracy: 0.7793\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5121 - accuracy: 0.7638 - val_loss: 0.5059 - val_accuracy: 0.7753\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.5779 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0123s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.2727 - accuracy: 0.4341\n",
      "training acc:  0.7637500166893005 , training loss:  0.5120656490325928 , val acc:  0.7752659320831299 , val loss:  0.5059382915496826 , test acc:  0.4341442883014679 , test loss:  1.2727291584014893\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt4/450word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function second likely best either add must etc seems give around help lot always option made means though small us page world light keep bit space anything real url$ trying end years least large following note non order \\to & maybe x person done day start    size reason said process % already level read 0 > based someone simply rather life found issue times information specific type solution little check similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line put < c idea water god working group list state assume past whether run matter place last amount server great paper correct energy control open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year original object higher online bytes making useful left difference current needs area user values free price future everything lens test provide away common almost results thus seem input 10 call lower position class camera easy although name book got old target error within write comes true unless clear    words low mm full feel smaller wo available top source thought experience others big map avoid perhaps several ask close access effect tried numbers e.g. changes tell actual created multiple difficult game three length takes larger standard 4 layer design story earth believe anyone exactly theory \\\\ days wrong terms fine history air mind hand sound language model ' #    turn along sort writing generally yes post become   taking easily nothing known money search product running gives happen per support company n planet block cases slightly allow uses ones 100 children necessary step bad    whole certain sometimes related gets research says return humans build knowledge setting context cause 6 normal computer seen \\ situation main move food men characters n$ functions english inside body problems structure examples added choose rules play job ability thanks points therefore depends mode limit content meaning reading exact ground view flash\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.8478 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0044s vs `on_train_batch_end` time: 0.0158s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6454 - accuracy: 0.7031 - val_loss: 0.6467 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6259 - accuracy: 0.7163 - val_loss: 0.6319 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6314 - accuracy: 0.6925 - val_loss: 0.6211 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6163 - accuracy: 0.7031 - val_loss: 0.6122 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6084 - accuracy: 0.7044 - val_loss: 0.6125 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6126 - accuracy: 0.6913 - val_loss: 0.5957 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.5982 - accuracy: 0.7000 - val_loss: 0.5913 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5770 - accuracy: 0.7181 - val_loss: 0.5815 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5791 - accuracy: 0.7063 - val_loss: 0.5757 - val_accuracy: 0.7021\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5787 - accuracy: 0.7044 - val_loss: 0.5698 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5825 - accuracy: 0.6944 - val_loss: 0.5627 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5687 - accuracy: 0.7000 - val_loss: 0.5584 - val_accuracy: 0.7035\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5555 - accuracy: 0.7094 - val_loss: 0.5545 - val_accuracy: 0.7021\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5603 - accuracy: 0.7063 - val_loss: 0.5462 - val_accuracy: 0.7035\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5515 - accuracy: 0.7031 - val_loss: 0.5409 - val_accuracy: 0.7035\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5559 - accuracy: 0.6969 - val_loss: 0.5418 - val_accuracy: 0.7726\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5392 - accuracy: 0.7350 - val_loss: 0.5408 - val_accuracy: 0.7021\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5364 - accuracy: 0.7212 - val_loss: 0.5281 - val_accuracy: 0.7327\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5522 - accuracy: 0.7225 - val_loss: 0.5277 - val_accuracy: 0.7035\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5431 - accuracy: 0.7225 - val_loss: 0.5223 - val_accuracy: 0.7301\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5334 - accuracy: 0.7337 - val_loss: 0.5194 - val_accuracy: 0.7340\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5257 - accuracy: 0.7450 - val_loss: 0.5161 - val_accuracy: 0.7380\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5254 - accuracy: 0.7381 - val_loss: 0.5137 - val_accuracy: 0.7420\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5453 - accuracy: 0.7381 - val_loss: 0.5122 - val_accuracy: 0.7753\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5140 - accuracy: 0.7575 - val_loss: 0.5096 - val_accuracy: 0.7540\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5363 - accuracy: 0.7487 - val_loss: 0.5073 - val_accuracy: 0.7699\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5185 - accuracy: 0.7519 - val_loss: 0.5053 - val_accuracy: 0.7739\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5375 - accuracy: 0.7525 - val_loss: 0.5034 - val_accuracy: 0.7726\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5332 - accuracy: 0.7531 - val_loss: 0.5051 - val_accuracy: 0.7819\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5292 - accuracy: 0.7525 - val_loss: 0.5018 - val_accuracy: 0.7806\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.5051 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0143s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.2117 - accuracy: 0.4317\n",
      "training acc:  0.7524999976158142 , training loss:  0.5292270183563232 , val acc:  0.7805851101875305 , val loss:  0.5018169283866882 , test acc:  0.4317323863506317 , test loss:  1.2116714715957642\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_800/opt4/500word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like / use also could time get   need make way - even work using want people think 'm first two see may might = much know 1 something different example question used well 've however good really 're number case set still take since many say problem try new things 2 find probably possible another point answer enough value + right without let go system better actually every data going power \\mathcal back long 'd given 'll able code change ca sure look image 3 create part less high thing form function second likely best either add must etc seems give around help lot always option made means though small us page world light keep bit space anything real url$ trying end years least large following note non order \\to & maybe x person done day start    size reason said process % already level read 0 > based someone simply rather life found issue times information specific type solution little check similar output edit word file far usually ai mean simple result never field looking key come often works single human side important instead understand text b hard makes fact course quite line put < c idea water god working group list state assume past whether run matter place last amount server great paper correct energy control open next method range due pretty sense i.e. called character color getting particular consider site 5 kind show version short general term else year original object higher online bytes making useful left difference current needs area user values free price future everything lens test provide away common almost results thus seem input 10 call lower position class camera easy although name book got old target error within write comes true unless clear    words low mm full feel smaller wo available top source thought experience others big map avoid perhaps several ask close access effect tried numbers e.g. changes tell actual created multiple difficult game three length takes larger standard 4 layer design story earth believe anyone exactly theory \\\\ days wrong terms fine history air mind hand sound language model ' #    turn along sort writing generally yes post become   taking easily nothing known money search product running gives happen per support company n planet block cases slightly allow uses ones 100 children necessary step bad    whole certain sometimes related gets research says return humans build knowledge setting context cause 6 normal computer seen \\ situation main move food men characters n$ functions english inside body problems structure examples added choose rules play job ability thanks points therefore depends mode limit content meaning reading exact ground view flash major risk approach account force network assuming whatever solve fire total half outside stop directly conditions link google reasons noise cost update head perfect magic pressure yet options remember require distance needed rate save looks weapons completely zero wine address select background worth e algorithm software strong easier goes equation\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.4956 - accuracy: 0.8750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0115s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6263 - accuracy: 0.7256 - val_loss: 0.6393 - val_accuracy: 0.7021\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6376 - accuracy: 0.6975 - val_loss: 0.6299 - val_accuracy: 0.7021\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6307 - accuracy: 0.6938 - val_loss: 0.6217 - val_accuracy: 0.7021\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6128 - accuracy: 0.7075 - val_loss: 0.6106 - val_accuracy: 0.7021\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6106 - accuracy: 0.7000 - val_loss: 0.6019 - val_accuracy: 0.7021\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6017 - accuracy: 0.7031 - val_loss: 0.5933 - val_accuracy: 0.7021\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5862 - accuracy: 0.7156 - val_loss: 0.5861 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6022 - accuracy: 0.6762 - val_loss: 0.5781 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5876 - accuracy: 0.6888 - val_loss: 0.5760 - val_accuracy: 0.7101\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5733 - accuracy: 0.6981 - val_loss: 0.5622 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5793 - accuracy: 0.6888 - val_loss: 0.5542 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5562 - accuracy: 0.7063 - val_loss: 0.5477 - val_accuracy: 0.7035\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5452 - accuracy: 0.7150 - val_loss: 0.5423 - val_accuracy: 0.7141\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5514 - accuracy: 0.7281 - val_loss: 0.5367 - val_accuracy: 0.7035\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5535 - accuracy: 0.7306 - val_loss: 0.5330 - val_accuracy: 0.7035\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5428 - accuracy: 0.7337 - val_loss: 0.5267 - val_accuracy: 0.7168\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5283 - accuracy: 0.7387 - val_loss: 0.5240 - val_accuracy: 0.7074\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5354 - accuracy: 0.7325 - val_loss: 0.5175 - val_accuracy: 0.7620\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5086 - accuracy: 0.7519 - val_loss: 0.5168 - val_accuracy: 0.7234\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5230 - accuracy: 0.7487 - val_loss: 0.5187 - val_accuracy: 0.7101\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5367 - accuracy: 0.7538 - val_loss: 0.5098 - val_accuracy: 0.7726\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5101 - accuracy: 0.7613 - val_loss: 0.5093 - val_accuracy: 0.7340\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5247 - accuracy: 0.7613 - val_loss: 0.5038 - val_accuracy: 0.7726\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5217 - accuracy: 0.7681 - val_loss: 0.5059 - val_accuracy: 0.7420\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5187 - accuracy: 0.7525 - val_loss: 0.5000 - val_accuracy: 0.7726\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5262 - accuracy: 0.7644 - val_loss: 0.4980 - val_accuracy: 0.7739\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5133 - accuracy: 0.7681 - val_loss: 0.4970 - val_accuracy: 0.7846\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5159 - accuracy: 0.7569 - val_loss: 0.4964 - val_accuracy: 0.7660\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5238 - accuracy: 0.7525 - val_loss: 0.4938 - val_accuracy: 0.7819\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5013 - accuracy: 0.7713 - val_loss: 0.4923 - val_accuracy: 0.7699\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.6572 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0197s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 1.3002 - accuracy: 0.4280\n",
      "training acc:  0.7712500095367432 , training loss:  0.5012962222099304 , val acc:  0.769946813583374 , val loss:  0.492261677980423 , test acc:  0.4279572069644928 , test loss:  1.3002264499664307\n",
      "\n",
      "Started evaluation for: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1000.csv\n",
      "Loading training file from path:  /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1000.csv\n",
      "(2481, 6)\n",
      "Loading testing file from path:  /home/sukanya/PhD/Datasets/PAN SCD/pan19-style-change-detection/processed/test.csv\n",
      "(9538, 5)\n",
      "opt1\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt1/50word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6655 - accuracy: 0.6181 - val_loss: 0.6505 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6567 - accuracy: 0.6556 - val_loss: 0.6481 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6623 - accuracy: 0.6406 - val_loss: 0.6449 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6522 - accuracy: 0.6531 - val_loss: 0.6421 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6465 - accuracy: 0.6587 - val_loss: 0.6387 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6397 - accuracy: 0.6644 - val_loss: 0.6349 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6341 - accuracy: 0.6681 - val_loss: 0.6299 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6482 - accuracy: 0.6425 - val_loss: 0.6267 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6399 - accuracy: 0.6494 - val_loss: 0.6282 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6376 - accuracy: 0.6463 - val_loss: 0.6204 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6267 - accuracy: 0.6562 - val_loss: 0.6160 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6204 - accuracy: 0.6631 - val_loss: 0.6117 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6234 - accuracy: 0.6538 - val_loss: 0.6074 - val_accuracy: 0.6673\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6143 - accuracy: 0.6675 - val_loss: 0.6054 - val_accuracy: 0.6673\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6196 - accuracy: 0.6450 - val_loss: 0.6001 - val_accuracy: 0.6673\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6089 - accuracy: 0.6625 - val_loss: 0.5993 - val_accuracy: 0.6694\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6126 - accuracy: 0.6531 - val_loss: 0.5933 - val_accuracy: 0.6673\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6126 - accuracy: 0.6525 - val_loss: 0.5884 - val_accuracy: 0.6673\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6043 - accuracy: 0.6538 - val_loss: 0.5921 - val_accuracy: 0.7319\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6050 - accuracy: 0.6706 - val_loss: 0.5819 - val_accuracy: 0.6734\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6015 - accuracy: 0.6531 - val_loss: 0.5777 - val_accuracy: 0.6694\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5950 - accuracy: 0.6700 - val_loss: 0.5782 - val_accuracy: 0.7258\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5915 - accuracy: 0.6675 - val_loss: 0.5718 - val_accuracy: 0.6673\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5850 - accuracy: 0.6875 - val_loss: 0.5735 - val_accuracy: 0.7581\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5812 - accuracy: 0.6888 - val_loss: 0.5812 - val_accuracy: 0.7419\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5899 - accuracy: 0.6787 - val_loss: 0.5658 - val_accuracy: 0.7540\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5807 - accuracy: 0.7050 - val_loss: 0.5604 - val_accuracy: 0.6996\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5851 - accuracy: 0.7063 - val_loss: 0.5632 - val_accuracy: 0.6673\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5825 - accuracy: 0.6981 - val_loss: 0.5565 - val_accuracy: 0.6895\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5732 - accuracy: 0.6913 - val_loss: 0.5541 - val_accuracy: 0.7036\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.1413 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_test_batch_end` time: 0.0193s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 14s 24ms/step - loss: 1.1511 - accuracy: 0.4176\n",
      "training acc:  0.6912500262260437 , training loss:  0.573233425617218 , val acc:  0.7036290168762207 , val loss:  0.5540745258331299 , test acc:  0.41757550835609436 , test loss:  1.1510822772979736\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt1/100word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.7185 - accuracy: 0.5719 - val_loss: 0.6492 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6541 - accuracy: 0.6525 - val_loss: 0.6443 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6504 - accuracy: 0.6538 - val_loss: 0.6410 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6489 - accuracy: 0.6488 - val_loss: 0.6384 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6463 - accuracy: 0.6456 - val_loss: 0.6339 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6416 - accuracy: 0.6450 - val_loss: 0.6287 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6386 - accuracy: 0.6463 - val_loss: 0.6198 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6351 - accuracy: 0.6475 - val_loss: 0.6177 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6302 - accuracy: 0.6469 - val_loss: 0.6147 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6224 - accuracy: 0.6562 - val_loss: 0.6062 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6209 - accuracy: 0.6531 - val_loss: 0.6042 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6092 - accuracy: 0.6600 - val_loss: 0.5965 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6067 - accuracy: 0.6681 - val_loss: 0.6027 - val_accuracy: 0.7581\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6112 - accuracy: 0.6600 - val_loss: 0.5876 - val_accuracy: 0.6714\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6041 - accuracy: 0.6587 - val_loss: 0.5818 - val_accuracy: 0.6673\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5995 - accuracy: 0.6575 - val_loss: 0.5782 - val_accuracy: 0.6714\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5909 - accuracy: 0.6831 - val_loss: 0.5735 - val_accuracy: 0.6714\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6023 - accuracy: 0.6569 - val_loss: 0.5699 - val_accuracy: 0.6815\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5889 - accuracy: 0.6744 - val_loss: 0.5683 - val_accuracy: 0.7399\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5783 - accuracy: 0.6819 - val_loss: 0.5720 - val_accuracy: 0.7722\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5856 - accuracy: 0.6919 - val_loss: 0.5603 - val_accuracy: 0.7399\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5804 - accuracy: 0.6794 - val_loss: 0.5555 - val_accuracy: 0.7177\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5717 - accuracy: 0.6969 - val_loss: 0.5551 - val_accuracy: 0.6734\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5762 - accuracy: 0.6988 - val_loss: 0.5552 - val_accuracy: 0.6714\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5703 - accuracy: 0.7006 - val_loss: 0.5520 - val_accuracy: 0.7702\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5636 - accuracy: 0.7175 - val_loss: 0.5439 - val_accuracy: 0.7339\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5664 - accuracy: 0.7219 - val_loss: 0.5421 - val_accuracy: 0.7198\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5653 - accuracy: 0.7094 - val_loss: 0.5395 - val_accuracy: 0.7520\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5588 - accuracy: 0.7319 - val_loss: 0.5372 - val_accuracy: 0.7560\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5667 - accuracy: 0.7244 - val_loss: 0.5383 - val_accuracy: 0.7702\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.1369 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0148s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.2245 - accuracy: 0.4244\n",
      "training acc:  0.7243750095367432 , training loss:  0.566721498966217 , val acc:  0.7701612710952759 , val loss:  0.5382826924324036 , test acc:  0.4243917763233185 , test loss:  1.2245464324951172\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt1/150word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most know me such = each something good different question really 1 now example well used its being 've am been however after 're case still set take say since many try who probably number those problem had point between his another things find our new answer enough both 2 over\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 1.6949 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0027s vs `on_train_batch_end` time: 0.0104s). Check your callbacks.\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.7286 - accuracy: 0.6450 - val_loss: 0.6686 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6654 - accuracy: 0.6600 - val_loss: 0.6616 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6620 - accuracy: 0.6600 - val_loss: 0.6553 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6641 - accuracy: 0.6481 - val_loss: 0.6495 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6509 - accuracy: 0.6600 - val_loss: 0.6440 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6515 - accuracy: 0.6531 - val_loss: 0.6390 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6449 - accuracy: 0.6538 - val_loss: 0.6396 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6423 - accuracy: 0.6481 - val_loss: 0.6285 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6366 - accuracy: 0.6506 - val_loss: 0.6298 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6256 - accuracy: 0.6687 - val_loss: 0.6165 - val_accuracy: 0.6673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6131 - accuracy: 0.6781 - val_loss: 0.6112 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6225 - accuracy: 0.6531 - val_loss: 0.6052 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6153 - accuracy: 0.6581 - val_loss: 0.6000 - val_accuracy: 0.6673\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6127 - accuracy: 0.6519 - val_loss: 0.5947 - val_accuracy: 0.6673\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6098 - accuracy: 0.6556 - val_loss: 0.5890 - val_accuracy: 0.6673\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5996 - accuracy: 0.6581 - val_loss: 0.5870 - val_accuracy: 0.7177\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5986 - accuracy: 0.6737 - val_loss: 0.5785 - val_accuracy: 0.6673\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5950 - accuracy: 0.6919 - val_loss: 0.5734 - val_accuracy: 0.6754\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5908 - accuracy: 0.6694 - val_loss: 0.5735 - val_accuracy: 0.7520\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5808 - accuracy: 0.6969 - val_loss: 0.5669 - val_accuracy: 0.7540\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5888 - accuracy: 0.6956 - val_loss: 0.5611 - val_accuracy: 0.6754\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5858 - accuracy: 0.6888 - val_loss: 0.5690 - val_accuracy: 0.7500\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5789 - accuracy: 0.7188 - val_loss: 0.5535 - val_accuracy: 0.7097\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5700 - accuracy: 0.7063 - val_loss: 0.5530 - val_accuracy: 0.7722\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5702 - accuracy: 0.7250 - val_loss: 0.5491 - val_accuracy: 0.7722\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5636 - accuracy: 0.7169 - val_loss: 0.5580 - val_accuracy: 0.7460\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5619 - accuracy: 0.7219 - val_loss: 0.5430 - val_accuracy: 0.7702\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5616 - accuracy: 0.7250 - val_loss: 0.5379 - val_accuracy: 0.7560\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5633 - accuracy: 0.7225 - val_loss: 0.5365 - val_accuracy: 0.7298\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5642 - accuracy: 0.7244 - val_loss: 0.5333 - val_accuracy: 0.7540\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.2168 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_test_batch_end` time: 0.0114s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 1.2658 - accuracy: 0.4230\n",
      "training acc:  0.7243750095367432 , training loss:  0.5642292499542236 , val acc:  0.7540322542190552 , val loss:  0.5332813262939453 , test acc:  0.42302852869033813 , test loss:  1.2658345699310303\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt1/200word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most know me such = each something good different question really 1 now example well used its being 've am been however after 're case still set take say since many try who probably number those problem had point between his another things find our new answer enough both 2 over possible were too why while go actually \\mathcal did system let better through right without + value power long 'd every back going down given able 'll code before 3 data ca change sure look own part image thing less create best off around high give form having either second\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.7227 - accuracy: 0.5581 - val_loss: 0.6471 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6519 - accuracy: 0.6556 - val_loss: 0.6406 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6457 - accuracy: 0.6550 - val_loss: 0.6399 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6433 - accuracy: 0.6544 - val_loss: 0.6354 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6436 - accuracy: 0.6450 - val_loss: 0.6240 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6299 - accuracy: 0.6631 - val_loss: 0.6206 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6323 - accuracy: 0.6469 - val_loss: 0.6163 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6266 - accuracy: 0.6544 - val_loss: 0.6090 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6143 - accuracy: 0.6644 - val_loss: 0.6022 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6142 - accuracy: 0.6562 - val_loss: 0.5968 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6095 - accuracy: 0.6562 - val_loss: 0.5929 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6091 - accuracy: 0.6531 - val_loss: 0.5861 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6045 - accuracy: 0.6587 - val_loss: 0.5825 - val_accuracy: 0.6673\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5955 - accuracy: 0.6862 - val_loss: 0.5902 - val_accuracy: 0.6673\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5936 - accuracy: 0.6656 - val_loss: 0.5722 - val_accuracy: 0.6694\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5960 - accuracy: 0.6656 - val_loss: 0.5678 - val_accuracy: 0.6976\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5899 - accuracy: 0.6875 - val_loss: 0.5657 - val_accuracy: 0.7379\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5859 - accuracy: 0.6919 - val_loss: 0.5603 - val_accuracy: 0.7258\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5838 - accuracy: 0.6956 - val_loss: 0.5559 - val_accuracy: 0.7218\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5859 - accuracy: 0.7038 - val_loss: 0.5562 - val_accuracy: 0.7702\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5703 - accuracy: 0.7063 - val_loss: 0.5492 - val_accuracy: 0.7218\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5695 - accuracy: 0.7150 - val_loss: 0.5521 - val_accuracy: 0.6714\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5691 - accuracy: 0.7025 - val_loss: 0.5516 - val_accuracy: 0.7641\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5685 - accuracy: 0.7163 - val_loss: 0.5436 - val_accuracy: 0.7742\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5687 - accuracy: 0.7287 - val_loss: 0.5382 - val_accuracy: 0.7681\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5713 - accuracy: 0.7194 - val_loss: 0.5353 - val_accuracy: 0.7621\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5653 - accuracy: 0.7312 - val_loss: 0.5328 - val_accuracy: 0.7520ETA: 0s - loss: 0.5656 - ac\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5580 - accuracy: 0.7412 - val_loss: 0.5313 - val_accuracy: 0.7500\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5586 - accuracy: 0.7294 - val_loss: 0.5287 - val_accuracy: 0.7621\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5525 - accuracy: 0.7394 - val_loss: 0.5272 - val_accuracy: 0.7722\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.2302 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0144s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.2889 - accuracy: 0.4249\n",
      "training acc:  0.7393749952316284 , training loss:  0.5524923205375671 , val acc:  0.7721773982048035 , val loss:  0.5271816253662109 , test acc:  0.4249161183834076 , test loss:  1.2889410257339478\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt1/250word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most know me such = each something good different question really 1 now example well used its being 've am been however after 're case still set take say since many try who probably number those problem had point between his another things find our new answer enough both 2 over possible were too why while go actually \\mathcal did system let better through right without + value power long 'd every back going down given able 'll code before 3 data ca change sure look own part image thing less create best off around high give form having either second lot means add etc must always likely small help above us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based few following said made doing non 0 trying level she day x life order page   \n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6616 - accuracy: 0.6500 - val_loss: 0.6528 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6572 - accuracy: 0.6562 - val_loss: 0.6467 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6492 - accuracy: 0.6587 - val_loss: 0.6417 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6502 - accuracy: 0.6500 - val_loss: 0.6358 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6418 - accuracy: 0.6575 - val_loss: 0.6301 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6486 - accuracy: 0.6369 - val_loss: 0.6248 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6412 - accuracy: 0.6400 - val_loss: 0.6214 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6333 - accuracy: 0.6481 - val_loss: 0.6148 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6169 - accuracy: 0.6675 - val_loss: 0.6106 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6199 - accuracy: 0.6587 - val_loss: 0.6042 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6261 - accuracy: 0.6444 - val_loss: 0.5994 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6069 - accuracy: 0.6625 - val_loss: 0.5950 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6117 - accuracy: 0.6481 - val_loss: 0.5909 - val_accuracy: 0.6714\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6050 - accuracy: 0.6587 - val_loss: 0.6012 - val_accuracy: 0.7621\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6003 - accuracy: 0.6812 - val_loss: 0.5794 - val_accuracy: 0.6694\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5988 - accuracy: 0.6725 - val_loss: 0.5754 - val_accuracy: 0.6673\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5866 - accuracy: 0.6750 - val_loss: 0.5717 - val_accuracy: 0.6694\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5893 - accuracy: 0.6737 - val_loss: 0.5668 - val_accuracy: 0.7218\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5856 - accuracy: 0.6900 - val_loss: 0.5628 - val_accuracy: 0.6956\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5752 - accuracy: 0.6981 - val_loss: 0.5628 - val_accuracy: 0.6694\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5801 - accuracy: 0.6931 - val_loss: 0.5558 - val_accuracy: 0.7359\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5733 - accuracy: 0.7138 - val_loss: 0.5546 - val_accuracy: 0.6754\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5772 - accuracy: 0.7075 - val_loss: 0.5508 - val_accuracy: 0.7722\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5733 - accuracy: 0.7356 - val_loss: 0.5462 - val_accuracy: 0.7601\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5768 - accuracy: 0.7094 - val_loss: 0.5437 - val_accuracy: 0.7238\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5717 - accuracy: 0.7169 - val_loss: 0.5400 - val_accuracy: 0.7540\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5639 - accuracy: 0.7381 - val_loss: 0.5374 - val_accuracy: 0.7560\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5661 - accuracy: 0.7119 - val_loss: 0.5360 - val_accuracy: 0.7722\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5539 - accuracy: 0.7450 - val_loss: 0.5342 - val_accuracy: 0.7339\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5708 - accuracy: 0.7200 - val_loss: 0.5305 - val_accuracy: 0.7581\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.2041 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0123s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 1.2475 - accuracy: 0.4242\n",
      "training acc:  0.7200000286102295 , training loss:  0.5708169937133789 , val acc:  0.7580645084381104 , val loss:  0.5305095911026001 , test acc:  0.42418205738067627 , test loss:  1.247544288635254\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt1/300word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most know me such = each something good different question really 1 now example well used its being 've am been however after 're case still set take say since many try who probably number those problem had point between his another things find our new answer enough both 2 over possible were too why while go actually \\mathcal did system let better through right without + value power long 'd every back going down given able 'll code before 3 data ca change sure look own part image thing less create best off around high give form having either second lot means add etc must always likely small help above us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based few following said made doing non 0 trying level she day x life order page    start option simply once size > often already result come human someone edit rather little % usually specific again output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b her <\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.8027 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_end` time: 0.0155s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.7011 - accuracy: 0.6562 - val_loss: 0.6662 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6710 - accuracy: 0.6500 - val_loss: 0.6590 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6676 - accuracy: 0.6425 - val_loss: 0.6523 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6590 - accuracy: 0.6488 - val_loss: 0.6444 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6487 - accuracy: 0.6600 - val_loss: 0.6405 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6501 - accuracy: 0.6456 - val_loss: 0.6370 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6386 - accuracy: 0.6550 - val_loss: 0.6245 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6418 - accuracy: 0.6413 - val_loss: 0.6203 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6245 - accuracy: 0.6594 - val_loss: 0.6124 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6250 - accuracy: 0.6494 - val_loss: 0.6068 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6249 - accuracy: 0.6488 - val_loss: 0.6016 - val_accuracy: 0.6694\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6112 - accuracy: 0.6781 - val_loss: 0.6023 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5976 - accuracy: 0.6706 - val_loss: 0.5880 - val_accuracy: 0.6754\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5972 - accuracy: 0.6737 - val_loss: 0.5865 - val_accuracy: 0.6673\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5918 - accuracy: 0.6712 - val_loss: 0.5750 - val_accuracy: 0.6694\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5870 - accuracy: 0.6731 - val_loss: 0.5695 - val_accuracy: 0.7117\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5914 - accuracy: 0.6988 - val_loss: 0.5643 - val_accuracy: 0.6996\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5756 - accuracy: 0.7025 - val_loss: 0.5671 - val_accuracy: 0.6673\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5777 - accuracy: 0.7125 - val_loss: 0.5749 - val_accuracy: 0.6673\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5715 - accuracy: 0.6975 - val_loss: 0.5510 - val_accuracy: 0.7460\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5749 - accuracy: 0.7256 - val_loss: 0.5492 - val_accuracy: 0.7702\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5716 - accuracy: 0.7119 - val_loss: 0.5441 - val_accuracy: 0.7681\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5682 - accuracy: 0.7287 - val_loss: 0.5391 - val_accuracy: 0.7480\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5639 - accuracy: 0.7156 - val_loss: 0.5401 - val_accuracy: 0.7762\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5619 - accuracy: 0.7375 - val_loss: 0.5331 - val_accuracy: 0.7601\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5607 - accuracy: 0.7256 - val_loss: 0.5318 - val_accuracy: 0.7379\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5458 - accuracy: 0.7212 - val_loss: 0.5276 - val_accuracy: 0.7621\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5555 - accuracy: 0.7425 - val_loss: 0.5289 - val_accuracy: 0.7722\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5644 - accuracy: 0.7244 - val_loss: 0.5287 - val_accuracy: 0.7601\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5566 - accuracy: 0.7312 - val_loss: 0.5214 - val_accuracy: 0.7661\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.3071 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_test_batch_end` time: 0.0142s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.3370 - accuracy: 0.4239\n",
      "training acc:  0.731249988079071 , training loss:  0.5565559267997742 , val acc:  0.7661290168762207 , val loss:  0.5214452147483826 , test acc:  0.42386746406555176 , test loss:  1.3369646072387695\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt1/350word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most know me such = each something good different question really 1 now example well used its being 've am been however after 're case still set take say since many try who probably number those problem had point between his another things find our new answer enough both 2 over possible were too why while go actually \\mathcal did system let better through right without + value power long 'd every back going down given able 'll code before 3 data ca change sure look own part image thing less create best off around high give form having either second lot means add etc must always likely small help above us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based few following said made doing non 0 trying level she day x life order page    start option simply once size > often already result come human someone edit rather little % usually specific again output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b her < state instead god & type place character control him works under last important group water list run mean until sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether itself mm range next server amount object general version consider short term\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.9922 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0096s vs `on_train_batch_end` time: 0.0493s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6979 - accuracy: 0.5875 - val_loss: 0.6525 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6546 - accuracy: 0.6550 - val_loss: 0.6438 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6581 - accuracy: 0.6394 - val_loss: 0.6422 - val_accuracy: 0.6673: 0.6592 - ac\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6446 - accuracy: 0.6531 - val_loss: 0.6319 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6429 - accuracy: 0.6481 - val_loss: 0.6251 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6317 - accuracy: 0.6569 - val_loss: 0.6190 - val_accuracy: 0.6673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6267 - accuracy: 0.6587 - val_loss: 0.6141 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6200 - accuracy: 0.6637 - val_loss: 0.6077 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6135 - accuracy: 0.6619 - val_loss: 0.6011 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6068 - accuracy: 0.6656 - val_loss: 0.5936 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5962 - accuracy: 0.6800 - val_loss: 0.5918 - val_accuracy: 0.7198\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6005 - accuracy: 0.6750 - val_loss: 0.5846 - val_accuracy: 0.7218\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6013 - accuracy: 0.6731 - val_loss: 0.5767 - val_accuracy: 0.6694\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5976 - accuracy: 0.6769 - val_loss: 0.5708 - val_accuracy: 0.6754\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5896 - accuracy: 0.6956 - val_loss: 0.5750 - val_accuracy: 0.7702\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5835 - accuracy: 0.6994 - val_loss: 0.5631 - val_accuracy: 0.7581\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5804 - accuracy: 0.7006 - val_loss: 0.5566 - val_accuracy: 0.7379\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5823 - accuracy: 0.7150 - val_loss: 0.5610 - val_accuracy: 0.6694\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5714 - accuracy: 0.7175 - val_loss: 0.5505 - val_accuracy: 0.6996\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5708 - accuracy: 0.7156 - val_loss: 0.5467 - val_accuracy: 0.7137\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5673 - accuracy: 0.7138 - val_loss: 0.5434 - val_accuracy: 0.7218\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5763 - accuracy: 0.7194 - val_loss: 0.5387 - val_accuracy: 0.7601 ac\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5680 - accuracy: 0.7212 - val_loss: 0.5390 - val_accuracy: 0.7722\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5600 - accuracy: 0.7369 - val_loss: 0.5354 - val_accuracy: 0.7298\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5563 - accuracy: 0.7325 - val_loss: 0.5333 - val_accuracy: 0.7298\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5645 - accuracy: 0.7269 - val_loss: 0.5318 - val_accuracy: 0.7298\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5527 - accuracy: 0.7475 - val_loss: 0.5274 - val_accuracy: 0.7500\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5501 - accuracy: 0.7387 - val_loss: 0.5276 - val_accuracy: 0.7520\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5501 - accuracy: 0.7331 - val_loss: 0.5309 - val_accuracy: 0.7540\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5613 - accuracy: 0.7231 - val_loss: 0.5235 - val_accuracy: 0.7661\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.2690 - accuracy: 0.4242\n",
      "training acc:  0.7231249809265137 , training loss:  0.5612874627113342 , val acc:  0.7661290168762207 , val loss:  0.5235108137130737 , test acc:  0.42418205738067627 , test loss:  1.268973469734192\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt1/400word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most know me such = each something good different question really 1 now example well used its being 've am been however after 're case still set take say since many try who probably number those problem had point between his another things find our new answer enough both 2 over possible were too why while go actually \\mathcal did system let better through right without + value power long 'd every back going down given able 'll code before 3 data ca change sure look own part image thing less create best off around high give form having either second lot means add etc must always likely small help above us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based few following said made doing non 0 trying level she day x life order page    start option simply once size > often already result come human someone edit rather little % usually specific again output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b her < state instead god & type place character control him works under last important group water list run mean until sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether itself mm range next server amount object general version consider short term target i.e. making kind useful color write below test particular class almost energy thus else camera site user got 10 common everything although needs full lower current away higher free difference thought values book original story position show others area seem low call provide name 5 easy available left big\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.9185 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_train_batch_end` time: 0.0511s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.7276 - accuracy: 0.6519 - val_loss: 0.6718 - val_accuracy: 0.6633\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6734 - accuracy: 0.6525 - val_loss: 0.6632 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6675 - accuracy: 0.6494 - val_loss: 0.6544 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6532 - accuracy: 0.6612 - val_loss: 0.6456 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6530 - accuracy: 0.6481 - val_loss: 0.6363 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6390 - accuracy: 0.6612 - val_loss: 0.6274 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6380 - accuracy: 0.6488 - val_loss: 0.6200 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6312 - accuracy: 0.6456 - val_loss: 0.6123 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6174 - accuracy: 0.6675 - val_loss: 0.6030 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6193 - accuracy: 0.6656 - val_loss: 0.5957 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6112 - accuracy: 0.6650 - val_loss: 0.6043 - val_accuracy: 0.7601\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6057 - accuracy: 0.6762 - val_loss: 0.5808 - val_accuracy: 0.6694\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6010 - accuracy: 0.6750 - val_loss: 0.5747 - val_accuracy: 0.6694\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5942 - accuracy: 0.6769 - val_loss: 0.5691 - val_accuracy: 0.7258\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5890 - accuracy: 0.7038 - val_loss: 0.5681 - val_accuracy: 0.6694\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5878 - accuracy: 0.6894 - val_loss: 0.5611 - val_accuracy: 0.7681\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5802 - accuracy: 0.7156 - val_loss: 0.5525 - val_accuracy: 0.7339\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5702 - accuracy: 0.7231 - val_loss: 0.5484 - val_accuracy: 0.7218\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5640 - accuracy: 0.7300 - val_loss: 0.5438 - val_accuracy: 0.7258\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5729 - accuracy: 0.7212 - val_loss: 0.5395 - val_accuracy: 0.7621\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5614 - accuracy: 0.7281 - val_loss: 0.5363 - val_accuracy: 0.7661\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5632 - accuracy: 0.7387 - val_loss: 0.5424 - val_accuracy: 0.6956\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5596 - accuracy: 0.7356 - val_loss: 0.5302 - val_accuracy: 0.7681\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5554 - accuracy: 0.7381 - val_loss: 0.5281 - val_accuracy: 0.7601\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5551 - accuracy: 0.7462 - val_loss: 0.5251 - val_accuracy: 0.7641\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5510 - accuracy: 0.7387 - val_loss: 0.5237 - val_accuracy: 0.7601\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5577 - accuracy: 0.7369 - val_loss: 0.5264 - val_accuracy: 0.7641\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5496 - accuracy: 0.7369 - val_loss: 0.5185 - val_accuracy: 0.7722\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5573 - accuracy: 0.7237 - val_loss: 0.5256 - val_accuracy: 0.7399\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5496 - accuracy: 0.7287 - val_loss: 0.5261 - val_accuracy: 0.7399\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.4380 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_test_batch_end` time: 0.0107s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.3735 - accuracy: 0.4222\n",
      "training acc:  0.7287499904632568 , training loss:  0.5496430397033691 , val acc:  0.7399193644523621 , val loss:  0.5260524153709412 , test acc:  0.4221895933151245 , test loss:  1.3735355138778687\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt1/450word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most know me such = each something good different question really 1 now example well used its being 've am been however after 're case still set take say since many try who probably number those problem had point between his another things find our new answer enough both 2 over possible were too why while go actually \\mathcal did system let better through right without + value power long 'd every back going down given able 'll code before 3 data ca change sure look own part image thing less create best off around high give form having either second lot means add etc must always likely small help above us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based few following said made doing non 0 trying level she day x life order page    start option simply once size > often already result come human someone edit rather little % usually specific again output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b her < state instead god & type place character control him works under last important group water list run mean until sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether itself mm range next server amount object general version consider short term target i.e. making kind useful color write below test particular class almost energy thus else camera site user got 10 common everything although needs full lower current away higher free difference thought values book original story position show others area seem low call provide name 5 easy available left big results year clear map words input    unless close true ask within actual against old language price feel top changes 4 access past game avoid length smaller source mind larger wo several believe experience tell effect difficult comes numbers days wrong design terms earth e.g. # per bad become multiple\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.9355 - accuracy: 0.5938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0212s vs `on_train_batch_end` time: 0.0624s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6913 - accuracy: 0.6575 - val_loss: 0.6657 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6659 - accuracy: 0.6544 - val_loss: 0.6592 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6647 - accuracy: 0.6519 - val_loss: 0.6528 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6570 - accuracy: 0.6519 - val_loss: 0.6459 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6457 - accuracy: 0.6650 - val_loss: 0.6385 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6420 - accuracy: 0.6612 - val_loss: 0.6320 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6437 - accuracy: 0.6444 - val_loss: 0.6332 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6299 - accuracy: 0.6587 - val_loss: 0.6275 - val_accuracy: 0.6694\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6298 - accuracy: 0.6562 - val_loss: 0.6122 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6240 - accuracy: 0.6488 - val_loss: 0.6055 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6125 - accuracy: 0.6637 - val_loss: 0.5995 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6154 - accuracy: 0.6525 - val_loss: 0.5931 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6039 - accuracy: 0.6544 - val_loss: 0.5867 - val_accuracy: 0.6673\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6036 - accuracy: 0.6737 - val_loss: 0.5814 - val_accuracy: 0.6673\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5924 - accuracy: 0.6737 - val_loss: 0.5757 - val_accuracy: 0.6794\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5917 - accuracy: 0.6669 - val_loss: 0.5732 - val_accuracy: 0.7440\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5967 - accuracy: 0.6781 - val_loss: 0.5690 - val_accuracy: 0.7621\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5812 - accuracy: 0.6981 - val_loss: 0.5620 - val_accuracy: 0.6774\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5819 - accuracy: 0.6881 - val_loss: 0.5594 - val_accuracy: 0.7702\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5769 - accuracy: 0.7163 - val_loss: 0.5520 - val_accuracy: 0.7198\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5732 - accuracy: 0.7094 - val_loss: 0.5491 - val_accuracy: 0.7641\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5679 - accuracy: 0.7200 - val_loss: 0.5470 - val_accuracy: 0.7742\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5622 - accuracy: 0.7406 - val_loss: 0.5467 - val_accuracy: 0.6895\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5661 - accuracy: 0.7231 - val_loss: 0.5375 - val_accuracy: 0.7641\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5529 - accuracy: 0.7456 - val_loss: 0.5341 - val_accuracy: 0.7621\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5630 - accuracy: 0.7412 - val_loss: 0.5313 - val_accuracy: 0.7621\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5601 - accuracy: 0.7400 - val_loss: 0.5294 - val_accuracy: 0.7601\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5543 - accuracy: 0.7519 - val_loss: 0.5303 - val_accuracy: 0.7399\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5499 - accuracy: 0.7356 - val_loss: 0.5253 - val_accuracy: 0.7742\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5527 - accuracy: 0.7531 - val_loss: 0.5219 - val_accuracy: 0.7681\n",
      "  1/596 [..............................] - ETA: 4s - loss: 1.2258 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0146s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.2328 - accuracy: 0.4257\n",
      "training acc:  0.753125011920929 , training loss:  0.5526942014694214 , val acc:  0.7681451439857483 , val loss:  0.5219069123268127 , test acc:  0.4256501793861389 , test loss:  1.2327580451965332\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt1/500word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most know me such = each something good different question really 1 now example well used its being 've am been however after 're case still set take say since many try who probably number those problem had point between his another things find our new answer enough both 2 over possible were too why while go actually \\mathcal did system let better through right without + value power long 'd every back going down given able 'll code before 3 data ca change sure look own part image thing less create best off around high give form having either second lot means add etc must always likely small help above us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based few following said made doing non 0 trying level she day x life order page    start option simply once size > often already result come human someone edit rather little % usually specific again output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b her < state instead god & type place character control him works under last important group water list run mean until sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether itself mm range next server amount object general version consider short term target i.e. making kind useful color write below test particular class almost energy thus else camera site user got 10 common everything although needs full lower current away higher free difference thought values book original story position show others area seem low call provide name 5 easy available left big results year clear map words input    unless close true ask within actual against old language price feel top changes 4 access past game avoid length smaller source mind larger wo several believe experience tell effect difficult comes numbers days wrong design terms earth e.g. # per bad become multiple writing fine hand sound build n created further 100 three model along perhaps generally cases future gives easily research history anyone layer step ' standard support sort takes exactly error yes air turn return necessary   humans body whole known men characters product uses money sometimes planet seen running allow\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6637 - accuracy: 0.6431 - val_loss: 0.6535 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6531 - accuracy: 0.6625 - val_loss: 0.6481 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6486 - accuracy: 0.6631 - val_loss: 0.6445 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6452 - accuracy: 0.6606 - val_loss: 0.6361 - val_accuracy: 0.6673s - loss: 0.6440 - ac\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6426 - accuracy: 0.6525 - val_loss: 0.6307 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6414 - accuracy: 0.6438 - val_loss: 0.6279 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6264 - accuracy: 0.6656 - val_loss: 0.6175 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6242 - accuracy: 0.6600 - val_loss: 0.6126 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6222 - accuracy: 0.6513 - val_loss: 0.6058 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6146 - accuracy: 0.6562 - val_loss: 0.5984 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6039 - accuracy: 0.6694 - val_loss: 0.5986 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6058 - accuracy: 0.6562 - val_loss: 0.5864 - val_accuracy: 0.6714\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5938 - accuracy: 0.6831 - val_loss: 0.5803 - val_accuracy: 0.6673 accu\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5959 - accuracy: 0.6606 - val_loss: 0.5756 - val_accuracy: 0.7319\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5851 - accuracy: 0.7119 - val_loss: 0.5678 - val_accuracy: 0.6855\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5803 - accuracy: 0.6925 - val_loss: 0.5625 - val_accuracy: 0.6895\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5835 - accuracy: 0.6919 - val_loss: 0.5593 - val_accuracy: 0.7621\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5780 - accuracy: 0.7056 - val_loss: 0.5530 - val_accuracy: 0.7500\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5756 - accuracy: 0.7269 - val_loss: 0.5486 - val_accuracy: 0.7621\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5668 - accuracy: 0.7287 - val_loss: 0.5520 - val_accuracy: 0.6774\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5658 - accuracy: 0.7275 - val_loss: 0.5419 - val_accuracy: 0.7177\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5623 - accuracy: 0.7344 - val_loss: 0.5382 - val_accuracy: 0.7319\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5550 - accuracy: 0.7375 - val_loss: 0.5343 - val_accuracy: 0.7742\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5579 - accuracy: 0.7469 - val_loss: 0.5305 - val_accuracy: 0.7621\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5556 - accuracy: 0.7381 - val_loss: 0.5275 - val_accuracy: 0.7722\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5558 - accuracy: 0.7381 - val_loss: 0.5248 - val_accuracy: 0.7641\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5456 - accuracy: 0.7538 - val_loss: 0.5223 - val_accuracy: 0.7641\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5587 - accuracy: 0.7350 - val_loss: 0.5210 - val_accuracy: 0.7641\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5576 - accuracy: 0.7450 - val_loss: 0.5182 - val_accuracy: 0.7742\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5483 - accuracy: 0.7400 - val_loss: 0.5164 - val_accuracy: 0.7722\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.2354 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0168s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.2275 - accuracy: 0.4252\n",
      "training acc:  0.7400000095367432 , training loss:  0.5483265519142151 , val acc:  0.7721773982048035 , val loss:  0.5163773894309998 , test acc:  0.4252307116985321 , test loss:  1.2274936437606812\n",
      "\n",
      "opt2\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt2/50word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 1.1721 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_train_batch_end` time: 0.0263s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6963 - accuracy: 0.6500 - val_loss: 0.6614 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6741 - accuracy: 0.6413 - val_loss: 0.6613 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6606 - accuracy: 0.6525 - val_loss: 0.6586 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6586 - accuracy: 0.6550 - val_loss: 0.6458 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6540 - accuracy: 0.6531 - val_loss: 0.6419 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6555 - accuracy: 0.6500 - val_loss: 0.6403 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6474 - accuracy: 0.6538 - val_loss: 0.6359 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6373 - accuracy: 0.6612 - val_loss: 0.6303 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6377 - accuracy: 0.6562 - val_loss: 0.6276 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6330 - accuracy: 0.6606 - val_loss: 0.6216 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6318 - accuracy: 0.6538 - val_loss: 0.6374 - val_accuracy: 0.7016\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6294 - accuracy: 0.6581 - val_loss: 0.6303 - val_accuracy: 0.6875\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6213 - accuracy: 0.6631 - val_loss: 0.6091 - val_accuracy: 0.6673\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6186 - accuracy: 0.6531 - val_loss: 0.6047 - val_accuracy: 0.6673\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6165 - accuracy: 0.6575 - val_loss: 0.6015 - val_accuracy: 0.6673\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6091 - accuracy: 0.6612 - val_loss: 0.5987 - val_accuracy: 0.6673\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6130 - accuracy: 0.6538 - val_loss: 0.5957 - val_accuracy: 0.6673\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6040 - accuracy: 0.6587 - val_loss: 0.5881 - val_accuracy: 0.6673\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6046 - accuracy: 0.6625 - val_loss: 0.5849 - val_accuracy: 0.6673\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5983 - accuracy: 0.6544 - val_loss: 0.5799 - val_accuracy: 0.6673\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5894 - accuracy: 0.6675 - val_loss: 0.5825 - val_accuracy: 0.7480\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5847 - accuracy: 0.6669 - val_loss: 0.5765 - val_accuracy: 0.7298\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5936 - accuracy: 0.6906 - val_loss: 0.5787 - val_accuracy: 0.6673\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5908 - accuracy: 0.6744 - val_loss: 0.5671 - val_accuracy: 0.6673\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5749 - accuracy: 0.6675 - val_loss: 0.5631 - val_accuracy: 0.6673\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5777 - accuracy: 0.6956 - val_loss: 0.5579 - val_accuracy: 0.7056\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5707 - accuracy: 0.7131 - val_loss: 0.5609 - val_accuracy: 0.6673\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5826 - accuracy: 0.6837 - val_loss: 0.5524 - val_accuracy: 0.6694\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5735 - accuracy: 0.7025 - val_loss: 0.5533 - val_accuracy: 0.7903\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5629 - accuracy: 0.7138 - val_loss: 0.5448 - val_accuracy: 0.7440\n",
      "  1/596 [..............................] - ETA: 8s - loss: 1.1112 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0129s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.1102 - accuracy: 0.4219\n",
      "training acc:  0.7137500047683716 , training loss:  0.5629283785820007 , val acc:  0.7439516186714172 , val loss:  0.5448095798492432 , test acc:  0.421875 , test loss:  1.1102237701416016\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt2/100word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6960 - accuracy: 0.6513 - val_loss: 0.6633 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6668 - accuracy: 0.6550 - val_loss: 0.6598 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6644 - accuracy: 0.6513 - val_loss: 0.6542 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6547 - accuracy: 0.6625 - val_loss: 0.6497 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6552 - accuracy: 0.6556 - val_loss: 0.6456 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6474 - accuracy: 0.6625 - val_loss: 0.6416 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6531 - accuracy: 0.6456 - val_loss: 0.6378 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6490 - accuracy: 0.6438 - val_loss: 0.6335 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6402 - accuracy: 0.6569 - val_loss: 0.6289 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6441 - accuracy: 0.6425 - val_loss: 0.6245 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6314 - accuracy: 0.6587 - val_loss: 0.6194 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6267 - accuracy: 0.6581 - val_loss: 0.6145 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6237 - accuracy: 0.6581 - val_loss: 0.6095 - val_accuracy: 0.6673\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6187 - accuracy: 0.6513 - val_loss: 0.6078 - val_accuracy: 0.6673\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6173 - accuracy: 0.6525 - val_loss: 0.5988 - val_accuracy: 0.6673\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6070 - accuracy: 0.6606 - val_loss: 0.5957 - val_accuracy: 0.6673\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6114 - accuracy: 0.6488 - val_loss: 0.5886 - val_accuracy: 0.6673\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5975 - accuracy: 0.6587 - val_loss: 0.5831 - val_accuracy: 0.6673\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5947 - accuracy: 0.6681 - val_loss: 0.5784 - val_accuracy: 0.6673\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5951 - accuracy: 0.6587 - val_loss: 0.5773 - val_accuracy: 0.7278\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5850 - accuracy: 0.6950 - val_loss: 0.5686 - val_accuracy: 0.6694\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5850 - accuracy: 0.6869 - val_loss: 0.5632 - val_accuracy: 0.6694\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5744 - accuracy: 0.6675 - val_loss: 0.5583 - val_accuracy: 0.6714\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5778 - accuracy: 0.6831 - val_loss: 0.5546 - val_accuracy: 0.7278\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5661 - accuracy: 0.7156 - val_loss: 0.5493 - val_accuracy: 0.7198\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5635 - accuracy: 0.7013 - val_loss: 0.5455 - val_accuracy: 0.6855\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5610 - accuracy: 0.7200 - val_loss: 0.5427 - val_accuracy: 0.6815\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5586 - accuracy: 0.7113 - val_loss: 0.5415 - val_accuracy: 0.7984\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5645 - accuracy: 0.7275 - val_loss: 0.5341 - val_accuracy: 0.7722\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5501 - accuracy: 0.7425 - val_loss: 0.5297 - val_accuracy: 0.7681\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.1422 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0155s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.1529 - accuracy: 0.4264\n",
      "training acc:  0.7425000071525574 , training loss:  0.5500649213790894 , val acc:  0.7681451439857483 , val loss:  0.5296711325645447 , test acc:  0.42638424038887024 , test loss:  1.152942180633545\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt2/150word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most know me such = each something good different question really 1 now example well used its being 've am been however after 're case still set take say since many try who probably number those problem had point between his another things find our new answer enough both\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 1.0910 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_end` time: 0.0539s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6802 - accuracy: 0.6469 - val_loss: 0.6667 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6643 - accuracy: 0.6550 - val_loss: 0.6554 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6602 - accuracy: 0.6569 - val_loss: 0.6518 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6556 - accuracy: 0.6525 - val_loss: 0.6484 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6550 - accuracy: 0.6500 - val_loss: 0.6484 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6518 - accuracy: 0.6506 - val_loss: 0.6408 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6498 - accuracy: 0.6500 - val_loss: 0.6351 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6427 - accuracy: 0.6525 - val_loss: 0.6302 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6386 - accuracy: 0.6531 - val_loss: 0.6264 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6334 - accuracy: 0.6594 - val_loss: 0.6214 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6250 - accuracy: 0.6662 - val_loss: 0.6184 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6274 - accuracy: 0.6556 - val_loss: 0.6183 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6173 - accuracy: 0.6600 - val_loss: 0.6087 - val_accuracy: 0.6673\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6187 - accuracy: 0.6538 - val_loss: 0.6039 - val_accuracy: 0.6673\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6153 - accuracy: 0.6544 - val_loss: 0.5983 - val_accuracy: 0.6673\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6078 - accuracy: 0.6569 - val_loss: 0.5964 - val_accuracy: 0.6673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6125 - accuracy: 0.6450 - val_loss: 0.5891 - val_accuracy: 0.6673\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6014 - accuracy: 0.6562 - val_loss: 0.5865 - val_accuracy: 0.6714\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5996 - accuracy: 0.6762 - val_loss: 0.5811 - val_accuracy: 0.6673\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5933 - accuracy: 0.6606 - val_loss: 0.5773 - val_accuracy: 0.7117\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5919 - accuracy: 0.6744 - val_loss: 0.5714 - val_accuracy: 0.6673\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5922 - accuracy: 0.6762 - val_loss: 0.5738 - val_accuracy: 0.7944\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5829 - accuracy: 0.7025 - val_loss: 0.5617 - val_accuracy: 0.6673\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5819 - accuracy: 0.6794 - val_loss: 0.5579 - val_accuracy: 0.7480\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5752 - accuracy: 0.7225 - val_loss: 0.5527 - val_accuracy: 0.6714\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5691 - accuracy: 0.7013 - val_loss: 0.5480 - val_accuracy: 0.6996ss: 0 - ETA: 0s - loss: 0.5691 - accuracy: 0.70\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5700 - accuracy: 0.6938 - val_loss: 0.5438 - val_accuracy: 0.7359\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5670 - accuracy: 0.7244 - val_loss: 0.5402 - val_accuracy: 0.7621\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5526 - accuracy: 0.7406 - val_loss: 0.5395 - val_accuracy: 0.6815\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5604 - accuracy: 0.7150 - val_loss: 0.5342 - val_accuracy: 0.7117\n",
      "  1/596 [..............................] - ETA: 8s - loss: 1.1918 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_test_batch_end` time: 0.0193s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.1541 - accuracy: 0.4193\n",
      "training acc:  0.7149999737739563 , training loss:  0.5604211688041687 , val acc:  0.711693525314331 , val loss:  0.534162163734436 , test acc:  0.4192533493041992 , test loss:  1.1541380882263184\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt2/200word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most know me such = each something good different question really 1 now example well used its being 've am been however after 're case still set take say since many try who probably number those problem had point between his another things find our new answer enough both 2 over possible were too why while go actually \\mathcal did system let better through right without + value power long 'd every back going down given able 'll code before 3 data ca change sure look own part image thing less create best off around high give form having\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6808 - accuracy: 0.5950 - val_loss: 0.6510 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6470 - accuracy: 0.6687 - val_loss: 0.6436 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6528 - accuracy: 0.6481 - val_loss: 0.6391 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6414 - accuracy: 0.6650 - val_loss: 0.6335 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6386 - accuracy: 0.6569 - val_loss: 0.6294 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6392 - accuracy: 0.6525 - val_loss: 0.6268 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6410 - accuracy: 0.6344 - val_loss: 0.6259 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6233 - accuracy: 0.6656 - val_loss: 0.6135 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6207 - accuracy: 0.6581 - val_loss: 0.6058 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6114 - accuracy: 0.6650 - val_loss: 0.6028 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6069 - accuracy: 0.6644 - val_loss: 0.5957 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6096 - accuracy: 0.6525 - val_loss: 0.5882 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5920 - accuracy: 0.6675 - val_loss: 0.5847 - val_accuracy: 0.6774\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5927 - accuracy: 0.6756 - val_loss: 0.5811 - val_accuracy: 0.6673\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5920 - accuracy: 0.6662 - val_loss: 0.5719 - val_accuracy: 0.6673\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5828 - accuracy: 0.6812 - val_loss: 0.5677 - val_accuracy: 0.6673\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5835 - accuracy: 0.6775 - val_loss: 0.5593 - val_accuracy: 0.6915\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5789 - accuracy: 0.6994 - val_loss: 0.5571 - val_accuracy: 0.6673\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5688 - accuracy: 0.7063 - val_loss: 0.5561 - val_accuracy: 0.7964\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5716 - accuracy: 0.7100 - val_loss: 0.5598 - val_accuracy: 0.7843\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5660 - accuracy: 0.7344 - val_loss: 0.5427 - val_accuracy: 0.7923\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5622 - accuracy: 0.7331 - val_loss: 0.5345 - val_accuracy: 0.7440\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5559 - accuracy: 0.7344 - val_loss: 0.5352 - val_accuracy: 0.6815\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5573 - accuracy: 0.7375 - val_loss: 0.5297 - val_accuracy: 0.8024\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5520 - accuracy: 0.7588 - val_loss: 0.5226 - val_accuracy: 0.7540\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5438 - accuracy: 0.7531 - val_loss: 0.5208 - val_accuracy: 0.8024\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5395 - accuracy: 0.7656 - val_loss: 0.5286 - val_accuracy: 0.7722\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5363 - accuracy: 0.7563 - val_loss: 0.5122 - val_accuracy: 0.7984\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5434 - accuracy: 0.7456 - val_loss: 0.5134 - val_accuracy: 0.8044\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5340 - accuracy: 0.7613 - val_loss: 0.5058 - val_accuracy: 0.7802\n",
      "  1/596 [..............................] - ETA: 3s - loss: 1.3273 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0201s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 15s 25ms/step - loss: 1.3200 - accuracy: 0.4292\n",
      "training acc:  0.7612500190734863 , training loss:  0.5340379476547241 , val acc:  0.7802419066429138 , val loss:  0.505821943283081 , test acc:  0.42921561002731323 , test loss:  1.3200150728225708\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt2/250word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most know me such = each something good different question really 1 now example well used its being 've am been however after 're case still set take say since many try who probably number those problem had point between his another things find our new answer enough both 2 over possible were too why while go actually \\mathcal did system let better through right without + value power long 'd every back going down given able 'll code before 3 data ca change sure look own part image thing less create best off around high give form having either second lot means add etc must always likely small help above us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based few following said made doing non 0 trying level she day x life order\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.8758 - accuracy: 0.5938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_train_batch_end` time: 0.0395s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6882 - accuracy: 0.6506 - val_loss: 0.6623 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6584 - accuracy: 0.6675 - val_loss: 0.6579 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6632 - accuracy: 0.6531 - val_loss: 0.6528 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6611 - accuracy: 0.6525 - val_loss: 0.6477 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6544 - accuracy: 0.6531 - val_loss: 0.6505 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6477 - accuracy: 0.6569 - val_loss: 0.6382 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6475 - accuracy: 0.6556 - val_loss: 0.6354 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6410 - accuracy: 0.6544 - val_loss: 0.6289 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6397 - accuracy: 0.6500 - val_loss: 0.6237 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6328 - accuracy: 0.6575 - val_loss: 0.6189 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6308 - accuracy: 0.6475 - val_loss: 0.6154 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6243 - accuracy: 0.6550 - val_loss: 0.6094 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6203 - accuracy: 0.6544 - val_loss: 0.6045 - val_accuracy: 0.6673\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6169 - accuracy: 0.6481 - val_loss: 0.5998 - val_accuracy: 0.6673\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6090 - accuracy: 0.6581 - val_loss: 0.5954 - val_accuracy: 0.6673\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6078 - accuracy: 0.6519 - val_loss: 0.5894 - val_accuracy: 0.6673\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5986 - accuracy: 0.6587 - val_loss: 0.5856 - val_accuracy: 0.6694\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6054 - accuracy: 0.6531 - val_loss: 0.5817 - val_accuracy: 0.6875\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5915 - accuracy: 0.6706 - val_loss: 0.5763 - val_accuracy: 0.6673\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5919 - accuracy: 0.6806 - val_loss: 0.5760 - val_accuracy: 0.7762\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5880 - accuracy: 0.7094 - val_loss: 0.5665 - val_accuracy: 0.6673\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5880 - accuracy: 0.6712 - val_loss: 0.5612 - val_accuracy: 0.7097\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5788 - accuracy: 0.7038 - val_loss: 0.5564 - val_accuracy: 0.6815\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5809 - accuracy: 0.6881 - val_loss: 0.5525 - val_accuracy: 0.6875\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5780 - accuracy: 0.7063 - val_loss: 0.5485 - val_accuracy: 0.7097\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5679 - accuracy: 0.7356 - val_loss: 0.5445 - val_accuracy: 0.7319\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5611 - accuracy: 0.7188 - val_loss: 0.5455 - val_accuracy: 0.8004\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5576 - accuracy: 0.7406 - val_loss: 0.5450 - val_accuracy: 0.8004\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5561 - accuracy: 0.7500 - val_loss: 0.5388 - val_accuracy: 0.6754\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5603 - accuracy: 0.7325 - val_loss: 0.5428 - val_accuracy: 0.6694\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.3226 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0177s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.2415 - accuracy: 0.4124\n",
      "training acc:  0.7325000166893005 , training loss:  0.5602794885635376 , val acc:  0.6693548560142517 , val loss:  0.5427945256233215 , test acc:  0.4124370813369751 , test loss:  1.241456151008606\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt2/300word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most know me such = each something good different question really 1 now example well used its being 've am been however after 're case still set take say since many try who probably number those problem had point between his another things find our new answer enough both 2 over possible were too why while go actually \\mathcal did system let better through right without + value power long 'd every back going down given able 'll code before 3 data ca change sure look own part image thing less create best off around high give form having either second lot means add etc must always likely small help above us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based few following said made doing non 0 trying level she day x life order page    start option simply once size > often already result come human someone edit rather little % usually specific again output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.8421 - accuracy: 0.6562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0429s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.7168 - accuracy: 0.6481 - val_loss: 0.6683 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6631 - accuracy: 0.6631 - val_loss: 0.6638 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6567 - accuracy: 0.6662 - val_loss: 0.6588 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6640 - accuracy: 0.6525 - val_loss: 0.6536 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6656 - accuracy: 0.6413 - val_loss: 0.6484 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6485 - accuracy: 0.6631 - val_loss: 0.6439 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6493 - accuracy: 0.6544 - val_loss: 0.6417 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6454 - accuracy: 0.6556 - val_loss: 0.6372 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6427 - accuracy: 0.6488 - val_loss: 0.6312 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6400 - accuracy: 0.6513 - val_loss: 0.6249 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6338 - accuracy: 0.6519 - val_loss: 0.6216 - val_accuracy: 0.6673acy: 0.\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6273 - accuracy: 0.6575 - val_loss: 0.6183 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6288 - accuracy: 0.6444 - val_loss: 0.6098 - val_accuracy: 0.6673\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6197 - accuracy: 0.6525 - val_loss: 0.6040 - val_accuracy: 0.6673\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6163 - accuracy: 0.6506 - val_loss: 0.5982 - val_accuracy: 0.6673\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6076 - accuracy: 0.6569 - val_loss: 0.5952 - val_accuracy: 0.6694\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6016 - accuracy: 0.6737 - val_loss: 0.5935 - val_accuracy: 0.6673\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6025 - accuracy: 0.6525 - val_loss: 0.5814 - val_accuracy: 0.6673\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5920 - accuracy: 0.6594 - val_loss: 0.5835 - val_accuracy: 0.7742\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5875 - accuracy: 0.6994 - val_loss: 0.5736 - val_accuracy: 0.6673\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5907 - accuracy: 0.6850 - val_loss: 0.5651 - val_accuracy: 0.6714\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5796 - accuracy: 0.6706 - val_loss: 0.5606 - val_accuracy: 0.6694\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5851 - accuracy: 0.6900 - val_loss: 0.5562 - val_accuracy: 0.7379\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5784 - accuracy: 0.7113 - val_loss: 0.5504 - val_accuracy: 0.7319\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5727 - accuracy: 0.7350 - val_loss: 0.5461 - val_accuracy: 0.6855\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5661 - accuracy: 0.7312 - val_loss: 0.5438 - val_accuracy: 0.6754\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5620 - accuracy: 0.7200 - val_loss: 0.5384 - val_accuracy: 0.6935\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5573 - accuracy: 0.7306 - val_loss: 0.5333 - val_accuracy: 0.7863\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5526 - accuracy: 0.7525 - val_loss: 0.5280 - val_accuracy: 0.7762\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5483 - accuracy: 0.7513 - val_loss: 0.5254 - val_accuracy: 0.7984\n",
      "  1/596 [..............................] - ETA: 3s - loss: 1.1426 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0131s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.1385 - accuracy: 0.4287\n",
      "training acc:  0.7512500286102295 , training loss:  0.5483335256576538 , val acc:  0.7983871102333069 , val loss:  0.5254360437393188 , test acc:  0.4286912679672241 , test loss:  1.1384965181350708\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt2/350word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most know me such = each something good different question really 1 now example well used its being 've am been however after 're case still set take say since many try who probably number those problem had point between his another things find our new answer enough both 2 over possible were too why while go actually \\mathcal did system let better through right without + value power long 'd every back going down given able 'll code before 3 data ca change sure look own part image thing less create best off around high give form having either second lot means add etc must always likely small help above us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based few following said made doing non 0 trying level she day x life order page    start option simply once size > often already result come human someone edit rather little % usually specific again output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b her < state instead god & type place character control him works under last important group water list run mean until sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether itself mm range next server amount object general version consider\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7575 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_train_batch_end` time: 0.0213s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6565 - accuracy: 0.6681 - val_loss: 0.6576 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6614 - accuracy: 0.6513 - val_loss: 0.6494 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6565 - accuracy: 0.6494 - val_loss: 0.6436 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6510 - accuracy: 0.6506 - val_loss: 0.6395 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6433 - accuracy: 0.6587 - val_loss: 0.6322 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6430 - accuracy: 0.6463 - val_loss: 0.6265 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6333 - accuracy: 0.6575 - val_loss: 0.6211 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6298 - accuracy: 0.6569 - val_loss: 0.6183 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6312 - accuracy: 0.6425 - val_loss: 0.6098 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6266 - accuracy: 0.6475 - val_loss: 0.6044 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6224 - accuracy: 0.6406 - val_loss: 0.5989 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6108 - accuracy: 0.6544 - val_loss: 0.5936 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6056 - accuracy: 0.6587 - val_loss: 0.5877 - val_accuracy: 0.6673\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6025 - accuracy: 0.6606 - val_loss: 0.5840 - val_accuracy: 0.6673\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6018 - accuracy: 0.6581 - val_loss: 0.5771 - val_accuracy: 0.6694\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5965 - accuracy: 0.6675 - val_loss: 0.5746 - val_accuracy: 0.7238\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5849 - accuracy: 0.6881 - val_loss: 0.5762 - val_accuracy: 0.7964\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5840 - accuracy: 0.6806 - val_loss: 0.5625 - val_accuracy: 0.7137\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5791 - accuracy: 0.7044 - val_loss: 0.5571 - val_accuracy: 0.7198\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5783 - accuracy: 0.7044 - val_loss: 0.5518 - val_accuracy: 0.6875\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5661 - accuracy: 0.7100 - val_loss: 0.5528 - val_accuracy: 0.6694\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5665 - accuracy: 0.7231 - val_loss: 0.5450 - val_accuracy: 0.6734\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5616 - accuracy: 0.7337 - val_loss: 0.5383 - val_accuracy: 0.7621\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5608 - accuracy: 0.7256 - val_loss: 0.5369 - val_accuracy: 0.6915\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5506 - accuracy: 0.7406 - val_loss: 0.5363 - val_accuracy: 0.6815\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5401 - accuracy: 0.7444 - val_loss: 0.5258 - val_accuracy: 0.7641\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5429 - accuracy: 0.7675 - val_loss: 0.5223 - val_accuracy: 0.7802\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5364 - accuracy: 0.7675 - val_loss: 0.5198 - val_accuracy: 0.8004: 0.538\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5438 - accuracy: 0.7775 - val_loss: 0.5167 - val_accuracy: 0.7540\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5432 - accuracy: 0.7619 - val_loss: 0.5128 - val_accuracy: 0.8044\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.2512 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0145s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.2718 - accuracy: 0.4291\n",
      "training acc:  0.7618749737739563 , training loss:  0.5432398319244385 , val acc:  0.8044354915618896 , val loss:  0.512809157371521 , test acc:  0.42911073565483093 , test loss:  1.2717540264129639\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt2/400word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most know me such = each something good different question really 1 now example well used its being 've am been however after 're case still set take say since many try who probably number those problem had point between his another things find our new answer enough both 2 over possible were too why while go actually \\mathcal did system let better through right without + value power long 'd every back going down given able 'll code before 3 data ca change sure look own part image thing less create best off around high give form having either second lot means add etc must always likely small help above us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based few following said made doing non 0 trying level she day x life order page    start option simply once size > often already result come human someone edit rather little % usually specific again output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b her < state instead god & type place character control him works under last important group water list run mean until sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether itself mm range next server amount object general version consider short term target i.e. making kind useful color write below test particular class almost energy thus else camera site user got 10 common everything although needs full lower current away higher free difference thought values book original story position show others area seem low call provide name 5 easy available\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7926 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0158s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6664 - accuracy: 0.6269 - val_loss: 0.6517 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6574 - accuracy: 0.6494 - val_loss: 0.6491 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6541 - accuracy: 0.6494 - val_loss: 0.6399 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6461 - accuracy: 0.6575 - val_loss: 0.6363 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6472 - accuracy: 0.6444 - val_loss: 0.6330 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6416 - accuracy: 0.6488 - val_loss: 0.6235 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6313 - accuracy: 0.6575 - val_loss: 0.6188 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6287 - accuracy: 0.6513 - val_loss: 0.6116 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6201 - accuracy: 0.6581 - val_loss: 0.6058 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6171 - accuracy: 0.6544 - val_loss: 0.6042 - val_accuracy: 0.6694\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6126 - accuracy: 0.6500 - val_loss: 0.5964 - val_accuracy: 0.6694\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6039 - accuracy: 0.6769 - val_loss: 0.5885 - val_accuracy: 0.6694\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6060 - accuracy: 0.6681 - val_loss: 0.5829 - val_accuracy: 0.6673\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5975 - accuracy: 0.6812 - val_loss: 0.5815 - val_accuracy: 0.7581\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5948 - accuracy: 0.6725 - val_loss: 0.5729 - val_accuracy: 0.7339\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5851 - accuracy: 0.6906 - val_loss: 0.5644 - val_accuracy: 0.6835\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5812 - accuracy: 0.6938 - val_loss: 0.5601 - val_accuracy: 0.6694\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5828 - accuracy: 0.6969 - val_loss: 0.5591 - val_accuracy: 0.7984\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5733 - accuracy: 0.7175 - val_loss: 0.5530 - val_accuracy: 0.6694\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5717 - accuracy: 0.7138 - val_loss: 0.5443 - val_accuracy: 0.7702\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5633 - accuracy: 0.7306 - val_loss: 0.5390 - val_accuracy: 0.7097\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5546 - accuracy: 0.7294 - val_loss: 0.5341 - val_accuracy: 0.7742\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5607 - accuracy: 0.7387 - val_loss: 0.5300 - val_accuracy: 0.7339\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5499 - accuracy: 0.7513 - val_loss: 0.5293 - val_accuracy: 0.7097\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5447 - accuracy: 0.7450 - val_loss: 0.5217 - val_accuracy: 0.7984\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5378 - accuracy: 0.7650 - val_loss: 0.5189 - val_accuracy: 0.7440\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5481 - accuracy: 0.7513 - val_loss: 0.5215 - val_accuracy: 0.8065\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5379 - accuracy: 0.7644 - val_loss: 0.5108 - val_accuracy: 0.7722\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5276 - accuracy: 0.7756 - val_loss: 0.5079 - val_accuracy: 0.8105\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5205 - accuracy: 0.7681 - val_loss: 0.5028 - val_accuracy: 0.7923\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.3411 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0145s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.3363 - accuracy: 0.4289\n",
      "training acc:  0.7681249976158142 , training loss:  0.5205263495445251 , val acc:  0.7923387289047241 , val loss:  0.5028416514396667 , test acc:  0.4289010167121887 , test loss:  1.3362921476364136\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt2/450word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most know me such = each something good different question really 1 now example well used its being 've am been however after 're case still set take say since many try who probably number those problem had point between his another things find our new answer enough both 2 over possible were too why while go actually \\mathcal did system let better through right without + value power long 'd every back going down given able 'll code before 3 data ca change sure look own part image thing less create best off around high give form having either second lot means add etc must always likely small help above us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based few following said made doing non 0 trying level she day x life order page    start option simply once size > often already result come human someone edit rather little % usually specific again output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b her < state instead god & type place character control him works under last important group water list run mean until sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether itself mm range next server amount object general version consider short term target i.e. making kind useful color write below test particular class almost energy thus else camera site user got 10 common everything although needs full lower current away higher free difference thought values book original story position show others area seem low call provide name 5 easy available left big results year clear map words input    unless close true ask within actual against old language price feel top changes 4 access past game avoid length smaller source mind larger wo several believe experience tell effect difficult comes numbers days wrong design terms earth e.g. # per bad\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.8842 - accuracy: 0.5312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0323s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6731 - accuracy: 0.6587 - val_loss: 0.6611 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6711 - accuracy: 0.6425 - val_loss: 0.6551 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6522 - accuracy: 0.6631 - val_loss: 0.6493 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6538 - accuracy: 0.6525 - val_loss: 0.6442 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6445 - accuracy: 0.6600 - val_loss: 0.6358 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6467 - accuracy: 0.6481 - val_loss: 0.6300 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.6343 - accuracy: 0.6575 - val_loss: 0.6250 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6303 - accuracy: 0.6594 - val_loss: 0.6174 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6301 - accuracy: 0.6456 - val_loss: 0.6108 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6141 - accuracy: 0.6669 - val_loss: 0.6075 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6163 - accuracy: 0.6550 - val_loss: 0.5987 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6101 - accuracy: 0.6513 - val_loss: 0.5916 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5982 - accuracy: 0.6587 - val_loss: 0.5857 - val_accuracy: 0.6694\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6002 - accuracy: 0.6725 - val_loss: 0.5779 - val_accuracy: 0.6694\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5966 - accuracy: 0.6725 - val_loss: 0.5750 - val_accuracy: 0.7440\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5850 - accuracy: 0.6694 - val_loss: 0.5651 - val_accuracy: 0.7056\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5808 - accuracy: 0.6762 - val_loss: 0.5619 - val_accuracy: 0.7722\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5715 - accuracy: 0.7113 - val_loss: 0.5599 - val_accuracy: 0.8105\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5652 - accuracy: 0.7362 - val_loss: 0.5494 - val_accuracy: 0.6714\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5601 - accuracy: 0.7075 - val_loss: 0.5416 - val_accuracy: 0.7722\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5586 - accuracy: 0.7544 - val_loss: 0.5351 - val_accuracy: 0.7581\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5565 - accuracy: 0.7362 - val_loss: 0.5325 - val_accuracy: 0.8024\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5485 - accuracy: 0.7606 - val_loss: 0.5250 - val_accuracy: 0.7702\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5444 - accuracy: 0.7644 - val_loss: 0.5211 - val_accuracy: 0.7560\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5480 - accuracy: 0.7544 - val_loss: 0.5164 - val_accuracy: 0.7964\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5403 - accuracy: 0.7631 - val_loss: 0.5122 - val_accuracy: 0.7964\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5300 - accuracy: 0.7663 - val_loss: 0.5183 - val_accuracy: 0.7198\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5374 - accuracy: 0.7538 - val_loss: 0.5075 - val_accuracy: 0.8125\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5230 - accuracy: 0.7912 - val_loss: 0.5019 - val_accuracy: 0.7762\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5287 - accuracy: 0.7594 - val_loss: 0.4981 - val_accuracy: 0.8105\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.2737 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0148s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.2459 - accuracy: 0.4294\n",
      "training acc:  0.7593749761581421 , training loss:  0.5287138819694519 , val acc:  0.8104838728904724 , val loss:  0.49809664487838745 , test acc:  0.42942532896995544 , test loss:  1.2458910942077637\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt2/500word_list.txt\n",
      "WORD INDEX STR: the , . to a  of and is in that you i it $ for be this with   as on are have if or not can but would 's your an n't so do from they will at there by we one my which what more all some was when then like has use / about also just could any time only no get other than how them up out their need does   should make way - where even because very same work these want first two into people using think might 'm here see may he much most know me such = each something good different question really 1 now example well used its being 've am been however after 're case still set take say since many try who probably number those problem had point between his another things find our new answer enough both 2 over possible were too why while go actually \\mathcal did system let better through right without + value power long 'd every back going down given able 'll code before 3 data ca change sure look own part image thing less create best off around high give form having either second lot means add etc must always likely small help above us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based few following said made doing non 0 trying level she day x life order page    start option simply once size > often already result come human someone edit rather little % usually specific again output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b her < state instead god & type place character control him works under last important group water list run mean until sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether itself mm range next server amount object general version consider short term target i.e. making kind useful color write below test particular class almost energy thus else camera site user got 10 common everything although needs full lower current away higher free difference thought values book original story position show others area seem low call provide name 5 easy available left big results year clear map words input    unless close true ask within actual against old language price feel top changes 4 access past game avoid length smaller source mind larger wo several believe experience tell effect difficult comes numbers days wrong design terms earth e.g. # per bad become multiple writing fine hand sound build n created further 100 three model along perhaps generally cases future gives easily research history anyone layer step ' standard support sort takes exactly error yes air turn return necessary   humans body whole known men characters product uses money sometimes planet seen\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6793 - accuracy: 0.6012 - val_loss: 0.6524 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6578 - accuracy: 0.6519 - val_loss: 0.6465 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6559 - accuracy: 0.6438 - val_loss: 0.6533 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6494 - accuracy: 0.6569 - val_loss: 0.6377 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6429 - accuracy: 0.6581 - val_loss: 0.6331 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6326 - accuracy: 0.6687 - val_loss: 0.6272 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6411 - accuracy: 0.6425 - val_loss: 0.6215 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6294 - accuracy: 0.6600 - val_loss: 0.6173 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6279 - accuracy: 0.6525 - val_loss: 0.6152 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6240 - accuracy: 0.6581 - val_loss: 0.6060 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6246 - accuracy: 0.6444 - val_loss: 0.6031 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6092 - accuracy: 0.6650 - val_loss: 0.5999 - val_accuracy: 0.6694\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6115 - accuracy: 0.6506 - val_loss: 0.5921 - val_accuracy: 0.6673\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.6095 - accuracy: 0.6444 - val_loss: 0.5889 - val_accuracy: 0.6774\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5996 - accuracy: 0.6625 - val_loss: 0.5819 - val_accuracy: 0.6694\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5983 - accuracy: 0.6706 - val_loss: 0.5755 - val_accuracy: 0.6673\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5932 - accuracy: 0.6656 - val_loss: 0.5723 - val_accuracy: 0.7097\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.5837 - accuracy: 0.6812 - val_loss: 0.5669 - val_accuracy: 0.7117\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5828 - accuracy: 0.6812 - val_loss: 0.5614 - val_accuracy: 0.7097\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5797 - accuracy: 0.7044 - val_loss: 0.5642 - val_accuracy: 0.6673\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5736 - accuracy: 0.7013 - val_loss: 0.5520 - val_accuracy: 0.6956\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5708 - accuracy: 0.7094 - val_loss: 0.5474 - val_accuracy: 0.7298\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5671 - accuracy: 0.7281 - val_loss: 0.5431 - val_accuracy: 0.7097\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5689 - accuracy: 0.7219 - val_loss: 0.5402 - val_accuracy: 0.6976\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5635 - accuracy: 0.7294 - val_loss: 0.5376 - val_accuracy: 0.7984\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5610 - accuracy: 0.7375 - val_loss: 0.5347 - val_accuracy: 0.8065\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5507 - accuracy: 0.7481 - val_loss: 0.5276 - val_accuracy: 0.7802\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5478 - accuracy: 0.7563 - val_loss: 0.5238 - val_accuracy: 0.7742\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5472 - accuracy: 0.7513 - val_loss: 0.5216 - val_accuracy: 0.7440\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.5378 - accuracy: 0.7556 - val_loss: 0.5242 - val_accuracy: 0.8024\n",
      "  1/596 [..............................] - ETA: 8s - loss: 1.1735 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0143s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 1.2280 - accuracy: 0.4283\n",
      "training acc:  0.7556250095367432 , training loss:  0.5377840995788574 , val acc:  0.8024193644523621 , val loss:  0.5241997241973877 , test acc:  0.4282718002796173 , test loss:  1.2279603481292725\n",
      "\n",
      "opt3\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt3/50word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.8193 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0108s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.7384 - accuracy: 0.6419 - val_loss: 0.6872 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6701 - accuracy: 0.6612 - val_loss: 0.6544 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6488 - accuracy: 0.6506 - val_loss: 0.6300 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6331 - accuracy: 0.6438 - val_loss: 0.6126 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6159 - accuracy: 0.6612 - val_loss: 0.5998 - val_accuracy: 0.6694\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6191 - accuracy: 0.6569 - val_loss: 0.5893 - val_accuracy: 0.6956\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6031 - accuracy: 0.6719 - val_loss: 0.5832 - val_accuracy: 0.6694\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6065 - accuracy: 0.6712 - val_loss: 0.5768 - val_accuracy: 0.6976\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5966 - accuracy: 0.6712 - val_loss: 0.5830 - val_accuracy: 0.6734\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5958 - accuracy: 0.6737 - val_loss: 0.5692 - val_accuracy: 0.7137\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5928 - accuracy: 0.6794 - val_loss: 0.5678 - val_accuracy: 0.7077\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5945 - accuracy: 0.6712 - val_loss: 0.5656 - val_accuracy: 0.7177\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6053 - accuracy: 0.6644 - val_loss: 0.5639 - val_accuracy: 0.7198\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5970 - accuracy: 0.6825 - val_loss: 0.5659 - val_accuracy: 0.6996\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5997 - accuracy: 0.6700 - val_loss: 0.5647 - val_accuracy: 0.7117\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5956 - accuracy: 0.6875 - val_loss: 0.5620 - val_accuracy: 0.7218\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5972 - accuracy: 0.6856 - val_loss: 0.5647 - val_accuracy: 0.6996ss: 0.5984 - accura\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5976 - accuracy: 0.6731 - val_loss: 0.5610 - val_accuracy: 0.7218\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5898 - accuracy: 0.6844 - val_loss: 0.5606 - val_accuracy: 0.7238\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5879 - accuracy: 0.6769 - val_loss: 0.5654 - val_accuracy: 0.6996\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5982 - accuracy: 0.6656 - val_loss: 0.5594 - val_accuracy: 0.7157\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5878 - accuracy: 0.6925 - val_loss: 0.5599 - val_accuracy: 0.7238\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5949 - accuracy: 0.6794 - val_loss: 0.5588 - val_accuracy: 0.7157\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5980 - accuracy: 0.6831 - val_loss: 0.5602 - val_accuracy: 0.7198\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5937 - accuracy: 0.6831 - val_loss: 0.5584 - val_accuracy: 0.7157\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5844 - accuracy: 0.6869 - val_loss: 0.5591 - val_accuracy: 0.7238\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5888 - accuracy: 0.6944 - val_loss: 0.5584 - val_accuracy: 0.7238\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5945 - accuracy: 0.6819 - val_loss: 0.5592 - val_accuracy: 0.7218\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.1873 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0133s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9305 - accuracy: 0.4408\n",
      "training acc:  0.6818749904632568 , training loss:  0.5944936275482178 , val acc:  0.7217742204666138 , val loss:  0.5592281222343445 , test acc:  0.4407508373260498 , test loss:  0.9305087924003601\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt3/100word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6864 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0024s vs `on_train_batch_end` time: 0.0108s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6757 - accuracy: 0.6100 - val_loss: 0.6596 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6513 - accuracy: 0.6612 - val_loss: 0.6364 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6389 - accuracy: 0.6394 - val_loss: 0.6165 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6186 - accuracy: 0.6837 - val_loss: 0.5998 - val_accuracy: 0.7077\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6144 - accuracy: 0.6625 - val_loss: 0.5895 - val_accuracy: 0.6653\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6032 - accuracy: 0.6637 - val_loss: 0.5771 - val_accuracy: 0.7077\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6101 - accuracy: 0.6531 - val_loss: 0.5725 - val_accuracy: 0.70566118 - accura\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5926 - accuracy: 0.6862 - val_loss: 0.5693 - val_accuracy: 0.7016\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6045 - accuracy: 0.6544 - val_loss: 0.5634 - val_accuracy: 0.7278\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5915 - accuracy: 0.6819 - val_loss: 0.5605 - val_accuracy: 0.7359\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5851 - accuracy: 0.6931 - val_loss: 0.5577 - val_accuracy: 0.7218\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6081 - accuracy: 0.6650 - val_loss: 0.5569 - val_accuracy: 0.7359\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5955 - accuracy: 0.6731 - val_loss: 0.5589 - val_accuracy: 0.7056\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6132 - accuracy: 0.6519 - val_loss: 0.5567 - val_accuracy: 0.7319\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6045 - accuracy: 0.6681 - val_loss: 0.5567 - val_accuracy: 0.7238\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5983 - accuracy: 0.6669 - val_loss: 0.5558 - val_accuracy: 0.7218\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5941 - accuracy: 0.6800 - val_loss: 0.5601 - val_accuracy: 0.7016\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5974 - accuracy: 0.6581 - val_loss: 0.5558 - val_accuracy: 0.7198\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5937 - accuracy: 0.6687 - val_loss: 0.5564 - val_accuracy: 0.7077\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5952 - accuracy: 0.6606 - val_loss: 0.5574 - val_accuracy: 0.7077\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5911 - accuracy: 0.6806 - val_loss: 0.5538 - val_accuracy: 0.7399\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5983 - accuracy: 0.6850 - val_loss: 0.5536 - val_accuracy: 0.7238\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5945 - accuracy: 0.6831 - val_loss: 0.5524 - val_accuracy: 0.7298\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5842 - accuracy: 0.6938 - val_loss: 0.5520 - val_accuracy: 0.7218\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5960 - accuracy: 0.6662 - val_loss: 0.5519 - val_accuracy: 0.7218\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5911 - accuracy: 0.6794 - val_loss: 0.5518 - val_accuracy: 0.7359\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.5842 - accuracy: 0.6875 - val_loss: 0.5519 - val_accuracy: 0.7198\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5953 - accuracy: 0.6750 - val_loss: 0.5509 - val_accuracy: 0.7379\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6001 - accuracy: 0.6731 - val_loss: 0.5519 - val_accuracy: 0.7218\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6033 - accuracy: 0.6737 - val_loss: 0.5524 - val_accuracy: 0.7238\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9173 - accuracy: 0.4424\n",
      "training acc:  0.6737499833106995 , training loss:  0.6033065319061279 , val acc:  0.7237903475761414 , val loss:  0.5523746013641357 , test acc:  0.44242867827415466 , test loss:  0.9172905087471008\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt3/150word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high give form either second lot means add etc must always likely small help us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based following said made non 0 trying level day x life order page   \n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.9624 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0249s vs `on_train_batch_end` time: 0.0393s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.7177 - accuracy: 0.6381 - val_loss: 0.6831 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6663 - accuracy: 0.6612 - val_loss: 0.6489 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6439 - accuracy: 0.6587 - val_loss: 0.6239 - val_accuracy: 0.6774\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6289 - accuracy: 0.6556 - val_loss: 0.6060 - val_accuracy: 0.6633\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6129 - accuracy: 0.6694 - val_loss: 0.5924 - val_accuracy: 0.6714\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6099 - accuracy: 0.6769 - val_loss: 0.5837 - val_accuracy: 0.6774\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6083 - accuracy: 0.6787 - val_loss: 0.5858 - val_accuracy: 0.6754\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6038 - accuracy: 0.6531 - val_loss: 0.5689 - val_accuracy: 0.7319\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5860 - accuracy: 0.6906 - val_loss: 0.5684 - val_accuracy: 0.6996\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6081 - accuracy: 0.6706 - val_loss: 0.5616 - val_accuracy: 0.7177\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5985 - accuracy: 0.6762 - val_loss: 0.5607 - val_accuracy: 0.7097\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6013 - accuracy: 0.6675 - val_loss: 0.5590 - val_accuracy: 0.7177\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6007 - accuracy: 0.6737 - val_loss: 0.5575 - val_accuracy: 0.7157\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6011 - accuracy: 0.6675 - val_loss: 0.5573 - val_accuracy: 0.7157\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5895 - accuracy: 0.6781 - val_loss: 0.5591 - val_accuracy: 0.7036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6016 - accuracy: 0.6650 - val_loss: 0.5570 - val_accuracy: 0.7077\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5987 - accuracy: 0.6725 - val_loss: 0.5579 - val_accuracy: 0.7016\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5976 - accuracy: 0.6662 - val_loss: 0.5553 - val_accuracy: 0.7157\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5919 - accuracy: 0.6750 - val_loss: 0.5541 - val_accuracy: 0.7399\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5966 - accuracy: 0.6687 - val_loss: 0.5539 - val_accuracy: 0.7177\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5881 - accuracy: 0.6787 - val_loss: 0.5525 - val_accuracy: 0.7278\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5941 - accuracy: 0.6750 - val_loss: 0.5570 - val_accuracy: 0.7036\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5947 - accuracy: 0.6731 - val_loss: 0.5523 - val_accuracy: 0.7218\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5945 - accuracy: 0.6819 - val_loss: 0.5518 - val_accuracy: 0.7379\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5922 - accuracy: 0.6825 - val_loss: 0.5536 - val_accuracy: 0.7097\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5895 - accuracy: 0.6737 - val_loss: 0.5514 - val_accuracy: 0.7319\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5932 - accuracy: 0.6844 - val_loss: 0.5542 - val_accuracy: 0.7056\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5952 - accuracy: 0.6737 - val_loss: 0.5524 - val_accuracy: 0.7177\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6001 - accuracy: 0.6650 - val_loss: 0.5526 - val_accuracy: 0.7540\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.1909 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0148s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.8904 - accuracy: 0.4541\n",
      "training acc:  0.6650000214576721 , training loss:  0.6001459360122681 , val acc:  0.7540322542190552 , val loss:  0.5525959134101868 , test acc:  0.45406877994537354 , test loss:  0.890415370464325\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt3/200word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high give form either second lot means add etc must always likely small help us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based following said made non 0 trying level day x life order page    start option simply size > often already result come human someone edit rather little % usually specific output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b < state instead god\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6727 - accuracy: 0.6156 - val_loss: 0.6352 - val_accuracy: 0.6734\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6347 - accuracy: 0.6556 - val_loss: 0.6121 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6205 - accuracy: 0.6544 - val_loss: 0.5904 - val_accuracy: 0.6734\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6090 - accuracy: 0.6675 - val_loss: 0.5780 - val_accuracy: 0.6794\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6010 - accuracy: 0.6750 - val_loss: 0.5690 - val_accuracy: 0.6956\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6029 - accuracy: 0.6694 - val_loss: 0.5623 - val_accuracy: 0.7056\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6050 - accuracy: 0.6719 - val_loss: 0.5639 - val_accuracy: 0.6915\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5994 - accuracy: 0.6700 - val_loss: 0.5556 - val_accuracy: 0.7218\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6089 - accuracy: 0.6538 - val_loss: 0.5553 - val_accuracy: 0.7218\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5959 - accuracy: 0.6762 - val_loss: 0.5532 - val_accuracy: 0.7298\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5869 - accuracy: 0.6794 - val_loss: 0.5542 - val_accuracy: 0.7097\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6068 - accuracy: 0.6594 - val_loss: 0.5514 - val_accuracy: 0.7440\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5872 - accuracy: 0.6831 - val_loss: 0.5509 - val_accuracy: 0.7157\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6022 - accuracy: 0.6662 - val_loss: 0.5517 - val_accuracy: 0.7056\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5994 - accuracy: 0.6700 - val_loss: 0.5493 - val_accuracy: 0.7298\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6076 - accuracy: 0.6744 - val_loss: 0.5507 - val_accuracy: 0.7641\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5979 - accuracy: 0.6687 - val_loss: 0.5499 - val_accuracy: 0.7218\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5937 - accuracy: 0.6631 - val_loss: 0.5493 - val_accuracy: 0.7520\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.1711 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0144s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.8919 - accuracy: 0.4490\n",
      "training acc:  0.6631249785423279 , training loss:  0.5937146544456482 , val acc:  0.7520161271095276 , val loss:  0.5493189692497253 , test acc:  0.4490352272987366 , test loss:  0.8918625712394714\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt3/250word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high give form either second lot means add etc must always likely small help us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based following said made non 0 trying level day x life order page    start option simply size > often already result come human someone edit rather little % usually specific output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b < state instead god & type place character control works last important group water list run mean sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether mm range next server amount object general version consider short term target i.e. making kind useful color write\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7119 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0107s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6938 - accuracy: 0.6587 - val_loss: 0.6678 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6527 - accuracy: 0.6644 - val_loss: 0.6351 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6341 - accuracy: 0.6525 - val_loss: 0.6103 - val_accuracy: 0.6734\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6262 - accuracy: 0.6375 - val_loss: 0.5943 - val_accuracy: 0.6815\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6089 - accuracy: 0.6562 - val_loss: 0.5820 - val_accuracy: 0.6895\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5962 - accuracy: 0.6675 - val_loss: 0.5706 - val_accuracy: 0.7077\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6032 - accuracy: 0.6644 - val_loss: 0.5660 - val_accuracy: 0.7520\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5929 - accuracy: 0.6850 - val_loss: 0.5607 - val_accuracy: 0.7077\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5938 - accuracy: 0.6750 - val_loss: 0.5583 - val_accuracy: 0.7399\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5981 - accuracy: 0.6700 - val_loss: 0.5569 - val_accuracy: 0.7097\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5871 - accuracy: 0.6756 - val_loss: 0.5544 - val_accuracy: 0.7440\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5978 - accuracy: 0.6687 - val_loss: 0.5549 - val_accuracy: 0.7056\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5898 - accuracy: 0.6850 - val_loss: 0.5542 - val_accuracy: 0.7056\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5912 - accuracy: 0.6769 - val_loss: 0.5524 - val_accuracy: 0.7177\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5874 - accuracy: 0.6900 - val_loss: 0.5521 - val_accuracy: 0.7117\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5960 - accuracy: 0.6812 - val_loss: 0.5514 - val_accuracy: 0.7560\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5910 - accuracy: 0.6787 - val_loss: 0.5571 - val_accuracy: 0.7036\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6055 - accuracy: 0.6600 - val_loss: 0.5523 - val_accuracy: 0.7177\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5955 - accuracy: 0.6619 - val_loss: 0.5518 - val_accuracy: 0.7298\n",
      "  1/596 [..............................] - ETA: 0s - loss: 1.1942 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0145s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9110 - accuracy: 0.4441\n",
      "training acc:  0.6618750095367432 , training loss:  0.5954849123954773 , val acc:  0.7298387289047241 , val loss:  0.551832914352417 , test acc:  0.4441065490245819 , test loss:  0.910999059677124\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt3/300word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high give form either second lot means add etc must always likely small help us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based following said made non 0 trying level day x life order page    start option simply size > often already result come human someone edit rather little % usually specific output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b < state instead god & type place character control works last important group water list run mean sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether mm range next server amount object general version consider short term target i.e. making kind useful color write test particular class almost energy thus else camera site user got 10 common everything although needs full lower current away higher free difference thought values book original story position show others area seem low call provide name 5 easy available left big results year clear map words input    unless\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6750 - accuracy: 0.6194 - val_loss: 0.6567 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6478 - accuracy: 0.6581 - val_loss: 0.6291 - val_accuracy: 0.6694\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6300 - accuracy: 0.6625 - val_loss: 0.6083 - val_accuracy: 0.6633\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6181 - accuracy: 0.6594 - val_loss: 0.5932 - val_accuracy: 0.6855\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6113 - accuracy: 0.6544 - val_loss: 0.5881 - val_accuracy: 0.6653\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6071 - accuracy: 0.6544 - val_loss: 0.5763 - val_accuracy: 0.7097\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6067 - accuracy: 0.6594 - val_loss: 0.5716 - val_accuracy: 0.7077\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5955 - accuracy: 0.6719 - val_loss: 0.5689 - val_accuracy: 0.7036\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5931 - accuracy: 0.6650 - val_loss: 0.5660 - val_accuracy: 0.7016\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5908 - accuracy: 0.6669 - val_loss: 0.5630 - val_accuracy: 0.6996\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5944 - accuracy: 0.6631 - val_loss: 0.5665 - val_accuracy: 0.6855\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6052 - accuracy: 0.6637 - val_loss: 0.5593 - val_accuracy: 0.7097\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6014 - accuracy: 0.6712 - val_loss: 0.5584 - val_accuracy: 0.7097\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5935 - accuracy: 0.6675 - val_loss: 0.5571 - val_accuracy: 0.7157\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5868 - accuracy: 0.6869 - val_loss: 0.5561 - val_accuracy: 0.7097\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5929 - accuracy: 0.6850 - val_loss: 0.5555 - val_accuracy: 0.7097\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5905 - accuracy: 0.6731 - val_loss: 0.5542 - val_accuracy: 0.7319\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5940 - accuracy: 0.6775 - val_loss: 0.5567 - val_accuracy: 0.7016\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5846 - accuracy: 0.6819 - val_loss: 0.5551 - val_accuracy: 0.7056\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6053 - accuracy: 0.6706 - val_loss: 0.5554 - val_accuracy: 0.7056\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.2111 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0009s vs `on_test_batch_end` time: 0.0064s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9202 - accuracy: 0.4342\n",
      "training acc:  0.6706249713897705 , training loss:  0.605311393737793 , val acc:  0.7056451439857483 , val loss:  0.5554163455963135 , test acc:  0.4342491626739502 , test loss:  0.9201626181602478\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt3/350word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high give form either second lot means add etc must always likely small help us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based following said made non 0 trying level day x life order page    start option simply size > often already result come human someone edit rather little % usually specific output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b < state instead god & type place character control works last important group water list run mean sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether mm range next server amount object general version consider short term target i.e. making kind useful color write test particular class almost energy thus else camera site user got 10 common everything although needs full lower current away higher free difference thought values book original story position show others area seem low call provide name 5 easy available left big results year clear map words input    unless close true ask within actual old language price feel top changes 4 access past game avoid length smaller source mind larger wo several believe experience tell effect difficult comes numbers days wrong design terms earth e.g. # per bad become multiple writing fine hand sound build n created 100 three\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6714 - accuracy: 0.6231 - val_loss: 0.6419 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6395 - accuracy: 0.6500 - val_loss: 0.6096 - val_accuracy: 0.7056\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6140 - accuracy: 0.6662 - val_loss: 0.5899 - val_accuracy: 0.6714\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6138 - accuracy: 0.6550 - val_loss: 0.5767 - val_accuracy: 0.7198\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6001 - accuracy: 0.6669 - val_loss: 0.5686 - val_accuracy: 0.7056\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5968 - accuracy: 0.6637 - val_loss: 0.5636 - val_accuracy: 0.7056\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6051 - accuracy: 0.6556 - val_loss: 0.5610 - val_accuracy: 0.7198\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5967 - accuracy: 0.6656 - val_loss: 0.5605 - val_accuracy: 0.7077\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5972 - accuracy: 0.6675 - val_loss: 0.5608 - val_accuracy: 0.7016\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5867 - accuracy: 0.6794 - val_loss: 0.5556 - val_accuracy: 0.7157\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5891 - accuracy: 0.6756 - val_loss: 0.5595 - val_accuracy: 0.7621\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6002 - accuracy: 0.6681 - val_loss: 0.5547 - val_accuracy: 0.7097\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6104 - accuracy: 0.6656 - val_loss: 0.5583 - val_accuracy: 0.7036\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5971 - accuracy: 0.6669 - val_loss: 0.5550 - val_accuracy: 0.7097\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5898 - accuracy: 0.6744 - val_loss: 0.5536 - val_accuracy: 0.7319\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5925 - accuracy: 0.6800 - val_loss: 0.5559 - val_accuracy: 0.7056\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5952 - accuracy: 0.6781 - val_loss: 0.5529 - val_accuracy: 0.7440\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5932 - accuracy: 0.6794 - val_loss: 0.5537 - val_accuracy: 0.7077\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5929 - accuracy: 0.6787 - val_loss: 0.5529 - val_accuracy: 0.7198\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5947 - accuracy: 0.6844 - val_loss: 0.5552 - val_accuracy: 0.7056\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.2368 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0155s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.9332 - accuracy: 0.4305\n",
      "training acc:  0.684374988079071 , training loss:  0.5947002172470093 , val acc:  0.7056451439857483 , val loss:  0.5551849007606506 , test acc:  0.4304739832878113 , test loss:  0.9332374930381775\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt3/400word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high give form either second lot means add etc must always likely small help us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based following said made non 0 trying level day x life order page    start option simply size > often already result come human someone edit rather little % usually specific output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b < state instead god & type place character control works last important group water list run mean sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether mm range next server amount object general version consider short term target i.e. making kind useful color write test particular class almost energy thus else camera site user got 10 common everything although needs full lower current away higher free difference thought values book original story position show others area seem low call provide name 5 easy available left big results year clear map words input    unless close true ask within actual old language price feel top changes 4 access past game avoid length smaller source mind larger wo several believe experience tell effect difficult comes numbers days wrong design terms earth e.g. # per bad become multiple writing fine hand sound build n created 100 three model along perhaps generally cases future gives easily research history anyone layer step ' standard support sort takes exactly error yes air turn return necessary   humans body whole known men characters product uses money sometimes planet seen running allow company certain ability problems play situation stop cause taking normal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6919 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0212s vs `on_train_batch_end` time: 0.0448s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6706 - accuracy: 0.6306 - val_loss: 0.6394 - val_accuracy: 0.6976\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6357 - accuracy: 0.6531 - val_loss: 0.6113 - val_accuracy: 0.6714\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6166 - accuracy: 0.6550 - val_loss: 0.5927 - val_accuracy: 0.6734\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6112 - accuracy: 0.6694 - val_loss: 0.5894 - val_accuracy: 0.6613\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6044 - accuracy: 0.6519 - val_loss: 0.5747 - val_accuracy: 0.6935\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6056 - accuracy: 0.6694 - val_loss: 0.5708 - val_accuracy: 0.6915\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5949 - accuracy: 0.6800 - val_loss: 0.5707 - val_accuracy: 0.6835\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5940 - accuracy: 0.6794 - val_loss: 0.5614 - val_accuracy: 0.7077\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5939 - accuracy: 0.6744 - val_loss: 0.5652 - val_accuracy: 0.6915\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5943 - accuracy: 0.6719 - val_loss: 0.5608 - val_accuracy: 0.7016\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5958 - accuracy: 0.6750 - val_loss: 0.5570 - val_accuracy: 0.7137\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5925 - accuracy: 0.6762 - val_loss: 0.5567 - val_accuracy: 0.7560\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5864 - accuracy: 0.6875 - val_loss: 0.5581 - val_accuracy: 0.7016\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5954 - accuracy: 0.6656 - val_loss: 0.5549 - val_accuracy: 0.7278\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6030 - accuracy: 0.6806 - val_loss: 0.5578 - val_accuracy: 0.7016\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5894 - accuracy: 0.6812 - val_loss: 0.5557 - val_accuracy: 0.7077\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5934 - accuracy: 0.6750 - val_loss: 0.5567 - val_accuracy: 0.7016\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.2011 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0136s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.9087 - accuracy: 0.4309\n",
      "training acc:  0.675000011920929 , training loss:  0.5934197902679443 , val acc:  0.7016128897666931 , val loss:  0.5566556453704834 , test acc:  0.4308934509754181 , test loss:  0.9086716175079346\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt3/450word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high give form either second lot means add etc must always likely small help us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based following said made non 0 trying level day x life order page    start option simply size > often already result come human someone edit rather little % usually specific output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b < state instead god & type place character control works last important group water list run mean sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether mm range next server amount object general version consider short term target i.e. making kind useful color write test particular class almost energy thus else camera site user got 10 common everything although needs full lower current away higher free difference thought values book original story position show others area seem low call provide name 5 easy available left big results year clear map words input    unless close true ask within actual old language price feel top changes 4 access past game avoid length smaller source mind larger wo several believe experience tell effect difficult comes numbers days wrong design terms earth e.g. # per bad become multiple writing fine hand sound build n created 100 three model along perhaps generally cases future gives easily research history anyone layer step ' standard support sort takes exactly error yes air turn return necessary   humans body whole known men characters product uses money sometimes planet seen running allow company certain ability problems play situation stop cause taking normal move related knowledge food theory approach half setting magic main search happen inside sequence \\ nothing speed remember    whatever job tried children examples flash reading context english gets wine post says slightly 6 project options structure choose ground select needed total yet computer quality exact view directly weapons \\\\\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6858 - accuracy: 0.6569 - val_loss: 0.6606 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6461 - accuracy: 0.6675 - val_loss: 0.6238 - val_accuracy: 0.6734\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6315 - accuracy: 0.6494 - val_loss: 0.6013 - val_accuracy: 0.7056\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6069 - accuracy: 0.6631 - val_loss: 0.5850 - val_accuracy: 0.7036\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6062 - accuracy: 0.6656 - val_loss: 0.5754 - val_accuracy: 0.7036\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5994 - accuracy: 0.6706 - val_loss: 0.5723 - val_accuracy: 0.6935\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6055 - accuracy: 0.6644 - val_loss: 0.5647 - val_accuracy: 0.7319\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5968 - accuracy: 0.6619 - val_loss: 0.5620 - val_accuracy: 0.7319\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6068 - accuracy: 0.6731 - val_loss: 0.5600 - val_accuracy: 0.7097\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6004 - accuracy: 0.6606 - val_loss: 0.5593 - val_accuracy: 0.7117\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6026 - accuracy: 0.6625 - val_loss: 0.5583 - val_accuracy: 0.7319\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6009 - accuracy: 0.6775 - val_loss: 0.5642 - val_accuracy: 0.6935\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6021 - accuracy: 0.6569 - val_loss: 0.5574 - val_accuracy: 0.7077\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5944 - accuracy: 0.6769 - val_loss: 0.5571 - val_accuracy: 0.7056\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5954 - accuracy: 0.6850 - val_loss: 0.5549 - val_accuracy: 0.7198\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5998 - accuracy: 0.6644 - val_loss: 0.5566 - val_accuracy: 0.7036\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5965 - accuracy: 0.6687 - val_loss: 0.5554 - val_accuracy: 0.7056\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5935 - accuracy: 0.6731 - val_loss: 0.5560 - val_accuracy: 0.7601\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.0774 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_test_batch_end` time: 0.0063s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.8650 - accuracy: 0.4506\n",
      "training acc:  0.6731250286102295 , training loss:  0.5935018062591553 , val acc:  0.7600806355476379 , val loss:  0.5560077428817749 , test acc:  0.4506082236766815 , test loss:  0.8649514317512512\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt3/500word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high give form either second lot means add etc must always likely small help us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based following said made non 0 trying level day x life order page    start option simply size > often already result come human someone edit rather little % usually specific output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b < state instead god & type place character control works last important group water list run mean sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether mm range next server amount object general version consider short term target i.e. making kind useful color write test particular class almost energy thus else camera site user got 10 common everything although needs full lower current away higher free difference thought values book original story position show others area seem low call provide name 5 easy available left big results year clear map words input    unless close true ask within actual old language price feel top changes 4 access past game avoid length smaller source mind larger wo several believe experience tell effect difficult comes numbers days wrong design terms earth e.g. # per bad become multiple writing fine hand sound build n created 100 three model along perhaps generally cases future gives easily research history anyone layer step ' standard support sort takes exactly error yes air turn return necessary   humans body whole known men characters product uses money sometimes planet seen running allow company certain ability problems play situation stop cause taking normal move related knowledge food theory approach half setting magic main search happen inside sequence \\ nothing speed remember    whatever job tried children examples flash reading context english gets wine post says slightly 6 project options structure choose ground select needed total yet computer quality exact view directly weapons \\\\ therefore noise explanation rate major points fire outside personal ones added meaning produce completely weapon rules reasons guess link elements thanks assuming block later otherwise functions require risk depends n$ states distance de pressure mode black google zero cost looks entire questions focus    mentioned exist force perfect women head\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.6885 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_train_batch_end` time: 0.0105s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6702 - accuracy: 0.6438 - val_loss: 0.6369 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6307 - accuracy: 0.6587 - val_loss: 0.6045 - val_accuracy: 0.6956\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6139 - accuracy: 0.6631 - val_loss: 0.5838 - val_accuracy: 0.7117\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6015 - accuracy: 0.6650 - val_loss: 0.5731 - val_accuracy: 0.7097\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6076 - accuracy: 0.6569 - val_loss: 0.5669 - val_accuracy: 0.7117\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5941 - accuracy: 0.6712 - val_loss: 0.5630 - val_accuracy: 0.7480\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5942 - accuracy: 0.6769 - val_loss: 0.5594 - val_accuracy: 0.7177\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5905 - accuracy: 0.6737 - val_loss: 0.5655 - val_accuracy: 0.6875\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5977 - accuracy: 0.6737 - val_loss: 0.5574 - val_accuracy: 0.7097\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6002 - accuracy: 0.6744 - val_loss: 0.5578 - val_accuracy: 0.7077\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6011 - accuracy: 0.6625 - val_loss: 0.5572 - val_accuracy: 0.7560\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5872 - accuracy: 0.7000 - val_loss: 0.5619 - val_accuracy: 0.6956\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5981 - accuracy: 0.6600 - val_loss: 0.5559 - val_accuracy: 0.7581\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5982 - accuracy: 0.6756 - val_loss: 0.5539 - val_accuracy: 0.7339\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5901 - accuracy: 0.6825 - val_loss: 0.5547 - val_accuracy: 0.7601\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5950 - accuracy: 0.6881 - val_loss: 0.5577 - val_accuracy: 0.6996\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6058 - accuracy: 0.6669 - val_loss: 0.5574 - val_accuracy: 0.7016\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.1804 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0134s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.9219 - accuracy: 0.4274\n",
      "training acc:  0.6668750047683716 , training loss:  0.6057723164558411 , val acc:  0.7016128897666931 , val loss:  0.5573519468307495 , test acc:  0.4274328947067261 , test loss:  0.9219064116477966\n",
      "\n",
      "opt4\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt4/50word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 1.1157 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0114s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 64ms/step - loss: 0.7160 - accuracy: 0.5806 - val_loss: 0.6472 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6516 - accuracy: 0.6600 - val_loss: 0.6422 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6501 - accuracy: 0.6519 - val_loss: 0.6376 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6477 - accuracy: 0.6463 - val_loss: 0.6319 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6420 - accuracy: 0.6513 - val_loss: 0.6315 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6435 - accuracy: 0.6388 - val_loss: 0.6219 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6354 - accuracy: 0.6500 - val_loss: 0.6187 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6294 - accuracy: 0.6531 - val_loss: 0.6141 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6239 - accuracy: 0.6575 - val_loss: 0.6128 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6227 - accuracy: 0.6506 - val_loss: 0.6031 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6149 - accuracy: 0.6519 - val_loss: 0.6030 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6094 - accuracy: 0.6575 - val_loss: 0.5940 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6103 - accuracy: 0.6650 - val_loss: 0.5913 - val_accuracy: 0.6673\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6002 - accuracy: 0.6650 - val_loss: 0.5862 - val_accuracy: 0.6673\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6048 - accuracy: 0.6500 - val_loss: 0.5814 - val_accuracy: 0.6673\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6066 - accuracy: 0.6737 - val_loss: 0.5773 - val_accuracy: 0.6673\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5926 - accuracy: 0.6687 - val_loss: 0.5752 - val_accuracy: 0.6673\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5912 - accuracy: 0.6625 - val_loss: 0.5815 - val_accuracy: 0.7480\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5868 - accuracy: 0.6787 - val_loss: 0.5679 - val_accuracy: 0.6673\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5840 - accuracy: 0.6812 - val_loss: 0.5669 - val_accuracy: 0.7157\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5848 - accuracy: 0.6794 - val_loss: 0.5600 - val_accuracy: 0.6694\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5785 - accuracy: 0.6800 - val_loss: 0.5566 - val_accuracy: 0.6673\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5734 - accuracy: 0.6988 - val_loss: 0.5555 - val_accuracy: 0.7117\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5825 - accuracy: 0.6806 - val_loss: 0.5514 - val_accuracy: 0.6956\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5760 - accuracy: 0.6850 - val_loss: 0.5481 - val_accuracy: 0.6815\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.5729 - accuracy: 0.6988 - val_loss: 0.5489 - val_accuracy: 0.7540\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5697 - accuracy: 0.7069 - val_loss: 0.5433 - val_accuracy: 0.7117\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5759 - accuracy: 0.6925 - val_loss: 0.5417 - val_accuracy: 0.7379\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5620 - accuracy: 0.6969 - val_loss: 0.5392 - val_accuracy: 0.7379\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5553 - accuracy: 0.7050 - val_loss: 0.5443 - val_accuracy: 0.7560\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.1332 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0144s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.0369 - accuracy: 0.4381\n",
      "training acc:  0.7049999833106995 , training loss:  0.555274248123169 , val acc:  0.7560483813285828 , val loss:  0.5442736744880676 , test acc:  0.438129186630249 , test loss:  1.036939024925232\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt4/100word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 1s - loss: 0.6937 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0116s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.6713 - accuracy: 0.6369 - val_loss: 0.6550 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6560 - accuracy: 0.6587 - val_loss: 0.6487 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6462 - accuracy: 0.6656 - val_loss: 0.6406 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6491 - accuracy: 0.6488 - val_loss: 0.6348 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6433 - accuracy: 0.6544 - val_loss: 0.6275 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6387 - accuracy: 0.6525 - val_loss: 0.6216 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6288 - accuracy: 0.6562 - val_loss: 0.6212 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6289 - accuracy: 0.6469 - val_loss: 0.6270 - val_accuracy: 0.7560\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6251 - accuracy: 0.6606 - val_loss: 0.6031 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6109 - accuracy: 0.6637 - val_loss: 0.5973 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6179 - accuracy: 0.6538 - val_loss: 0.6011 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6071 - accuracy: 0.6538 - val_loss: 0.5861 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6010 - accuracy: 0.6581 - val_loss: 0.5808 - val_accuracy: 0.6673\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6011 - accuracy: 0.6781 - val_loss: 0.5755 - val_accuracy: 0.6673\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5927 - accuracy: 0.6775 - val_loss: 0.5720 - val_accuracy: 0.6694\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5813 - accuracy: 0.6700 - val_loss: 0.5659 - val_accuracy: 0.6673\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5873 - accuracy: 0.6737 - val_loss: 0.5620 - val_accuracy: 0.6673\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5827 - accuracy: 0.6825 - val_loss: 0.5608 - val_accuracy: 0.6673\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5774 - accuracy: 0.6731 - val_loss: 0.5560 - val_accuracy: 0.7560\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5704 - accuracy: 0.6775 - val_loss: 0.5497 - val_accuracy: 0.7137\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5710 - accuracy: 0.6956 - val_loss: 0.5466 - val_accuracy: 0.6694\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5718 - accuracy: 0.7119 - val_loss: 0.5433 - val_accuracy: 0.6694\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5687 - accuracy: 0.6819 - val_loss: 0.5387 - val_accuracy: 0.7258\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5674 - accuracy: 0.7081 - val_loss: 0.5399 - val_accuracy: 0.7823\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5531 - accuracy: 0.7163 - val_loss: 0.5337 - val_accuracy: 0.6815\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5591 - accuracy: 0.7319 - val_loss: 0.5298 - val_accuracy: 0.7097\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5632 - accuracy: 0.7063 - val_loss: 0.5285 - val_accuracy: 0.6976\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5642 - accuracy: 0.7231 - val_loss: 0.5269 - val_accuracy: 0.7762\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5480 - accuracy: 0.7437 - val_loss: 0.5252 - val_accuracy: 0.7843\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5491 - accuracy: 0.7325 - val_loss: 0.5204 - val_accuracy: 0.7641\n",
      "  1/596 [..............................] - ETA: 4s - loss: 1.4705 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_test_batch_end` time: 0.0179s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.1954 - accuracy: 0.4313\n",
      "training acc:  0.7325000166893005 , training loss:  0.549059271812439 , val acc:  0.7641128897666931 , val loss:  0.5203588008880615 , test acc:  0.4313129186630249 , test loss:  1.1954340934753418\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt4/150word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high give form either second lot means add etc must always likely small help us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based following said made non 0 trying level day x life order\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 1.4299 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_train_batch_end` time: 0.0105s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.7231 - accuracy: 0.6519 - val_loss: 0.6712 - val_accuracy: 0.6613\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6740 - accuracy: 0.6494 - val_loss: 0.6700 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6636 - accuracy: 0.6581 - val_loss: 0.6565 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6568 - accuracy: 0.6562 - val_loss: 0.6497 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6460 - accuracy: 0.6637 - val_loss: 0.6396 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6466 - accuracy: 0.6550 - val_loss: 0.6326 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6332 - accuracy: 0.6631 - val_loss: 0.6325 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6366 - accuracy: 0.6550 - val_loss: 0.6181 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6279 - accuracy: 0.6531 - val_loss: 0.6142 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6219 - accuracy: 0.6575 - val_loss: 0.6050 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6153 - accuracy: 0.6669 - val_loss: 0.6022 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6191 - accuracy: 0.6444 - val_loss: 0.6003 - val_accuracy: 0.7177\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6066 - accuracy: 0.6769 - val_loss: 0.5979 - val_accuracy: 0.7661\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6017 - accuracy: 0.6569 - val_loss: 0.5798 - val_accuracy: 0.6673\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5964 - accuracy: 0.6662 - val_loss: 0.5742 - val_accuracy: 0.6673\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5964 - accuracy: 0.6625 - val_loss: 0.5685 - val_accuracy: 0.6673\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5805 - accuracy: 0.6631 - val_loss: 0.5632 - val_accuracy: 0.6694\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5835 - accuracy: 0.6963 - val_loss: 0.5581 - val_accuracy: 0.6694\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5868 - accuracy: 0.6725 - val_loss: 0.5574 - val_accuracy: 0.7641\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5716 - accuracy: 0.7244 - val_loss: 0.5487 - val_accuracy: 0.6915\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5777 - accuracy: 0.6762 - val_loss: 0.5556 - val_accuracy: 0.7823\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5687 - accuracy: 0.7181 - val_loss: 0.5414 - val_accuracy: 0.7540\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5593 - accuracy: 0.7225 - val_loss: 0.5369 - val_accuracy: 0.6976\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5604 - accuracy: 0.7156 - val_loss: 0.5377 - val_accuracy: 0.7883\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5546 - accuracy: 0.7487 - val_loss: 0.5293 - val_accuracy: 0.7500\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5592 - accuracy: 0.7419 - val_loss: 0.5287 - val_accuracy: 0.7823\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5459 - accuracy: 0.7387 - val_loss: 0.5230 - val_accuracy: 0.7641\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5484 - accuracy: 0.7513 - val_loss: 0.5203 - val_accuracy: 0.7621\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5445 - accuracy: 0.7525 - val_loss: 0.5183 - val_accuracy: 0.7399\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5404 - accuracy: 0.7481 - val_loss: 0.5241 - val_accuracy: 0.7742\n",
      "  1/596 [..............................] - ETA: 5s - loss: 1.3742 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0126s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 14s 24ms/step - loss: 1.1203 - accuracy: 0.4345\n",
      "training acc:  0.7481250166893005 , training loss:  0.5404345989227295 , val acc:  0.774193525314331 , val loss:  0.52411949634552 , test acc:  0.4344588816165924 , test loss:  1.120268702507019\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt4/200word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high give form either second lot means add etc must always likely small help us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based following said made non 0 trying level day x life order page    start option simply size > often already result come human someone edit rather little % usually specific output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b < state\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.8101 - accuracy: 0.6400 - val_loss: 0.6801 - val_accuracy: 0.6331\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6759 - accuracy: 0.6475 - val_loss: 0.6732 - val_accuracy: 0.6633\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6712 - accuracy: 0.6581 - val_loss: 0.6669 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6728 - accuracy: 0.6400 - val_loss: 0.6577 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6538 - accuracy: 0.6669 - val_loss: 0.6512 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6545 - accuracy: 0.6569 - val_loss: 0.6471 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6493 - accuracy: 0.6556 - val_loss: 0.6404 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6435 - accuracy: 0.6531 - val_loss: 0.6349 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6373 - accuracy: 0.6562 - val_loss: 0.6270 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6301 - accuracy: 0.6612 - val_loss: 0.6185 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6355 - accuracy: 0.6406 - val_loss: 0.6137 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6218 - accuracy: 0.6506 - val_loss: 0.6060 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6212 - accuracy: 0.6481 - val_loss: 0.6020 - val_accuracy: 0.6673\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6141 - accuracy: 0.6731 - val_loss: 0.5970 - val_accuracy: 0.6694\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6044 - accuracy: 0.6575 - val_loss: 0.5883 - val_accuracy: 0.6694\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6009 - accuracy: 0.6600 - val_loss: 0.5813 - val_accuracy: 0.6673\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5975 - accuracy: 0.6762 - val_loss: 0.5791 - val_accuracy: 0.6673\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5919 - accuracy: 0.6775 - val_loss: 0.5696 - val_accuracy: 0.6774\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5894 - accuracy: 0.6825 - val_loss: 0.5638 - val_accuracy: 0.6895\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5793 - accuracy: 0.6681 - val_loss: 0.5574 - val_accuracy: 0.6734\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5777 - accuracy: 0.6888 - val_loss: 0.5524 - val_accuracy: 0.6835\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5623 - accuracy: 0.7131 - val_loss: 0.5522 - val_accuracy: 0.6673\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5621 - accuracy: 0.6931 - val_loss: 0.5430 - val_accuracy: 0.7077\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5630 - accuracy: 0.7125 - val_loss: 0.5439 - val_accuracy: 0.7903\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5656 - accuracy: 0.7287 - val_loss: 0.5357 - val_accuracy: 0.6915\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5639 - accuracy: 0.7194 - val_loss: 0.5307 - val_accuracy: 0.7399\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5583 - accuracy: 0.7281 - val_loss: 0.5271 - val_accuracy: 0.7520\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5545 - accuracy: 0.7431 - val_loss: 0.5248 - val_accuracy: 0.7681\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5448 - accuracy: 0.7419 - val_loss: 0.5206 - val_accuracy: 0.7500\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5466 - accuracy: 0.7344 - val_loss: 0.5174 - val_accuracy: 0.7621\n",
      "  1/596 [..............................] - ETA: 7s - loss: 1.4179 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_test_batch_end` time: 0.0230s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.1316 - accuracy: 0.4303\n",
      "training acc:  0.734375 , training loss:  0.5466419458389282 , val acc:  0.7620967626571655 , val loss:  0.5174374580383301 , test acc:  0.43026426434516907 , test loss:  1.131577968597412\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt4/250word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high give form either second lot means add etc must always likely small help us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based following said made non 0 trying level day x life order page    start option simply size > often already result come human someone edit rather little % usually specific output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b < state instead god & type place character control works last important group water list run mean sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether mm range next server amount object general version consider short term target i.e. making kind useful\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6828 - accuracy: 0.5975 - val_loss: 0.6499 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6602 - accuracy: 0.6431 - val_loss: 0.6422 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6502 - accuracy: 0.6444 - val_loss: 0.6327 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6430 - accuracy: 0.6481 - val_loss: 0.6243 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6317 - accuracy: 0.6513 - val_loss: 0.6164 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6297 - accuracy: 0.6419 - val_loss: 0.6099 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6142 - accuracy: 0.6662 - val_loss: 0.6001 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6124 - accuracy: 0.6519 - val_loss: 0.5926 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6041 - accuracy: 0.6637 - val_loss: 0.5856 - val_accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5971 - accuracy: 0.6619 - val_loss: 0.5791 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6020 - accuracy: 0.6675 - val_loss: 0.5728 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5885 - accuracy: 0.6750 - val_loss: 0.5669 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5822 - accuracy: 0.6831 - val_loss: 0.5620 - val_accuracy: 0.6895\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5809 - accuracy: 0.6756 - val_loss: 0.5561 - val_accuracy: 0.6774\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5797 - accuracy: 0.6963 - val_loss: 0.5538 - val_accuracy: 0.6673\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5685 - accuracy: 0.7081 - val_loss: 0.5577 - val_accuracy: 0.7802\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5687 - accuracy: 0.7025 - val_loss: 0.5425 - val_accuracy: 0.7419\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5635 - accuracy: 0.7400 - val_loss: 0.5392 - val_accuracy: 0.7520\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5576 - accuracy: 0.7456 - val_loss: 0.5343 - val_accuracy: 0.7480\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5638 - accuracy: 0.7200 - val_loss: 0.5389 - val_accuracy: 0.7823\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5604 - accuracy: 0.7356 - val_loss: 0.5297 - val_accuracy: 0.7782\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5562 - accuracy: 0.7362 - val_loss: 0.5259 - val_accuracy: 0.7258\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5481 - accuracy: 0.7425 - val_loss: 0.5230 - val_accuracy: 0.7802\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5515 - accuracy: 0.7369 - val_loss: 0.5192 - val_accuracy: 0.7520ETA\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5398 - accuracy: 0.7350 - val_loss: 0.5166 - val_accuracy: 0.7621\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5479 - accuracy: 0.7456 - val_loss: 0.5170 - val_accuracy: 0.7863\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5455 - accuracy: 0.7538 - val_loss: 0.5136 - val_accuracy: 0.7843\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.5441 - accuracy: 0.7487 - val_loss: 0.5100 - val_accuracy: 0.7540\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5362 - accuracy: 0.7525 - val_loss: 0.5087 - val_accuracy: 0.7520s - loss: 0.5382 - accura\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5378 - accuracy: 0.7469 - val_loss: 0.5066 - val_accuracy: 0.7540\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.3800 - accuracy: 0.4281\n",
      "training acc:  0.746874988079071 , training loss:  0.5377739071846008 , val acc:  0.7540322542190552 , val loss:  0.5065898895263672 , test acc:  0.4280620813369751 , test loss:  1.3799866437911987\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt4/300word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high give form either second lot means add etc must always likely small help us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based following said made non 0 trying level day x life order page    start option simply size > often already result come human someone edit rather little % usually specific output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b < state instead god & type place character control works last important group water list run mean sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether mm range next server amount object general version consider short term target i.e. making kind useful color write test particular class almost energy thus else camera site user got 10 common everything although needs full lower current away higher free difference thought values book original story position show others area seem low call provide name 5 easy available left big results year clear map words input\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6807 - accuracy: 0.6050 - val_loss: 0.6494 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6487 - accuracy: 0.6644 - val_loss: 0.6399 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6515 - accuracy: 0.6425 - val_loss: 0.6312 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6462 - accuracy: 0.6406 - val_loss: 0.6218 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6347 - accuracy: 0.6425 - val_loss: 0.6126 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6287 - accuracy: 0.6494 - val_loss: 0.6080 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6202 - accuracy: 0.6562 - val_loss: 0.6017 - val_accuracy: 0.6754\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6085 - accuracy: 0.6644 - val_loss: 0.5890 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6024 - accuracy: 0.6625 - val_loss: 0.5831 - val_accuracy: 0.6774\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5968 - accuracy: 0.6737 - val_loss: 0.5771 - val_accuracy: 0.7177\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5844 - accuracy: 0.6781 - val_loss: 0.5684 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5868 - accuracy: 0.6806 - val_loss: 0.5613 - val_accuracy: 0.6875\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5815 - accuracy: 0.6913 - val_loss: 0.5551 - val_accuracy: 0.6694\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5795 - accuracy: 0.6888 - val_loss: 0.5566 - val_accuracy: 0.7863\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5726 - accuracy: 0.6906 - val_loss: 0.5450 - val_accuracy: 0.7218\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5652 - accuracy: 0.7119 - val_loss: 0.5403 - val_accuracy: 0.7258\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5653 - accuracy: 0.7188 - val_loss: 0.5581 - val_accuracy: 0.6673\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5606 - accuracy: 0.7206 - val_loss: 0.5380 - val_accuracy: 0.7843\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5610 - accuracy: 0.7312 - val_loss: 0.5278 - val_accuracy: 0.7460\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5529 - accuracy: 0.7331 - val_loss: 0.5239 - val_accuracy: 0.7520\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5472 - accuracy: 0.7556 - val_loss: 0.5212 - val_accuracy: 0.7440\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5428 - accuracy: 0.7406 - val_loss: 0.5175 - val_accuracy: 0.7520\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5510 - accuracy: 0.7456 - val_loss: 0.5203 - val_accuracy: 0.7742\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5410 - accuracy: 0.7506 - val_loss: 0.5122 - val_accuracy: 0.7560\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5411 - accuracy: 0.7475 - val_loss: 0.5116 - val_accuracy: 0.7903\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5480 - accuracy: 0.7450 - val_loss: 0.5080 - val_accuracy: 0.7520\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5394 - accuracy: 0.7387 - val_loss: 0.5193 - val_accuracy: 0.7440\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5388 - accuracy: 0.7475 - val_loss: 0.5109 - val_accuracy: 0.7722\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5346 - accuracy: 0.7538 - val_loss: 0.5021 - val_accuracy: 0.7802\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5287 - accuracy: 0.7575 - val_loss: 0.5159 - val_accuracy: 0.7480\n",
      "  1/596 [..............................] - ETA: 3s - loss: 1.6284 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_test_batch_end` time: 0.0160s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.3345 - accuracy: 0.4369\n",
      "training acc:  0.7574999928474426 , training loss:  0.5286912322044373 , val acc:  0.7479838728904724 , val loss:  0.5158903002738953 , test acc:  0.436870813369751 , test loss:  1.3344519138336182\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt4/350word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high give form either second lot means add etc must always likely small help us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based following said made non 0 trying level day x life order page    start option simply size > often already result come human someone edit rather little % usually specific output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b < state instead god & type place character control works last important group water list run mean sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether mm range next server amount object general version consider short term target i.e. making kind useful color write test particular class almost energy thus else camera site user got 10 common everything although needs full lower current away higher free difference thought values book original story position show others area seem low call provide name 5 easy available left big results year clear map words input    unless close true ask within actual old language price feel top changes 4 access past game avoid length smaller source mind larger wo several believe experience tell effect difficult comes numbers days wrong design terms earth e.g. # per bad become multiple writing fine hand sound build n created\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.5120 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0105s). Check your callbacks.\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.7173 - accuracy: 0.6488 - val_loss: 0.6808 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6750 - accuracy: 0.6494 - val_loss: 0.6644 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6599 - accuracy: 0.6606 - val_loss: 0.6532 - val_accuracy: 0.6673uracy: 0.\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.6574 - accuracy: 0.6488 - val_loss: 0.6423 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6433 - accuracy: 0.6575 - val_loss: 0.6538 - val_accuracy: 0.7823\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6412 - accuracy: 0.6606 - val_loss: 0.6222 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6302 - accuracy: 0.6562 - val_loss: 0.6145 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6178 - accuracy: 0.6656 - val_loss: 0.6050 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6122 - accuracy: 0.6619 - val_loss: 0.6017 - val_accuracy: 0.6915\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6087 - accuracy: 0.6612 - val_loss: 0.5875 - val_accuracy: 0.6673\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6067 - accuracy: 0.6513 - val_loss: 0.5830 - val_accuracy: 0.7157\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5907 - accuracy: 0.6956 - val_loss: 0.5722 - val_accuracy: 0.6673\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5887 - accuracy: 0.6756 - val_loss: 0.5714 - val_accuracy: 0.7802\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5825 - accuracy: 0.6975 - val_loss: 0.5575 - val_accuracy: 0.7177\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5783 - accuracy: 0.6913 - val_loss: 0.5522 - val_accuracy: 0.7480\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5743 - accuracy: 0.7050 - val_loss: 0.5452 - val_accuracy: 0.7460\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5611 - accuracy: 0.7275 - val_loss: 0.5441 - val_accuracy: 0.6694\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5562 - accuracy: 0.7181 - val_loss: 0.5407 - val_accuracy: 0.7843\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5512 - accuracy: 0.7325 - val_loss: 0.5312 - val_accuracy: 0.7157\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5517 - accuracy: 0.7312 - val_loss: 0.5262 - val_accuracy: 0.7802\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5569 - accuracy: 0.7381 - val_loss: 0.5223 - val_accuracy: 0.7843\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5363 - accuracy: 0.7550 - val_loss: 0.5175 - val_accuracy: 0.7480\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5396 - accuracy: 0.7500 - val_loss: 0.5186 - val_accuracy: 0.7258\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5406 - accuracy: 0.7563 - val_loss: 0.5097 - val_accuracy: 0.7661\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5332 - accuracy: 0.7581 - val_loss: 0.5085 - val_accuracy: 0.7843\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5316 - accuracy: 0.7494 - val_loss: 0.5053 - val_accuracy: 0.7843\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5398 - accuracy: 0.7431 - val_loss: 0.5093 - val_accuracy: 0.7742\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5369 - accuracy: 0.7544 - val_loss: 0.5035 - val_accuracy: 0.7520\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5302 - accuracy: 0.7569 - val_loss: 0.4989 - val_accuracy: 0.7601\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5364 - accuracy: 0.7450 - val_loss: 0.4955 - val_accuracy: 0.7823\n",
      "  1/596 [..............................] - ETA: 8s - loss: 1.6702 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0137s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.3013 - accuracy: 0.4302\n",
      "training acc:  0.7450000047683716 , training loss:  0.5364242792129517 , val acc:  0.7822580933570862 , val loss:  0.4954993426799774 , test acc:  0.43015938997268677 , test loss:  1.3013114929199219\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt4/400word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high give form either second lot means add etc must always likely small help us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based following said made non 0 trying level day x life order page    start option simply size > often already result come human someone edit rather little % usually specific output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b < state instead god & type place character control works last important group water list run mean sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether mm range next server amount object general version consider short term target i.e. making kind useful color write test particular class almost energy thus else camera site user got 10 common everything although needs full lower current away higher free difference thought values book original story position show others area seem low call provide name 5 easy available left big results year clear map words input    unless close true ask within actual old language price feel top changes 4 access past game avoid length smaller source mind larger wo several believe experience tell effect difficult comes numbers days wrong design terms earth e.g. # per bad become multiple writing fine hand sound build n created 100 three model along perhaps generally cases future gives easily research history anyone layer step ' standard support sort takes exactly error yes air turn return necessary   humans body whole known men characters product uses money sometimes planet seen running allow company certain ability problems play situation stop cause\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6788 - accuracy: 0.6513 - val_loss: 0.6633 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6673 - accuracy: 0.6438 - val_loss: 0.6512 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6506 - accuracy: 0.6600 - val_loss: 0.6414 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6436 - accuracy: 0.6587 - val_loss: 0.6336 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6406 - accuracy: 0.6500 - val_loss: 0.6221 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6338 - accuracy: 0.6500 - val_loss: 0.6129 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6175 - accuracy: 0.6650 - val_loss: 0.6107 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6186 - accuracy: 0.6581 - val_loss: 0.5940 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6082 - accuracy: 0.6500 - val_loss: 0.5869 - val_accuracy: 0.6774\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5981 - accuracy: 0.6594 - val_loss: 0.5830 - val_accuracy: 0.7581\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5902 - accuracy: 0.6681 - val_loss: 0.5748 - val_accuracy: 0.7722\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5893 - accuracy: 0.6931 - val_loss: 0.5628 - val_accuracy: 0.6673ETA: 0s - loss: 0.5897 - accura\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5785 - accuracy: 0.6938 - val_loss: 0.5534 - val_accuracy: 0.6956\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5767 - accuracy: 0.7100 - val_loss: 0.5467 - val_accuracy: 0.7097\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5668 - accuracy: 0.7006 - val_loss: 0.5458 - val_accuracy: 0.7823\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5636 - accuracy: 0.7350 - val_loss: 0.5348 - val_accuracy: 0.7500\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5551 - accuracy: 0.7250 - val_loss: 0.5329 - val_accuracy: 0.6935\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5607 - accuracy: 0.7262 - val_loss: 0.5288 - val_accuracy: 0.7802\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5551 - accuracy: 0.7531 - val_loss: 0.5204 - val_accuracy: 0.7560\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5523 - accuracy: 0.7544 - val_loss: 0.5204 - val_accuracy: 0.7722\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5441 - accuracy: 0.7525 - val_loss: 0.5145 - val_accuracy: 0.7500\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5431 - accuracy: 0.7431 - val_loss: 0.5106 - val_accuracy: 0.7560\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5348 - accuracy: 0.7462 - val_loss: 0.5069 - val_accuracy: 0.7823\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5393 - accuracy: 0.7475 - val_loss: 0.5040 - val_accuracy: 0.7722\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5350 - accuracy: 0.7462 - val_loss: 0.5085 - val_accuracy: 0.7722\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5324 - accuracy: 0.7481 - val_loss: 0.5008 - val_accuracy: 0.7742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5295 - accuracy: 0.7563 - val_loss: 0.5012 - val_accuracy: 0.7560\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5253 - accuracy: 0.7619 - val_loss: 0.4951 - val_accuracy: 0.7722\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5374 - accuracy: 0.7387 - val_loss: 0.4971 - val_accuracy: 0.7802\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5255 - accuracy: 0.7631 - val_loss: 0.4927 - val_accuracy: 0.7722\n",
      "  1/596 [..............................] - ETA: 8s - loss: 1.7825 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_test_batch_end` time: 0.0140s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.4033 - accuracy: 0.4297\n",
      "training acc:  0.7631250023841858 , training loss:  0.5254805088043213 , val acc:  0.7721773982048035 , val loss:  0.49274712800979614 , test acc:  0.42973992228507996 , test loss:  1.4033163785934448\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt4/450word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high give form either second lot means add etc must always likely small help us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based following said made non 0 trying level day x life order page    start option simply size > often already result come human someone edit rather little % usually specific output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b < state instead god & type place character control works last important group water list run mean sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether mm range next server amount object general version consider short term target i.e. making kind useful color write test particular class almost energy thus else camera site user got 10 common everything although needs full lower current away higher free difference thought values book original story position show others area seem low call provide name 5 easy available left big results year clear map words input    unless close true ask within actual old language price feel top changes 4 access past game avoid length smaller source mind larger wo several believe experience tell effect difficult comes numbers days wrong design terms earth e.g. # per bad become multiple writing fine hand sound build n created 100 three model along perhaps generally cases future gives easily research history anyone layer step ' standard support sort takes exactly error yes air turn return necessary   humans body whole known men characters product uses money sometimes planet seen running allow company certain ability problems play situation stop cause taking normal move related knowledge food theory approach half setting magic main search happen inside sequence \\ nothing speed remember    whatever job tried children examples flash reading context english gets wine post says slightly 6 project options structure choose ground select needed total yet computer quality exact view directly\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 1.0966 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0069s vs `on_train_batch_end` time: 0.0108s). Check your callbacks.\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.6999 - accuracy: 0.5600 - val_loss: 0.6502 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6516 - accuracy: 0.6581 - val_loss: 0.6429 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6518 - accuracy: 0.6469 - val_loss: 0.6346 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6436 - accuracy: 0.6506 - val_loss: 0.6267 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6402 - accuracy: 0.6413 - val_loss: 0.6185 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6219 - accuracy: 0.6644 - val_loss: 0.6140 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6221 - accuracy: 0.6488 - val_loss: 0.6013 - val_accuracy: 0.6673\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6137 - accuracy: 0.6587 - val_loss: 0.5932 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6057 - accuracy: 0.6550 - val_loss: 0.5860 - val_accuracy: 0.6694\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6010 - accuracy: 0.6694 - val_loss: 0.5780 - val_accuracy: 0.6794\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5862 - accuracy: 0.6737 - val_loss: 0.5705 - val_accuracy: 0.6673\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5813 - accuracy: 0.6625 - val_loss: 0.5633 - val_accuracy: 0.7258\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5814 - accuracy: 0.6963 - val_loss: 0.5562 - val_accuracy: 0.7520\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5734 - accuracy: 0.7156 - val_loss: 0.5485 - val_accuracy: 0.7298\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5663 - accuracy: 0.7031 - val_loss: 0.5474 - val_accuracy: 0.7944\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5695 - accuracy: 0.7250 - val_loss: 0.5379 - val_accuracy: 0.7036\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.5590 - accuracy: 0.7181 - val_loss: 0.5315 - val_accuracy: 0.7560\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5624 - accuracy: 0.7375 - val_loss: 0.5289 - val_accuracy: 0.7903\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5548 - accuracy: 0.7450 - val_loss: 0.5220 - val_accuracy: 0.7581\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5487 - accuracy: 0.7425 - val_loss: 0.5174 - val_accuracy: 0.7681\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5435 - accuracy: 0.7544 - val_loss: 0.5160 - val_accuracy: 0.7883\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5439 - accuracy: 0.7538 - val_loss: 0.5096 - val_accuracy: 0.7883\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5326 - accuracy: 0.7675 - val_loss: 0.5059 - val_accuracy: 0.7883\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5371 - accuracy: 0.7475 - val_loss: 0.5080 - val_accuracy: 0.7762\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5282 - accuracy: 0.7575 - val_loss: 0.5003 - val_accuracy: 0.7883\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5353 - accuracy: 0.7569 - val_loss: 0.4969 - val_accuracy: 0.7903\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5242 - accuracy: 0.7531 - val_loss: 0.4969 - val_accuracy: 0.7641\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5299 - accuracy: 0.7606 - val_loss: 0.4964 - val_accuracy: 0.7722\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5305 - accuracy: 0.7500 - val_loss: 0.4905 - val_accuracy: 0.7883\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5223 - accuracy: 0.7594 - val_loss: 0.4926 - val_accuracy: 0.7621\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.7060 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0149s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 1.3302 - accuracy: 0.4275\n",
      "training acc:  0.7593749761581421 , training loss:  0.5223020911216736 , val acc:  0.7620967626571655 , val loss:  0.4925644099712372 , test acc:  0.427537739276886 , test loss:  1.3301727771759033\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1000/opt4/500word_list.txt\n",
      "WORD INDEX STR: , .  $   would 's n't one like use / also could time get need   make way - even work want first two people using think might 'm see may much know = something good different question really 1 example well used 've however 're case still set take say since many try probably number problem point another things find new answer enough 2 possible go actually \\mathcal system let better right without + value power long 'd every back going given able 'll code 3 data ca change sure look part image thing less create best around high give form either second lot means add etc must always likely small help us world though seems \\to bit large least space end function years person process reason read real anything keep note light maybe url$ based following said made non 0 trying level day x life order page    start option simply size > often already result come human someone edit rather little % usually specific output times solution check done found single word issue line makes file similar quite assume side course far understand working put fact idea never simple c information looking b < state instead god & type place character control works last important group water list run mean sense hard pretty field open method called great paper key text bytes lens matter correct online due getting whether mm range next server amount object general version consider short term target i.e. making kind useful color write test particular class almost energy thus else camera site user got 10 common everything although needs full lower current away higher free difference thought values book original story position show others area seem low call provide name 5 easy available left big results year clear map words input    unless close true ask within actual old language price feel top changes 4 access past game avoid length smaller source mind larger wo several believe experience tell effect difficult comes numbers days wrong design terms earth e.g. # per bad become multiple writing fine hand sound build n created 100 three model along perhaps generally cases future gives easily research history anyone layer step ' standard support sort takes exactly error yes air turn return necessary   humans body whole known men characters product uses money sometimes planet seen running allow company certain ability problems play situation stop cause taking normal move related knowledge food theory approach half setting magic main search happen inside sequence \\ nothing speed remember    whatever job tried children examples flash reading context english gets wine post says slightly 6 project options structure choose ground select needed total yet computer quality exact view directly weapons \\\\ therefore noise explanation rate major points fire outside personal ones added meaning produce completely weapon rules reasons guess link elements thanks assuming block later otherwise functions require risk depends n$ states distance de pressure mode black google zero cost looks entire questions focus    mentioned exist force perfect\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6651 - accuracy: 0.6569 - val_loss: 0.6545 - val_accuracy: 0.6673\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6555 - accuracy: 0.6513 - val_loss: 0.6474 - val_accuracy: 0.6673\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6483 - accuracy: 0.6519 - val_loss: 0.6320 - val_accuracy: 0.6673\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6388 - accuracy: 0.6556 - val_loss: 0.6239 - val_accuracy: 0.6673\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.6260 - accuracy: 0.6581 - val_loss: 0.6116 - val_accuracy: 0.6673\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6199 - accuracy: 0.6569 - val_loss: 0.6016 - val_accuracy: 0.6673\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6102 - accuracy: 0.6681 - val_loss: 0.5993 - val_accuracy: 0.7500\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.6084 - accuracy: 0.6694 - val_loss: 0.5894 - val_accuracy: 0.6673\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.6002 - accuracy: 0.6587 - val_loss: 0.5751 - val_accuracy: 0.6734\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5873 - accuracy: 0.6712 - val_loss: 0.5683 - val_accuracy: 0.7520\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5890 - accuracy: 0.7119 - val_loss: 0.5599 - val_accuracy: 0.7520\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5759 - accuracy: 0.7113 - val_loss: 0.5525 - val_accuracy: 0.7601\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5645 - accuracy: 0.7056 - val_loss: 0.5441 - val_accuracy: 0.7520\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5643 - accuracy: 0.7344 - val_loss: 0.5457 - val_accuracy: 0.7762\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5583 - accuracy: 0.7406 - val_loss: 0.5335 - val_accuracy: 0.7843\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5578 - accuracy: 0.7262 - val_loss: 0.5264 - val_accuracy: 0.7641\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5488 - accuracy: 0.7481 - val_loss: 0.5228 - val_accuracy: 0.7480\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5479 - accuracy: 0.7450 - val_loss: 0.5241 - val_accuracy: 0.7056\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5393 - accuracy: 0.7450 - val_loss: 0.5133 - val_accuracy: 0.7601\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5468 - accuracy: 0.7487 - val_loss: 0.5098 - val_accuracy: 0.7883\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5272 - accuracy: 0.7494 - val_loss: 0.5058 - val_accuracy: 0.7762\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5331 - accuracy: 0.7450 - val_loss: 0.5048 - val_accuracy: 0.7601ss: 0.5314 - \n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5368 - accuracy: 0.7544 - val_loss: 0.4999 - val_accuracy: 0.7883\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5275 - accuracy: 0.7513 - val_loss: 0.4973 - val_accuracy: 0.7802\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5303 - accuracy: 0.7475 - val_loss: 0.4962 - val_accuracy: 0.7681\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5289 - accuracy: 0.7487 - val_loss: 0.4949 - val_accuracy: 0.7802\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5213 - accuracy: 0.7550 - val_loss: 0.4971 - val_accuracy: 0.7702\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.5207 - accuracy: 0.7569 - val_loss: 0.4944 - val_accuracy: 0.7702\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 65ms/step - loss: 0.5169 - accuracy: 0.7563 - val_loss: 0.4942 - val_accuracy: 0.7601\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.5111 - accuracy: 0.7719 - val_loss: 0.4854 - val_accuracy: 0.7923\n",
      "  1/596 [..............................] - ETA: 6s - loss: 1.7361 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0154s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 1.3599 - accuracy: 0.4325\n",
      "training acc:  0.7718750238418579 , training loss:  0.5111261606216431 , val acc:  0.7923387289047241 , val loss:  0.4854292571544647 , test acc:  0.43246644735336304 , test loss:  1.3598934412002563\n",
      "\n",
      "Started evaluation for: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1200.csv\n",
      "Loading training file from path:  /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1200.csv\n",
      "(1621, 6)\n",
      "Loading testing file from path:  /home/sukanya/PhD/Datasets/PAN SCD/pan19-style-change-detection/processed/test.csv\n",
      "(9538, 5)\n",
      "opt1\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt1/50word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.8846 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0106s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.7115 - accuracy: 0.5703WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 0.7113 - accuracy: 0.5718 - val_loss: 0.6966 - val_accuracy: 0.4969\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.7021 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0148s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6902 - accuracy: 0.5641\n",
      "training acc:  0.5717592835426331 , training loss:  0.711304247379303 , val acc:  0.49687498807907104 , val loss:  0.6965823769569397 , test acc:  0.5640729665756226 , test loss:  0.6901872158050537\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt1/100word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6482 - accuracy: 0.7188WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0455s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.7205 - accuracy: 0.5797 ETA: 2s -WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 71ms/step - loss: 0.7203 - accuracy: 0.5787 - val_loss: 0.6967 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.7323 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0171s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6979 - accuracy: 0.4516\n",
      "training acc:  0.5787037014961243 , training loss:  0.7202631235122681 , val acc:  0.559374988079071 , val loss:  0.6967060565948486 , test acc:  0.45155200362205505 , test loss:  0.6979342699050903\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt1/150word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know 'm me = something see good different each 1 really example used its now am well question being 're case since set however after were many those say been still take had who things probably between 've try point problem both answer find number his another actually enough 2 over\n",
      "Epoch 1/30\n",
      " 81/100 [=======================>......] - ETA: 1s - loss: 0.7186 - accuracy: 0.5262 ETA: 2s - loss: 0.7452 - accuracy:  - ETA: WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 70ms/step - loss: 0.7186 - accuracy: 0.5262 - val_loss: 0.6861 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.7683 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0156s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7251 - accuracy: 0.4123\n",
      "training acc:  0.5262345671653748 , training loss:  0.7186288237571716 , val acc:  0.559374988079071 , val loss:  0.6861024498939514 , test acc:  0.4123322069644928 , test loss:  0.7250880002975464\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt1/200word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know 'm me = something see good different each 1 really example used its now am well question being 're case since set however after were many those say been still take had who things probably between 've try point problem both answer find number his another actually enough 2 over possible while new did better value right too long through without why go our + let 'll back system 'd down power before every having going \\mathcal able sure look 3 change code less part ca given data own second off either form around create thing means likely lot give\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 1.3551 - accuracy: 0.5938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0199s vs `on_train_batch_end` time: 0.0374s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.8837 - accuracy: 0.5789WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 0.8823 - accuracy: 0.5756 - val_loss: 0.7108 - val_accuracy: 0.5281\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6920 - accuracy: 0.5323\n",
      "training acc:  0.5756173133850098 , training loss:  0.8823058605194092 , val acc:  0.528124988079071 , val loss:  0.7107716798782349 , test acc:  0.5322986841201782 , test loss:  0.6919991374015808\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt1/250word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know 'm me = something see good different each 1 really example used its now am well question being 're case since set however after were many those say been still take had who things probably between 've try point problem both answer find number his another actually enough 2 over possible while new did better value right too long through without why go our + let 'll back system 'd down power before every having going \\mathcal able sure look 3 change code less part ca given data own second off either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply above etc following help level few based little seems person real rather already trying note 0 often anything mm come made years us she doing word\n",
      "Epoch 1/30\n",
      " 81/100 [=======================>......] - ETA: 1s - loss: 0.8886 - accuracy: 0.4776WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 70ms/step - loss: 0.8886 - accuracy: 0.4776 - val_loss: 0.6753 - val_accuracy: 0.5594\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7285 - accuracy: 0.4123\n",
      "training acc:  0.47762346267700195 , training loss:  0.8885786533355713 , val acc:  0.559374988079071 , val loss:  0.6752691864967346 , test acc:  0.4123322069644928 , test loss:  0.7285492420196533\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt1/300word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know 'm me = something see good different each 1 really example used its now am well question being 're case since set however after were many those say been still take had who things probably between 've try point problem both answer find number his another actually enough 2 over possible while new did better value right too long through without why go our + let 'll back system 'd down power before every having going \\mathcal able sure look 3 change code less part ca given data own second off either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply above etc following help level few based little seems person real rather already trying note 0 often anything mm come made years us she doing word keep non once issue lens maybe usually specific again life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera under size single put someone course done output control field option fact quite god state open\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7759 - accuracy: 0.5312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0264s vs `on_train_batch_end` time: 0.0502s). Check your callbacks.\n",
      " 81/100 [=======================>......] - ETA: 1s - loss: 0.7034 - accuracy: 0.5463 ETA: 1s - loss: 0.7094 - WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 0.7034 - accuracy: 0.5463 - val_loss: 0.6893 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.7835 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0140s). Check your callbacks.\n",
      "596/596 [==============================] - ETA: 0s - loss: 0.7297 - accuracy: 0.41 - 15s 25ms/step - loss: 0.7296 - accuracy: 0.4123\n",
      "training acc:  0.5462962985038757 , training loss:  0.70340496301651 , val acc:  0.559374988079071 , val loss:  0.6893385648727417 , test acc:  0.4123322069644928 , test loss:  0.729566752910614\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt1/350word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know 'm me = something see good different each 1 really example used its now am well question being 're case since set however after were many those say been still take had who things probably between 've try point problem both answer find number his another actually enough 2 over possible while new did better value right too long through without why go our + let 'll back system 'd down power before every having going \\mathcal able sure look 3 change code less part ca given data own second off either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply above etc following help level few based little seems person real rather already trying note 0 often anything mm come made years us she doing word keep non once issue lens maybe usually specific again life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera under size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called him general pretty works idea understand human kind great consider sense < list version never making whether object paper % > until line due hard difference text i.e. color getting important itself online character bytes user next else\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 1s - loss: 1.1129 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0156s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.7865 - accuracy: 0.5016WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 81/100 [=======================>......] - 6s 70ms/step - loss: 0.7852 - accuracy: 0.5023 - val_loss: 0.6825 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.7672 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0156s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7286 - accuracy: 0.4123\n",
      "training acc:  0.5023148059844971 , training loss:  0.7851912975311279 , val acc:  0.559374988079071 , val loss:  0.6824725270271301 , test acc:  0.4123322069644928 , test loss:  0.7285842895507812\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt1/400word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know 'm me = something see good different each 1 really example used its now am well question being 're case since set however after were many those say been still take had who things probably between 've try point problem both answer find number his another actually enough 2 over possible while new did better value right too long through without why go our + let 'll back system 'd down power before every having going \\mathcal able sure look 3 change code less part ca given data own second off either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply above etc following help level few based little seems person real rather already trying note 0 often anything mm come made years us she doing word keep non once issue lens maybe usually specific again life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera under size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called him general pretty works idea understand human kind great consider sense < list version never making whether object paper % > until line due hard difference text i.e. color getting important itself online character bytes user next else day particular range correct method thus area write amount values common \\\\ experience others energy needs useful position free unless higher easy design full top show fine seem short everything almost below comes term her results 4 original ask wrong \\ name map away clear test call available generally 5\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6728 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_train_batch_end` time: 0.0411s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.6922 - accuracy: 0.5508WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 71ms/step - loss: 0.6921 - accuracy: 0.5502 - val_loss: 0.6870 - val_accuracy: 0.5625\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7049 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_test_batch_end` time: 0.0135s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7018 - accuracy: 0.4129\n",
      "training acc:  0.5501543283462524 , training loss:  0.6921433210372925 , val acc:  0.5625 , val loss:  0.6870070695877075 , test acc:  0.4128565490245819 , test loss:  0.7017701864242554\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt1/450word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know 'm me = something see good different each 1 really example used its now am well question being 're case since set however after were many those say been still take had who things probably between 've try point problem both answer find number his another actually enough 2 over possible while new did better value right too long through without why go our + let 'll back system 'd down power before every having going \\mathcal able sure look 3 change code less part ca given data own second off either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply above etc following help level few based little seems person real rather already trying note 0 often anything mm come made years us she doing word keep non once issue lens maybe usually specific again life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera under size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called him general pretty works idea understand human kind great consider sense < list version never making whether object paper % > until line due hard difference text i.e. color getting important itself online character bytes user next else day particular range correct method thus area write amount values common \\\\ experience others energy needs useful position free unless higher easy design full top show fine seem short everything almost below comes term her results 4 original ask wrong \\ name map away clear test call available generally 5 words 10 old thought although low several wo true error 100 provide close actual big mind target feel believe left input matter sort model become magic bad against effect source current got sound    three changes future multiple per nothing standard year access allow book gives language game terms step\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 1.2172 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0341s vs `on_train_batch_end` time: 0.0589s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.7712 - accuracy: 0.4922 ETA: 2s - lWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 70ms/step - loss: 0.7701 - accuracy: 0.4938 - val_loss: 0.6788 - val_accuracy: 0.5750\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.7217 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0137s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7157 - accuracy: 0.4133\n",
      "training acc:  0.4938271641731262 , training loss:  0.7700685262680054 , val acc:  0.574999988079071 , val loss:  0.678794264793396 , test acc:  0.4132760167121887 , test loss:  0.7157204151153564\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt1/500word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know 'm me = something see good different each 1 really example used its now am well question being 're case since set however after were many those say been still take had who things probably between 've try point problem both answer find number his another actually enough 2 over possible while new did better value right too long through without why go our + let 'll back system 'd down power before every having going \\mathcal able sure look 3 change code less part ca given data own second off either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply above etc following help level few based little seems person real rather already trying note 0 often anything mm come made years us she doing word keep non once issue lens maybe usually specific again life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera under size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called him general pretty works idea understand human kind great consider sense < list version never making whether object paper % > until line due hard difference text i.e. color getting important itself online character bytes user next else day particular range correct method thus area write amount values common \\\\ experience others energy needs useful position free unless higher easy design full top show fine seem short everything almost below comes term her results 4 original ask wrong \\ name map away clear test call available generally 5 words 10 old thought although low several wo true error 100 provide close actual big mind target feel believe left input matter sort model become magic bad against effect source current got sound    three changes future multiple per nothing standard year access allow book gives language game terms step related setting n e.g. price main easily site body along support difficult normal yes certain tell company taking hand stop turn within view avoid class anyone running story post children de money further takes ability quality lower necessary d noise seen uses search history major numbers happen knowledge theory job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6601 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0213s vs `on_train_batch_end` time: 0.0459s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.6896 - accuracy: 0.5594WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 70ms/step - loss: 0.6907 - accuracy: 0.5579 - val_loss: 0.7155 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.8426 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0067s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7431 - accuracy: 0.4123\n",
      "training acc:  0.5578703880310059 , training loss:  0.6907488107681274 , val acc:  0.559374988079071 , val loss:  0.71552574634552 , test acc:  0.4123322069644928 , test loss:  0.7431320548057556\n",
      "\n",
      "opt2\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt2/50word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some\n",
      "Epoch 1/30\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.6928 - accuracy: 0.5539WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 70ms/step - loss: 0.6921 - accuracy: 0.5571 - val_loss: 0.6912 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.7548 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0140s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7135 - accuracy: 0.4123\n",
      "training acc:  0.5570987462997437 , training loss:  0.6920722723007202 , val acc:  0.559374988079071 , val loss:  0.6912482976913452 , test acc:  0.4123322069644928 , test loss:  0.713458776473999\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt2/100word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most\n",
      "Epoch 1/30\n",
      " 81/100 [=======================>......] - ETA: 1s - loss: 0.7484 - accuracy: 0.5795WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 70ms/step - loss: 0.7484 - accuracy: 0.5795 - val_loss: 0.6997 - val_accuracy: 0.5562\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.7357 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_test_batch_end` time: 0.0205s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6975 - accuracy: 0.4711\n",
      "training acc:  0.5794752836227417 , training loss:  0.7484247088432312 , val acc:  0.5562499761581421 , val loss:  0.6997085213661194 , test acc:  0.47105705738067627 , test loss:  0.6974717974662781\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt2/150word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know 'm me = something see good different each 1 really example used its now am well question being 're case since set however after were many those say been still take had who things probably between 've try point problem both answer find number his another actually enough\n",
      "Epoch 1/30\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.7229 - accuracy: 0.5016WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 71ms/step - loss: 0.7227 - accuracy: 0.5008 - val_loss: 0.6853 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7474 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0138s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7163 - accuracy: 0.4123\n",
      "training acc:  0.5007715821266174 , training loss:  0.7226951718330383 , val acc:  0.559374988079071 , val loss:  0.6853307485580444 , test acc:  0.4123322069644928 , test loss:  0.7162913084030151\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt2/200word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know 'm me = something see good different each 1 really example used its now am well question being 're case since set however after were many those say been still take had who things probably between 've try point problem both answer find number his another actually enough 2 over possible while new did better value right too long through without why go our + let 'll back system 'd down power before every having going \\mathcal able sure look 3 change code less part ca given data own second off either form around create thing means likely\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.6779 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0079s vs `on_train_batch_end` time: 0.0305s). Check your callbacks.\n",
      " 81/100 [=======================>......] - ETA: 1s - loss: 0.6953 - accuracy: 0.5586WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 71ms/step - loss: 0.6953 - accuracy: 0.5586 - val_loss: 0.6908 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.7377 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0127s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7068 - accuracy: 0.4123\n",
      "training acc:  0.5586419701576233 , training loss:  0.6953038573265076 , val acc:  0.559374988079071 , val loss:  0.6907625198364258 , test acc:  0.4123322069644928 , test loss:  0.7067762613296509\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt2/250word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know 'm me = something see good different each 1 really example used its now am well question being 're case since set however after were many those say been still take had who things probably between 've try point problem both answer find number his another actually enough 2 over possible while new did better value right too long through without why go our + let 'll back system 'd down power before every having going \\mathcal able sure look 3 change code less part ca given data own second off either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply above etc following help level few based little seems person real rather already trying note 0 often anything mm come made years us she\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.8815 - accuracy: 0.5938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_train_batch_end` time: 0.0431s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.7980 - accuracy: 0.4828 ETA: 2s - loss: 0 - ETA: 1s - loss: 0.8045 - accuracy: WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 70ms/step - loss: 0.7963 - accuracy: 0.4853 - val_loss: 0.6789 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7526 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0130s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7257 - accuracy: 0.4123\n",
      "training acc:  0.485339492559433 , training loss:  0.7962886691093445 , val acc:  0.559374988079071 , val loss:  0.6788660287857056 , test acc:  0.4123322069644928 , test loss:  0.7256979942321777\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt2/300word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know 'm me = something see good different each 1 really example used its now am well question being 're case since set however after were many those say been still take had who things probably between 've try point problem both answer find number his another actually enough 2 over possible while new did better value right too long through without why go our + let 'll back system 'd down power before every having going \\mathcal able sure look 3 change code less part ca given data own second off either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply above etc following help level few based little seems person real rather already trying note 0 often anything mm come made years us she doing word keep non once issue lens maybe usually specific again life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera under size single put someone course done output control field option fact quite god\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7438 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0027s vs `on_train_batch_end` time: 0.0217s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.6908 - accuracy: 0.5648WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 71ms/step - loss: 0.6915 - accuracy: 0.5625 - val_loss: 0.6923 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7535 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0160s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7119 - accuracy: 0.4123\n",
      "training acc:  0.5625 , training loss:  0.6914843916893005 , val acc:  0.559374988079071 , val loss:  0.692322850227356 , test acc:  0.4123322069644928 , test loss:  0.7118849158287048\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt2/350word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know 'm me = something see good different each 1 really example used its now am well question being 're case since set however after were many those say been still take had who things probably between 've try point problem both answer find number his another actually enough 2 over possible while new did better value right too long through without why go our + let 'll back system 'd down power before every having going \\mathcal able sure look 3 change code less part ca given data own second off either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply above etc following help level few based little seems person real rather already trying note 0 often anything mm come made years us she doing word keep non once issue lens maybe usually specific again life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera under size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called him general pretty works idea understand human kind great consider sense < list version never making whether object paper % > until line due hard difference text i.e. color getting important itself online character bytes user\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.5611 - accuracy: 0.7188WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0278s vs `on_train_batch_end` time: 0.0431s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.7125 - accuracy: 0.5266 ETA: WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 0.7130 - accuracy: 0.5247 - val_loss: 0.6885 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7757 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0016s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7266 - accuracy: 0.4123\n",
      "training acc:  0.5246913433074951 , training loss:  0.7129709720611572 , val acc:  0.559374988079071 , val loss:  0.6885185837745667 , test acc:  0.4123322069644928 , test loss:  0.7266134023666382\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt2/400word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know 'm me = something see good different each 1 really example used its now am well question being 're case since set however after were many those say been still take had who things probably between 've try point problem both answer find number his another actually enough 2 over possible while new did better value right too long through without why go our + let 'll back system 'd down power before every having going \\mathcal able sure look 3 change code less part ca given data own second off either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply above etc following help level few based little seems person real rather already trying note 0 often anything mm come made years us she doing word keep non once issue lens maybe usually specific again life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera under size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called him general pretty works idea understand human kind great consider sense < list version never making whether object paper % > until line due hard difference text i.e. color getting important itself online character bytes user next else day particular range correct method thus area write amount values common \\\\ experience others energy needs useful position free unless higher easy design full top show fine seem short everything almost below comes term her results 4 original ask wrong \\ name map away clear test call available\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 1.3782 - accuracy: 0.2812WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0095s vs `on_train_batch_end` time: 0.0363s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.8364 - accuracy: 0.4516WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 71ms/step - loss: 0.8345 - accuracy: 0.4537 - val_loss: 0.6795 - val_accuracy: 0.7469\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.7087 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0164s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7114 - accuracy: 0.4259\n",
      "training acc:  0.45370370149612427 , training loss:  0.8344875574111938 , val acc:  0.746874988079071 , val loss:  0.6795207858085632 , test acc:  0.42585989832878113 , test loss:  0.7114148139953613\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt2/450word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know 'm me = something see good different each 1 really example used its now am well question being 're case since set however after were many those say been still take had who things probably between 've try point problem both answer find number his another actually enough 2 over possible while new did better value right too long through without why go our + let 'll back system 'd down power before every having going \\mathcal able sure look 3 change code less part ca given data own second off either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply above etc following help level few based little seems person real rather already trying note 0 often anything mm come made years us she doing word keep non once issue lens maybe usually specific again life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera under size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called him general pretty works idea understand human kind great consider sense < list version never making whether object paper % > until line due hard difference text i.e. color getting important itself online character bytes user next else day particular range correct method thus area write amount values common \\\\ experience others energy needs useful position free unless higher easy design full top show fine seem short everything almost below comes term her results 4 original ask wrong \\ name map away clear test call available generally 5 words 10 old thought although low several wo true error 100 provide close actual big mind target feel believe left input matter sort model become magic bad against effect source current got sound    three changes future multiple per nothing standard year access allow book gives language game\n",
      "Epoch 1/30\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.6893 - accuracy: 0.5688WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 71ms/step - loss: 0.6895 - accuracy: 0.5664 - val_loss: 0.6894 - val_accuracy: 0.5594\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7005 - accuracy: 0.4123\n",
      "training acc:  0.5663580298423767 , training loss:  0.6894527077674866 , val acc:  0.559374988079071 , val loss:  0.6894207000732422 , test acc:  0.4123322069644928 , test loss:  0.7004654407501221\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt2/500word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   are on have not or can if but would 's your an so n't do from at they will there by one my what more which all we some was when then like use has also about / just time no could any only get other them than how need up their does out should -   where make way even very same because two work first want people might these using think he may much here such most into know 'm me = something see good different each 1 really example used its now am well question being 're case since set however after were many those say been still take had who things probably between 've try point problem both answer find number his another actually enough 2 over possible while new did better value right too long through without why go our + let 'll back system 'd down power before every having going \\mathcal able sure look 3 change code less part ca given data own second off either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply above etc following help level few based little seems person real rather already trying note 0 often anything mm come made years us she doing word keep non once issue lens maybe usually specific again life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera under size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called him general pretty works idea understand human kind great consider sense < list version never making whether object paper % > until line due hard difference text i.e. color getting important itself online character bytes user next else day particular range correct method thus area write amount values common \\\\ experience others energy needs useful position free unless higher easy design full top show fine seem short everything almost below comes term her results 4 original ask wrong \\ name map away clear test call available generally 5 words 10 old thought although low several wo true error 100 provide close actual big mind target feel believe left input matter sort model become magic bad against effect source current got sound    three changes future multiple per nothing standard year access allow book gives language game terms step related setting n e.g. price main easily site body along support difficult normal yes certain tell company taking hand stop turn within view avoid class anyone running story post children de money further takes ability quality lower necessary d noise seen uses search history major numbers happen knowledge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.7051 - accuracy: 0.5289 ETA: 1s - loss: 0.709WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 71ms/step - loss: 0.7048 - accuracy: 0.5293 - val_loss: 0.6847 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.7218 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0134s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7078 - accuracy: 0.4123\n",
      "training acc:  0.529321014881134 , training loss:  0.7048359513282776 , val acc:  0.559374988079071 , val loss:  0.6847456693649292 , test acc:  0.4123322069644928 , test loss:  0.7077921628952026\n",
      "\n",
      "opt3\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt3/50word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say\n",
      "Epoch 1/30\n",
      " 81/100 [=======================>......] - ETA: 1s - loss: 0.6874 - accuracy: 0.6111WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 0.6874 - accuracy: 0.6111 - val_loss: 0.6778 - val_accuracy: 0.6562\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.7258 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_test_batch_end` time: 0.0142s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7015 - accuracy: 0.4403\n",
      "training acc:  0.6111111044883728 , training loss:  0.6873522996902466 , val acc:  0.65625 , val loss:  0.6777593493461609 , test acc:  0.440331369638443 , test loss:  0.7015344500541687\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt3/100word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.8490 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0567s). Check your callbacks.\n",
      " 81/100 [=======================>......] - ETA: 1s - loss: 0.7211 - accuracy: 0.5394WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 0.7211 - accuracy: 0.5394 - val_loss: 0.6654 - val_accuracy: 0.5719\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.8029 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0149s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7246 - accuracy: 0.4137\n",
      "training acc:  0.5393518805503845 , training loss:  0.721102774143219 , val acc:  0.5718749761581421 , val loss:  0.6654343605041504 , test acc:  0.41369548439979553 , test loss:  0.7246140241622925\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt3/150word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply etc following help level based little seems person real rather already trying note 0 often anything mm come made years us word\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.7342 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0087s vs `on_train_batch_end` time: 0.0210s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.7060 - accuracy: 0.5266WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 0.7058 - accuracy: 0.5270 - val_loss: 0.6891 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7291 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0153s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7044 - accuracy: 0.4123\n",
      "training acc:  0.5270061492919922 , training loss:  0.7057933807373047 , val acc:  0.559374988079071 , val loss:  0.6890959739685059 , test acc:  0.4123322069644928 , test loss:  0.704420804977417\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt3/200word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply etc following help level based little seems person real rather already trying note 0 often anything mm come made years us word keep non issue lens maybe usually specific life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera size single put someone course done output control field option fact quite god state open solution mean b\n",
      "Epoch 1/30\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.6857 - accuracy: 0.5625WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 81/100 [=======================>......] - 6s 68ms/step - loss: 0.6855 - accuracy: 0.5640 - val_loss: 0.6683 - val_accuracy: 0.7281\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.7357 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0159s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6997 - accuracy: 0.4714\n",
      "training acc:  0.5640432238578796 , training loss:  0.685495138168335 , val acc:  0.7281249761581421 , val loss:  0.6683219075202942 , test acc:  0.4713716506958008 , test loss:  0.6996698975563049\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt3/250word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply etc following help level based little seems person real rather already trying note 0 often anything mm come made years us word keep non issue lens maybe usually specific life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called general pretty works idea understand human kind great consider sense < list version never making whether object paper % > line due hard difference text i.e. color getting important online character bytes user next else day particular range correct method thus\n",
      "Epoch 1/30\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.6851 - accuracy: 0.5938WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 68ms/step - loss: 0.6847 - accuracy: 0.5965 - val_loss: 0.6704 - val_accuracy: 0.7281\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.7367 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_test_batch_end` time: 0.0201s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7021 - accuracy: 0.4577\n",
      "training acc:  0.5964506268501282 , training loss:  0.6846665143966675 , val acc:  0.7281249761581421 , val loss:  0.670383095741272 , test acc:  0.45773908495903015 , test loss:  0.7020862698554993\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt3/300word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply etc following help level based little seems person real rather already trying note 0 often anything mm come made years us word keep non issue lens maybe usually specific life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called general pretty works idea understand human kind great consider sense < list version never making whether object paper % > line due hard difference text i.e. color getting important online character bytes user next else day particular range correct method thus area write amount values common \\\\ experience others energy needs useful position free unless higher easy design full top show fine seem short everything almost comes term results 4 original ask wrong \\ name map away clear test call available generally 5 words 10 old thought although low several wo\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7762 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_train_batch_end` time: 0.0467s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.6819 - accuracy: 0.6164 ETA: 1s - loss: 0.6818 - WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 0.6815 - accuracy: 0.6188 - val_loss: 0.6628 - val_accuracy: 0.7250\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.7559 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_test_batch_end` time: 0.0157s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7048 - accuracy: 0.4551\n",
      "training acc:  0.6188271641731262 , training loss:  0.6815067529678345 , val acc:  0.7250000238418579 , val loss:  0.6628056764602661 , test acc:  0.45511746406555176 , test loss:  0.7048299908638\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt3/350word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply etc following help level based little seems person real rather already trying note 0 often anything mm come made years us word keep non issue lens maybe usually specific life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called general pretty works idea understand human kind great consider sense < list version never making whether object paper % > line due hard difference text i.e. color getting important online character bytes user next else day particular range correct method thus area write amount values common \\\\ experience others energy needs useful position free unless higher easy design full top show fine seem short everything almost comes term results 4 original ask wrong \\ name map away clear test call available generally 5 words 10 old thought although low several wo true error 100 provide close actual big mind target feel believe left input matter sort model become magic bad effect source current got sound    three changes future multiple per nothing standard year access allow book gives language game terms step related setting n e.g. price main easily site body\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.6929 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_end` time: 0.0211s). Check your callbacks.\n",
      " 81/100 [=======================>......] - ETA: 1s - loss: 0.6897 - accuracy: 0.6034WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 68ms/step - loss: 0.6897 - accuracy: 0.6034 - val_loss: 0.6759 - val_accuracy: 0.6313\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.7311 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0163s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7013 - accuracy: 0.4355\n",
      "training acc:  0.6033950448036194 , training loss:  0.6897121667861938 , val acc:  0.6312500238418579 , val loss:  0.6758508682250977 , test acc:  0.43550753593444824 , test loss:  0.70131516456604\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt3/400word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply etc following help level based little seems person real rather already trying note 0 often anything mm come made years us word keep non issue lens maybe usually specific life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called general pretty works idea understand human kind great consider sense < list version never making whether object paper % > line due hard difference text i.e. color getting important online character bytes user next else day particular range correct method thus area write amount values common \\\\ experience others energy needs useful position free unless higher easy design full top show fine seem short everything almost comes term results 4 original ask wrong \\ name map away clear test call available generally 5 words 10 old thought although low several wo true error 100 provide close actual big mind target feel believe left input matter sort model become magic bad effect source current got sound    three changes future multiple per nothing standard year access allow book gives language game terms step related setting n e.g. price main easily site body along support difficult normal yes certain tell company taking hand stop turn within view avoid class anyone running story post children de money takes ability quality lower necessary noise seen uses search history major numbers happen knowledge theory job past perhaps cause needed link approach    problems points cases humans\n",
      "Epoch 1/30\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.6828 - accuracy: 0.5805WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 0.6828 - accuracy: 0.5810 - val_loss: 0.6687 - val_accuracy: 0.6281\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.7554 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0146s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.7083 - accuracy: 0.4268\n",
      "training acc:  0.5810185074806213 , training loss:  0.6828103065490723 , val acc:  0.628125011920929 , val loss:  0.6686531901359558 , test acc:  0.42680367827415466 , test loss:  0.7082703113555908\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt3/450word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply etc following help level based little seems person real rather already trying note 0 often anything mm come made years us word keep non issue lens maybe usually specific life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called general pretty works idea understand human kind great consider sense < list version never making whether object paper % > line due hard difference text i.e. color getting important online character bytes user next else day particular range correct method thus area write amount values common \\\\ experience others energy needs useful position free unless higher easy design full top show fine seem short everything almost comes term results 4 original ask wrong \\ name map away clear test call available generally 5 words 10 old thought although low several wo true error 100 provide close actual big mind target feel believe left input matter sort model become magic bad effect source current got sound    three changes future multiple per nothing standard year access allow book gives language game terms step related setting n e.g. price main easily site body along support difficult normal yes certain tell company taking hand stop turn within view avoid class anyone running story post children de money takes ability quality lower necessary noise seen uses search history major numbers happen knowledge theory job past perhaps cause needed link approach    problems points cases humans smaller exactly days examples build gets sometimes speed length whole choose move project questions 6 planet english risk product remember situation explanation especially characters focus research tried air modern men rules ' personal looks states flash whatever yet attack distance assuming exact options guess larger weapon update context total sequence\n",
      "Epoch 1/30\n",
      " 81/100 [=======================>......] - ETA: 1s - loss: 0.6943 - accuracy: 0.6142 ETA: 1s - loss: 0.6972 - accuracy - ETA: 1s - loss: 0.6969 - WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 70ms/step - loss: 0.6943 - accuracy: 0.6142 - val_loss: 0.6734 - val_accuracy: 0.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6920 - accuracy: 0.4953\n",
      "training acc:  0.6141975522041321 , training loss:  0.6943140625953674 , val acc:  0.65625 , val loss:  0.6734176874160767 , test acc:  0.49528104066848755 , test loss:  0.6920109987258911\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt3/500word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply etc following help level based little seems person real rather already trying note 0 often anything mm come made years us word keep non issue lens maybe usually specific life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called general pretty works idea understand human kind great consider sense < list version never making whether object paper % > line due hard difference text i.e. color getting important online character bytes user next else day particular range correct method thus area write amount values common \\\\ experience others energy needs useful position free unless higher easy design full top show fine seem short everything almost comes term results 4 original ask wrong \\ name map away clear test call available generally 5 words 10 old thought although low several wo true error 100 provide close actual big mind target feel believe left input matter sort model become magic bad effect source current got sound    three changes future multiple per nothing standard year access allow book gives language game terms step related setting n e.g. price main easily site body along support difficult normal yes certain tell company taking hand stop turn within view avoid class anyone running story post children de money takes ability quality lower necessary noise seen uses search history major numbers happen knowledge theory job past perhaps cause needed link approach    problems points cases humans smaller exactly days examples build gets sometimes speed length whole choose move project questions 6 planet english risk product remember situation explanation especially characters focus research tried air modern men rules ' personal looks states flash whatever yet attack distance assuming exact options guess larger weapon update context total sequence wide half play offer cost mentioned depends structure says lenses food aperture factor layer fairly require completely logic   meaning weapons directly extra mostly writing longer inside added signal later expected background elements block car reasons conditions exist self degree direct known created issues frame reading subject computer program produce\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.8107 - accuracy: 0.2188WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0157s vs `on_train_batch_end` time: 0.0398s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.6861 - accuracy: 0.5789WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 70ms/step - loss: 0.6861 - accuracy: 0.5787 - val_loss: 0.6576 - val_accuracy: 0.7188\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7528 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0138s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7016 - accuracy: 0.4740\n",
      "training acc:  0.5787037014961243 , training loss:  0.6861402988433838 , val acc:  0.71875 , val loss:  0.6575790643692017 , test acc:  0.47399330139160156 , test loss:  0.7016347050666809\n",
      "\n",
      "opt4\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt4/50word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7797 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0099s vs `on_train_batch_end` time: 0.0234s). Check your callbacks.\n",
      " 81/100 [=======================>......] - ETA: 1s - loss: 0.6923 - accuracy: 0.5517WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 0.6923 - accuracy: 0.5517 - val_loss: 0.6921 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.7428 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0138s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7095 - accuracy: 0.4123\n",
      "training acc:  0.5516975522041321 , training loss:  0.6923193335533142 , val acc:  0.559374988079071 , val loss:  0.6921432018280029 , test acc:  0.4123322069644928 , test loss:  0.7095269560813904\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt4/100word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.7051 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0272s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.6908 - accuracy: 0.5531WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 0.6903 - accuracy: 0.5556 - val_loss: 0.6874 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.7442 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0131s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7125 - accuracy: 0.4123\n",
      "training acc:  0.5555555820465088 , training loss:  0.6903250813484192 , val acc:  0.559374988079071 , val loss:  0.6873670816421509 , test acc:  0.4123322069644928 , test loss:  0.7124832272529602\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt4/150word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply etc following help level based little seems person real rather already trying note 0 often anything mm come made years\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 1.9823 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0420s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 1.0369 - accuracy: 0.4375WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 1.0327 - accuracy: 0.4375 - val_loss: 0.6719 - val_accuracy: 0.7437\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7228 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_test_batch_end` time: 0.0139s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7146 - accuracy: 0.4382\n",
      "training acc:  0.4375 , training loss:  1.032677412033081 , val acc:  0.7437499761581421 , val loss:  0.671865701675415 , test acc:  0.4382340610027313 , test loss:  0.7145627737045288\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt4/200word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply etc following help level based little seems person real rather already trying note 0 often anything mm come made years us word keep non issue lens maybe usually specific life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera size single put someone course done output control field option fact quite god state open solution\n",
      "Epoch 1/30\n",
      " 81/100 [=======================>......] - ETA: 1s - loss: 0.7215 - accuracy: 0.5401 ETA: 1s - loss: 0.7229 - accuracy: 0.WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 0.7215 - accuracy: 0.5401 - val_loss: 0.7008 - val_accuracy: 0.5500\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7051 - accuracy: 0.4432\n",
      "training acc:  0.540123462677002 , training loss:  0.7214842438697815 , val acc:  0.550000011920929 , val loss:  0.7008084058761597 , test acc:  0.443162739276886 , test loss:  0.7050592303276062\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt4/250word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply etc following help level based little seems person real rather already trying note 0 often anything mm come made years us word keep non issue lens maybe usually specific life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called general pretty works idea understand human kind great consider sense < list version never making whether object paper % > line due hard difference text i.e. color getting important online character bytes user next else day particular range correct\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7590 - accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0026s vs `on_train_batch_end` time: 0.0491s). Check your callbacks.\n",
      " 81/100 [=======================>......] - ETA: 1s - loss: 0.6957 - accuracy: 0.5486WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 0.6957 - accuracy: 0.5486 - val_loss: 0.6967 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7835 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0147s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7288 - accuracy: 0.4123\n",
      "training acc:  0.5486111044883728 , training loss:  0.6957002878189087 , val acc:  0.559374988079071 , val loss:  0.6966816186904907 , test acc:  0.4123322069644928 , test loss:  0.728762149810791\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt4/300word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply etc following help level based little seems person real rather already trying note 0 often anything mm come made years us word keep non issue lens maybe usually specific life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called general pretty works idea understand human kind great consider sense < list version never making whether object paper % > line due hard difference text i.e. color getting important online character bytes user next else day particular range correct method thus area write amount values common \\\\ experience others energy needs useful position free unless higher easy design full top show fine seem short everything almost comes term results 4 original ask wrong \\ name map away clear test call available generally 5 words 10 old thought although low\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.8213 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0287s vs `on_train_batch_end` time: 0.0567s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.6943 - accuracy: 0.5797WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 70ms/step - loss: 0.6942 - accuracy: 0.5802 - val_loss: 0.6916 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7147 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_test_batch_end` time: 0.0139s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6998 - accuracy: 0.4123\n",
      "training acc:  0.5802469253540039 , training loss:  0.6942110061645508 , val acc:  0.559374988079071 , val loss:  0.691630482673645 , test acc:  0.4123322069644928 , test loss:  0.6998274326324463\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt4/350word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply etc following help level based little seems person real rather already trying note 0 often anything mm come made years us word keep non issue lens maybe usually specific life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called general pretty works idea understand human kind great consider sense < list version never making whether object paper % > line due hard difference text i.e. color getting important online character bytes user next else day particular range correct method thus area write amount values common \\\\ experience others energy needs useful position free unless higher easy design full top show fine seem short everything almost comes term results 4 original ask wrong \\ name map away clear test call available generally 5 words 10 old thought although low several wo true error 100 provide close actual big mind target feel believe left input matter sort model become magic bad effect source current got sound    three changes future multiple per nothing standard year access allow book gives language game terms step related setting n e.g. price main easily\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.8887 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0175s vs `on_train_batch_end` time: 0.0487s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.7014 - accuracy: 0.5820WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 0.7020 - accuracy: 0.5795 - val_loss: 0.6991 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.7487 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_test_batch_end` time: 0.0056s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7106 - accuracy: 0.4123\n",
      "training acc:  0.5794752836227417 , training loss:  0.7020063400268555 , val acc:  0.559374988079071 , val loss:  0.6990611553192139 , test acc:  0.4123322069644928 , test loss:  0.7106121778488159\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt4/400word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply etc following help level based little seems person real rather already trying note 0 often anything mm come made years us word keep non issue lens maybe usually specific life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called general pretty works idea understand human kind great consider sense < list version never making whether object paper % > line due hard difference text i.e. color getting important online character bytes user next else day particular range correct method thus area write amount values common \\\\ experience others energy needs useful position free unless higher easy design full top show fine seem short everything almost comes term results 4 original ask wrong \\ name map away clear test call available generally 5 words 10 old thought although low several wo true error 100 provide close actual big mind target feel believe left input matter sort model become magic bad effect source current got sound    three changes future multiple per nothing standard year access allow book gives language game terms step related setting n e.g. price main easily site body along support difficult normal yes certain tell company taking hand stop turn within view avoid class anyone running story post children de money takes ability quality lower necessary noise seen uses search history major numbers happen knowledge theory job past perhaps cause needed link approach    problems points\n",
      "Epoch 1/30\n",
      " 81/100 [=======================>......] - ETA: 1s - loss: 0.8196 - accuracy: 0.5664WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 0.8196 - accuracy: 0.5664 - val_loss: 0.7095 - val_accuracy: 0.3656\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.6822 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0135s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6837 - accuracy: 0.5688\n",
      "training acc:  0.5663580298423767 , training loss:  0.8196480870246887 , val acc:  0.3656249940395355 , val loss:  0.7095175981521606 , test acc:  0.568791925907135 , test loss:  0.6837326288223267\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt4/450word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply etc following help level based little seems person real rather already trying note 0 often anything mm come made years us word keep non issue lens maybe usually specific life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called general pretty works idea understand human kind great consider sense < list version never making whether object paper % > line due hard difference text i.e. color getting important online character bytes user next else day particular range correct method thus area write amount values common \\\\ experience others energy needs useful position free unless higher easy design full top show fine seem short everything almost comes term results 4 original ask wrong \\ name map away clear test call available generally 5 words 10 old thought although low several wo true error 100 provide close actual big mind target feel believe left input matter sort model become magic bad effect source current got sound    three changes future multiple per nothing standard year access allow book gives language game terms step related setting n e.g. price main easily site body along support difficult normal yes certain tell company taking hand stop turn within view avoid class anyone running story post children de money takes ability quality lower necessary noise seen uses search history major numbers happen knowledge theory job past perhaps cause needed link approach    problems points cases humans smaller exactly days examples build gets sometimes speed length whole choose move project questions 6 planet english risk product remember situation explanation especially characters focus research tried air modern men rules ' personal looks states flash whatever yet attack distance assuming exact options guess larger weapon update context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.8434 - accuracy: 0.3438WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0159s vs `on_train_batch_end` time: 0.0485s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.6920 - accuracy: 0.5703WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 69ms/step - loss: 0.6926 - accuracy: 0.5679 - val_loss: 0.6878 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7738 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0140s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7261 - accuracy: 0.4123\n",
      "training acc:  0.5679012537002563 , training loss:  0.6926198601722717 , val acc:  0.559374988079071 , val loss:  0.6878253221511841 , test acc:  0.4123322069644928 , test loss:  0.7260842323303223\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1200/opt4/500word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one like use also / time could get need -   make way even two work first want people might using think may much know 'm = something see good different 1 really example used well question 're case since set however many say still take things probably 've try point problem answer find number another actually enough 2 possible new better value right long without go + let 'll back system 'd power every going \\mathcal able sure look 3 change code less part ca given data second either form around create thing means likely lot give add image best process always high small & space function reason light least though must end start world read page large said bit simply etc following help level based little seems person real rather already trying note 0 often anything mm come made years us word keep non issue lens maybe usually specific life place times edit order url$ assume similar makes looking simple    x c found \\to information group result water run file check type camera size single put someone course done output control field option fact quite god state open solution mean b working key server side far # instead last called general pretty works idea understand human kind great consider sense < list version never making whether object paper % > line due hard difference text i.e. color getting important online character bytes user next else day particular range correct method thus area write amount values common \\\\ experience others energy needs useful position free unless higher easy design full top show fine seem short everything almost comes term results 4 original ask wrong \\ name map away clear test call available generally 5 words 10 old thought although low several wo true error 100 provide close actual big mind target feel believe left input matter sort model become magic bad effect source current got sound    three changes future multiple per nothing standard year access allow book gives language game terms step related setting n e.g. price main easily site body along support difficult normal yes certain tell company taking hand stop turn within view avoid class anyone running story post children de money takes ability quality lower necessary noise seen uses search history major numbers happen knowledge theory job past perhaps cause needed link approach    problems points cases humans smaller exactly days examples build gets sometimes speed length whole choose move project questions 6 planet english risk product remember situation explanation especially characters focus research tried air modern men rules ' personal looks states flash whatever yet attack distance assuming exact options guess larger weapon update context total sequence wide half play offer cost mentioned depends structure says lenses food aperture factor layer fairly require completely logic   meaning weapons directly extra mostly writing longer inside added signal later expected background elements block car reasons conditions exist self degree direct known created issues frame reading subject computer\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.9698 - accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0191s vs `on_train_batch_end` time: 0.0328s). Check your callbacks.\n",
      " 80/100 [=======================>......] - ETA: 1s - loss: 0.7088 - accuracy: 0.5555WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 81/100 [=======================>......] - 6s 68ms/step - loss: 0.7088 - accuracy: 0.5548 - val_loss: 0.7008 - val_accuracy: 0.5594\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.7426 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_test_batch_end` time: 0.0146s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7077 - accuracy: 0.4297\n",
      "training acc:  0.5547839403152466 , training loss:  0.7087572813034058 , val acc:  0.559374988079071 , val loss:  0.7008455395698547 , test acc:  0.42973992228507996 , test loss:  0.707720935344696\n",
      "\n",
      "Started evaluation for: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1400.csv\n",
      "Loading training file from path:  /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1400.csv\n",
      "(1201, 6)\n",
      "Loading testing file from path:  /home/sukanya/PhD/Datasets/PAN SCD/pan19-style-change-detection/processed/test.csv\n",
      "(9538, 5)\n",
      "opt1\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt1/50word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 1.4145 - accuracy: 0.5938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_train_batch_end` time: 0.0636s). Check your callbacks.\n",
      " 60/100 [=================>............] - ETA: 2s - loss: 1.0505 - accuracy: 0.5396WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 77ms/step - loss: 1.0505 - accuracy: 0.5396 - val_loss: 0.7783 - val_accuracy: 0.4542\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.8151 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0143s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7160 - accuracy: 0.4584\n",
      "training acc:  0.5395833253860474 , training loss:  1.050505518913269 , val acc:  0.4541666805744171 , val loss:  0.7783229351043701 , test acc:  0.45836830139160156 , test loss:  0.7159535884857178\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt1/100word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 60/100 [=================>............] - ETA: 2s - loss: 1.2575 - accuracy: 0.5396WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 76ms/step - loss: 1.2575 - accuracy: 0.5396 - val_loss: 0.8068 - val_accuracy: 0.4583\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.8408 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0083s vs `on_test_batch_end` time: 0.0124s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7183 - accuracy: 0.4710\n",
      "training acc:  0.5395833253860474 , training loss:  1.2575242519378662 , val acc:  0.4583333432674408 , val loss:  0.8067716360092163 , test acc:  0.47095218300819397 , test loss:  0.7182931900024414\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt1/150word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me know something see = used really good 'm different each now 1 its well example being question am since case had many probably were however been those set 're who things take try still say after point his 've between problem both possible enough actually new number find 2 better\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 1.3762 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_train_batch_end` time: 0.0438s). Check your callbacks.\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.8703 - accuracy: 0.5371WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 76ms/step - loss: 0.8677 - accuracy: 0.5323 - val_loss: 0.7071 - val_accuracy: 0.3625\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6936 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0124s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6826 - accuracy: 0.5964\n",
      "training acc:  0.5322916507720947 , training loss:  0.8677094578742981 , val acc:  0.36250001192092896 , val loss:  0.7070704102516174 , test acc:  0.5963716506958008 , test loss:  0.6825581789016724\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt1/200word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me know something see = used really good 'm different each now 1 its well example being question am since case had many probably were however been those set 're who things take try still say after point his 've between problem both possible enough actually new number find 2 better another answer go did too over why long our through without while system power let value + 'd before back right down data 'll able given every look going having code 3 image less either ca off process change thing sure part around space best create high own function give\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.7824 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0159s vs `on_train_batch_end` time: 0.0416s). Check your callbacks.\n",
      " 60/100 [=================>............] - ETA: 2s - loss: 0.7018 - accuracy: 0.4979WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 76ms/step - loss: 0.7018 - accuracy: 0.4979 - val_loss: 0.6959 - val_accuracy: 0.4583\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.6983 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0187s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6934 - accuracy: 0.4744\n",
      "training acc:  0.49791666865348816 , training loss:  0.7018256783485413 , val acc:  0.4583333432674408 , val loss:  0.6959293484687805 , test acc:  0.474412739276886 , test loss:  0.6933984160423279\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt1/250word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me know something see = used really good 'm different each now 1 its well example being question am since case had many probably were however been those set 're who things take try still say after point his 've between problem both possible enough actually new number find 2 better another answer go did too over why long our through without while system power let value + 'd before back right down data 'll able given every look going having code 3 image less either ca off process change thing sure part around space best create high own function give second always & small means mm lot light reason add likely must above lens maybe least simply form start large read etc level few said though end world x bit page help based little non following trying often real \\mathcal rather person years already once specific    life order usually\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 2.0951 - accuracy: 0.5312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0098s vs `on_train_batch_end` time: 0.0545s). Check your callbacks.\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 1.4128 - accuracy: 0.5381 ETA: 3s - loss: 1.5405 WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 77ms/step - loss: 1.4025 - accuracy: 0.5396 - val_loss: 0.9469 - val_accuracy: 0.4583\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.9917 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0160s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7747 - accuracy: 0.4414\n",
      "training acc:  0.5395833253860474 , training loss:  1.4025102853775024 , val acc:  0.4583333432674408 , val loss:  0.9469015598297119 , test acc:  0.44138002395629883 , test loss:  0.7747094631195068\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt1/300word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me know something see = used really good 'm different each now 1 its well example being question am since case had many probably were however been those set 're who things take try still say after point his 've between problem both possible enough actually new number find 2 better another answer go did too over why long our through without while system power let value + 'd before back right down data 'll able given every look going having code 3 image less either ca off process change thing sure part around space best create high own function give second always & small means mm lot light reason add likely must above lens maybe least simply form start large read etc level few said though end world x bit page help based little non following trying often real \\mathcal rather person years already once specific    life order usually water result option again anything made url$ keep doing seems note come simple type makes us she paper times information single place run b # under similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 1.7275 - accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0198s vs `on_train_batch_end` time: 0.0665s). Check your callbacks.\n",
      " 60/100 [=================>............] - ETA: 2s - loss: 1.0595 - accuracy: 0.5396WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 76ms/step - loss: 1.0595 - accuracy: 0.5396 - val_loss: 0.7761 - val_accuracy: 0.4542\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.7979 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0152s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7055 - accuracy: 0.4846\n",
      "training acc:  0.5395833253860474 , training loss:  1.0594686269760132 , val acc:  0.4541666805744171 , val loss:  0.7761000990867615 , test acc:  0.4845847189426422 , test loss:  0.7055237293243408\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt1/350word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me know something see = used really good 'm different each now 1 its well example being question am since case had many probably were however been those set 're who things take try still say after point his 've between problem both possible enough actually new number find 2 better another answer go did too over why long our through without while system power let value + 'd before back right down data 'll able given every look going having code 3 image less either ca off process change thing sure part around space best create high own function give second always & small means mm lot light reason add likely must above lens maybe least simply form start large read etc level few said though end world x bit page help based little non following trying often real \\mathcal rather person years already once specific    life order usually water result option again anything made url$ keep doing seems note come simple type makes us she paper times information single place run b # under similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line her never him particular open difference c whether position amount hard itself until \\\\ target area online\n",
      "Epoch 1/30\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.6973 - accuracy: 0.5074WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 76ms/step - loss: 0.6967 - accuracy: 0.5104 - val_loss: 0.7064 - val_accuracy: 0.4583\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7416 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0234s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7111 - accuracy: 0.4123\n",
      "training acc:  0.5104166865348816 , training loss:  0.6966927647590637 , val acc:  0.4583333432674408 , val loss:  0.7063636183738708 , test acc:  0.4123322069644928 , test loss:  0.7110500335693359\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt1/400word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me know something see = used really good 'm different each now 1 its well example being question am since case had many probably were however been those set 're who things take try still say after point his 've between problem both possible enough actually new number find 2 better another answer go did too over why long our through without while system power let value + 'd before back right down data 'll able given every look going having code 3 image less either ca off process change thing sure part around space best create high own function give second always & small means mm lot light reason add likely must above lens maybe least simply form start large read etc level few said though end world x bit page help based little non following trying often real \\mathcal rather person years already once specific    life order usually water result option again anything made url$ keep doing seems note come simple type makes us she paper times information single place run b # under similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line her never him particular open difference c whether position amount hard itself until \\\\ target area online available server day design almost below thus user else character higher \\to energy range top full method game short next text object low several actual unless close easy useful wo needs others provide generally results call test got 10 seem 5 noise everything show write comes true values thought away\n",
      "Epoch 1/30\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.7114 - accuracy: 0.4788WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 77ms/step - loss: 0.7110 - accuracy: 0.4792 - val_loss: 0.6971 - val_accuracy: 0.4583\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.7278 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0138s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7096 - accuracy: 0.4123\n",
      "training acc:  0.4791666567325592 , training loss:  0.7110346555709839 , val acc:  0.4583333432674408 , val loss:  0.6970602869987488 , test acc:  0.4123322069644928 , test loss:  0.7095770835876465\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt1/450word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me know something see = used really good 'm different each now 1 its well example being question am since case had many probably were however been those set 're who things take try still say after point his 've between problem both possible enough actually new number find 2 better another answer go did too over why long our through without while system power let value + 'd before back right down data 'll able given every look going having code 3 image less either ca off process change thing sure part around space best create high own function give second always & small means mm lot light reason add likely must above lens maybe least simply form start large read etc level few said though end world x bit page help based little non following trying often real \\mathcal rather person years already once specific    life order usually water result option again anything made url$ keep doing seems note come simple type makes us she paper times information single place run b # under similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line her never him particular open difference c whether position amount hard itself until \\\\ target area online available server day design almost below thus user else character higher \\to energy range top full method game short next text object low several actual unless close easy useful wo needs others provide generally results call test got 10 seem 5 noise everything show write comes true values thought away wrong big access old left clear term words fine free language current cases 100    against along common multiple gives class certain ask although per believe 4 support feel quality book sort bad e.g. matter original map uses three story become within body effect exactly experience future easily avoid standard\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.6913 - accuracy: 0.5085 ETA: 3s - loss: 0.6WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 77ms/step - loss: 0.6914 - accuracy: 0.5083 - val_loss: 0.7044 - val_accuracy: 0.4583\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7100 - accuracy: 0.4123\n",
      "training acc:  0.5083333253860474 , training loss:  0.691372811794281 , val acc:  0.4583333432674408 , val loss:  0.7043892741203308 , test acc:  0.4123322069644928 , test loss:  0.7099668383598328\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt1/500word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me know something see = used really good 'm different each now 1 its well example being question am since case had many probably were however been those set 're who things take try still say after point his 've between problem both possible enough actually new number find 2 better another answer go did too over why long our through without while system power let value + 'd before back right down data 'll able given every look going having code 3 image less either ca off process change thing sure part around space best create high own function give second always & small means mm lot light reason add likely must above lens maybe least simply form start large read etc level few said though end world x bit page help based little non following trying often real \\mathcal rather person years already once specific    life order usually water result option again anything made url$ keep doing seems note come simple type makes us she paper times information single place run b # under similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line her never him particular open difference c whether position amount hard itself until \\\\ target area online available server day design almost below thus user else character higher \\to energy range top full method game short next text object low several actual unless close easy useful wo needs others provide generally results call test got 10 seem 5 noise everything show write comes true values thought away wrong big access old left clear term words fine free language current cases 100    against along common multiple gives class certain ask although per believe 4 support feel quality book sort bad e.g. matter original map uses three story become within body effect exactly experience future easily avoid standard name price main yes step source money mind allow input post changes lenses weapon air nothing planet \\ magic error search numbers n year site cost related ability history tell s speed car turn happen product hand remember difficult setting running english situation choose terms seen model necessary whatever problems\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.7198 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0309s vs `on_train_batch_end` time: 0.0519s). Check your callbacks.\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.6991 - accuracy: 0.4852WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 78ms/step - loss: 0.6996 - accuracy: 0.4823 - val_loss: 0.7054 - val_accuracy: 0.4583\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.7268 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0142s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7021 - accuracy: 0.4123\n",
      "training acc:  0.48229166865348816 , training loss:  0.6995924711227417 , val acc:  0.4583333432674408 , val loss:  0.7053911685943604 , test acc:  0.4123322069644928 , test loss:  0.7020808458328247\n",
      "\n",
      "opt2\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt2/50word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.7118 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0496s). Check your callbacks.\n",
      " 60/100 [=================>............] - ETA: 2s - loss: 0.6988 - accuracy: 0.5375 ETA: 3s - loss: 0.6WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 4s 74ms/step - loss: 0.6988 - accuracy: 0.5375 - val_loss: 0.6961 - val_accuracy: 0.4542\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6992 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0135s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6938 - accuracy: 0.4526\n",
      "training acc:  0.5375000238418579 , training loss:  0.6988499164581299 , val acc:  0.4541666805744171 , val loss:  0.6961435675621033 , test acc:  0.4526006579399109 , test loss:  0.6938014626502991\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt2/100word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 1.5531 - accuracy: 0.2812WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0104s vs `on_train_batch_end` time: 0.0595s). Check your callbacks.\n",
      " 60/100 [=================>............] - ETA: 2s - loss: 0.7746 - accuracy: 0.5156WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 77ms/step - loss: 0.7746 - accuracy: 0.5156 - val_loss: 0.6929 - val_accuracy: 0.5417\n",
      "  1/596 [..............................] - ETA: 0s - loss: 0.6699 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0153s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6838 - accuracy: 0.5877\n",
      "training acc:  0.515625 , training loss:  0.774566650390625 , val acc:  0.5416666865348816 , val loss:  0.6929475665092468 , test acc:  0.5876677632331848 , test loss:  0.6837533116340637\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt2/150word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me know something see = used really good 'm different each now 1 its well example being question am since case had many probably were however been those set 're who things take try still say after point his 've between problem both possible enough actually new number find\n",
      "Epoch 1/30\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.7014 - accuracy: 0.5222WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 77ms/step - loss: 0.7012 - accuracy: 0.5271 - val_loss: 0.6888 - val_accuracy: 0.6708\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6953 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0155s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6979 - accuracy: 0.4119\n",
      "training acc:  0.5270833373069763 , training loss:  0.7012328505516052 , val acc:  0.6708333492279053 , val loss:  0.6887890100479126 , test acc:  0.411912739276886 , test loss:  0.6979210376739502\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt2/200word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me know something see = used really good 'm different each now 1 its well example being question am since case had many probably were however been those set 're who things take try still say after point his 've between problem both possible enough actually new number find 2 better another answer go did too over why long our through without while system power let value + 'd before back right down data 'll able given every look going having code 3 image less either ca off process change thing sure part around space best create high own\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.9942 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0620s). Check your callbacks.\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.7505 - accuracy: 0.4735WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 76ms/step - loss: 0.7495 - accuracy: 0.4698 - val_loss: 0.6978 - val_accuracy: 0.3333\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6843 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_test_batch_end` time: 0.0164s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6856 - accuracy: 0.5742\n",
      "training acc:  0.4697916805744171 , training loss:  0.7495485544204712 , val acc:  0.3333333432674408 , val loss:  0.6978141665458679 , test acc:  0.5742449760437012 , test loss:  0.6856366991996765\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt2/250word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me know something see = used really good 'm different each now 1 its well example being question am since case had many probably were however been those set 're who things take try still say after point his 've between problem both possible enough actually new number find 2 better another answer go did too over why long our through without while system power let value + 'd before back right down data 'll able given every look going having code 3 image less either ca off process change thing sure part around space best create high own function give second always & small means mm lot light reason add likely must above lens maybe least simply form start large read etc level few said though end world x bit page help based little non following trying often real \\mathcal rather person years already once specific    life\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.7938 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0061s vs `on_train_batch_end` time: 0.0575s). Check your callbacks.\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.6943 - accuracy: 0.5265WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 78ms/step - loss: 0.6946 - accuracy: 0.5219 - val_loss: 0.6940 - val_accuracy: 0.4583\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.7113 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_test_batch_end` time: 0.0170s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7020 - accuracy: 0.4123\n",
      "training acc:  0.5218750238418579 , training loss:  0.6945773959159851 , val acc:  0.4583333432674408 , val loss:  0.6940411329269409 , test acc:  0.4123322069644928 , test loss:  0.7019668221473694\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt2/300word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me know something see = used really good 'm different each now 1 its well example being question am since case had many probably were however been those set 're who things take try still say after point his 've between problem both possible enough actually new number find 2 better another answer go did too over why long our through without while system power let value + 'd before back right down data 'll able given every look going having code 3 image less either ca off process change thing sure part around space best create high own function give second always & small means mm lot light reason add likely must above lens maybe least simply form start large read etc level few said though end world x bit page help based little non following trying often real \\mathcal rather person years already once specific    life order usually water result option again anything made url$ keep doing seems note come simple type makes us she paper times information single place run b # under similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 60/100 [=================>............] - ETA: 2s - loss: 0.7355 - accuracy: 0.5208WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 77ms/step - loss: 0.7355 - accuracy: 0.5208 - val_loss: 0.6825 - val_accuracy: 0.5875\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.6871 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0178s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6992 - accuracy: 0.4075\n",
      "training acc:  0.5208333134651184 , training loss:  0.7355043888092041 , val acc:  0.5874999761581421 , val loss:  0.6825238466262817 , test acc:  0.40750840306282043 , test loss:  0.6991755962371826\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt2/350word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me know something see = used really good 'm different each now 1 its well example being question am since case had many probably were however been those set 're who things take try still say after point his 've between problem both possible enough actually new number find 2 better another answer go did too over why long our through without while system power let value + 'd before back right down data 'll able given every look going having code 3 image less either ca off process change thing sure part around space best create high own function give second always & small means mm lot light reason add likely must above lens maybe least simply form start large read etc level few said though end world x bit page help based little non following trying often real \\mathcal rather person years already once specific    life order usually water result option again anything made url$ keep doing seems note come simple type makes us she paper times information single place run b # under similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line her never him particular open difference c whether position amount hard itself until \\\\ target\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.8127 - accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_train_batch_end` time: 0.0586s). Check your callbacks.\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.7053 - accuracy: 0.5286WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 78ms/step - loss: 0.7051 - accuracy: 0.5219 - val_loss: 0.6947 - val_accuracy: 0.3375\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6900 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0130s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6905 - accuracy: 0.5748\n",
      "training acc:  0.5218750238418579 , training loss:  0.7051403522491455 , val acc:  0.3375000059604645 , val loss:  0.6947200298309326 , test acc:  0.5747693181037903 , test loss:  0.6904650330543518\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt2/400word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me know something see = used really good 'm different each now 1 its well example being question am since case had many probably were however been those set 're who things take try still say after point his 've between problem both possible enough actually new number find 2 better another answer go did too over why long our through without while system power let value + 'd before back right down data 'll able given every look going having code 3 image less either ca off process change thing sure part around space best create high own function give second always & small means mm lot light reason add likely must above lens maybe least simply form start large read etc level few said though end world x bit page help based little non following trying often real \\mathcal rather person years already once specific    life order usually water result option again anything made url$ keep doing seems note come simple type makes us she paper times information single place run b # under similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line her never him particular open difference c whether position amount hard itself until \\\\ target area online available server day design almost below thus user else character higher \\to energy range top full method game short next text object low several actual unless close easy useful wo needs others provide generally results call test got 10 seem 5 noise everything show write comes true values\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 1.5090 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0252s vs `on_train_batch_end` time: 0.0499s). Check your callbacks.\n",
      " 60/100 [=================>............] - ETA: 2s - loss: 0.8102 - accuracy: 0.4906 ETA: 2s - loss: 0.8190 - accuracy: WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 78ms/step - loss: 0.8102 - accuracy: 0.4906 - val_loss: 0.6971 - val_accuracy: 0.4583\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7445 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_test_batch_end` time: 0.0056s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7201 - accuracy: 0.41230s - loss: 0.7199 - ac\n",
      "training acc:  0.4906249940395355 , training loss:  0.8102197051048279 , val acc:  0.4583333432674408 , val loss:  0.6971012353897095 , test acc:  0.4123322069644928 , test loss:  0.7200770378112793\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt2/450word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me know something see = used really good 'm different each now 1 its well example being question am since case had many probably were however been those set 're who things take try still say after point his 've between problem both possible enough actually new number find 2 better another answer go did too over why long our through without while system power let value + 'd before back right down data 'll able given every look going having code 3 image less either ca off process change thing sure part around space best create high own function give second always & small means mm lot light reason add likely must above lens maybe least simply form start large read etc level few said though end world x bit page help based little non following trying often real \\mathcal rather person years already once specific    life order usually water result option again anything made url$ keep doing seems note come simple type makes us she paper times information single place run b # under similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line her never him particular open difference c whether position amount hard itself until \\\\ target area online available server day design almost below thus user else character higher \\to energy range top full method game short next text object low several actual unless close easy useful wo needs others provide generally results call test got 10 seem 5 noise everything show write comes true values thought away wrong big access old left clear term words fine free language current cases 100    against along common multiple gives class certain ask although per believe 4 support feel quality book sort bad e.g. matter original map uses three story become within body effect exactly experience future easily\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6954 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0333s vs `on_train_batch_end` time: 0.0570s). Check your callbacks.\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.6955 - accuracy: 0.5286 ETA: 4s - loss: 0.6937 WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 79ms/step - loss: 0.6954 - accuracy: 0.5281 - val_loss: 0.6806 - val_accuracy: 0.5625\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6943 - accuracy: 0.4984\n",
      "training acc:  0.528124988079071 , training loss:  0.695418119430542 , val acc:  0.5625 , val loss:  0.6806194186210632 , test acc:  0.49842700362205505 , test loss:  0.6943073868751526\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt2/500word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you i it $ for be this with as   on are have not or can if would but your 's so an n't do from at will they there by one my which more what all was some we when then use like / has about also just could time no only any get other than them how their need up out does should - where even way make same because very   two want might these work people into may first here think he much using most such me know something see = used really good 'm different each now 1 its well example being question am since case had many probably were however been those set 're who things take try still say after point his 've between problem both possible enough actually new number find 2 better another answer go did too over why long our through without while system power let value + 'd before back right down data 'll able given every look going having code 3 image less either ca off process change thing sure part around space best create high own function give second always & small means mm lot light reason add likely must above lens maybe least simply form start large read etc level few said though end world x bit page help based little non following trying often real \\mathcal rather person years already once specific    life order usually water result option again anything made url$ keep doing seems note come simple type makes us she paper times information single place run b # under similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line her never him particular open difference c whether position amount hard itself until \\\\ target area online available server day design almost below thus user else character higher \\to energy range top full method game short next text object low several actual unless close easy useful wo needs others provide generally results call test got 10 seem 5 noise everything show write comes true values thought away wrong big access old left clear term words fine free language current cases 100    against along common multiple gives class certain ask although per believe 4 support feel quality book sort bad e.g. matter original map uses three story become within body effect exactly experience future easily avoid standard name price main yes step source money mind allow input post changes lenses weapon air nothing planet \\ magic error search numbers n year site cost related ability history tell s speed car turn happen product hand remember difficult setting running english situation choose terms seen model necessary\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 4s - loss: 1.1368 - accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0104s vs `on_train_batch_end` time: 0.0539s). Check your callbacks.\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.7845 - accuracy: 0.4778WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 78ms/step - loss: 0.7832 - accuracy: 0.4729 - val_loss: 0.7004 - val_accuracy: 0.2583\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6826 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_test_batch_end` time: 0.0246s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6831 - accuracy: 0.5711\n",
      "training acc:  0.4729166626930237 , training loss:  0.7832154631614685 , val acc:  0.25833332538604736 , val loss:  0.7004486918449402 , test acc:  0.5710989832878113 , test loss:  0.6831328272819519\n",
      "\n",
      "opt3\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt3/50word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're\n",
      "Epoch 1/30\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.7381 - accuracy: 0.5328WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 4s 75ms/step - loss: 0.7375 - accuracy: 0.5302 - val_loss: 0.6669 - val_accuracy: 0.7250\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.7355 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0141s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7010 - accuracy: 0.4718\n",
      "training acc:  0.5302083492279053 , training loss:  0.7374744415283203 , val acc:  0.7250000238418579 , val loss:  0.6669413447380066 , test acc:  0.4717911183834076 , test loss:  0.7010419964790344\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt3/100word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6958 - accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0083s vs `on_train_batch_end` time: 0.0345s). Check your callbacks.\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.6883 - accuracy: 0.5805WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 75ms/step - loss: 0.6884 - accuracy: 0.5813 - val_loss: 0.6638 - val_accuracy: 0.6917\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7165 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0200s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6930 - accuracy: 0.4950\n",
      "training acc:  0.581250011920929 , training loss:  0.6883987188339233 , val acc:  0.6916666626930237 , val loss:  0.6638374328613281 , test acc:  0.49496644735336304 , test loss:  0.6930312514305115\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt3/150word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best create high function give second always & small means mm lot light reason add likely must lens maybe least simply form start large read etc level said though end world x bit page help based little non following trying often real \\mathcal rather person years already specific    life order\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7910 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0096s vs `on_train_batch_end` time: 0.0364s). Check your callbacks.\n",
      " 60/100 [=================>............] - ETA: 2s - loss: 0.7303 - accuracy: 0.4948WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 75ms/step - loss: 0.7303 - accuracy: 0.4948 - val_loss: 0.6749 - val_accuracy: 0.5417\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.6545 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0143s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6825 - accuracy: 0.5877\n",
      "training acc:  0.4947916567325592 , training loss:  0.7303359508514404 , val acc:  0.5416666865348816 , val loss:  0.67486172914505 , test acc:  0.5876677632331848 , test loss:  0.6825218796730042\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt3/200word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best create high function give second always & small means mm lot light reason add likely must lens maybe least simply form start large read etc level said though end world x bit page help based little non following trying often real \\mathcal rather person years already specific    life order usually water result option anything made url$ keep seems note come simple type makes us paper times information single place run b # similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7275 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0209s vs `on_train_batch_end` time: 0.0501s). Check your callbacks.\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.8355 - accuracy: 0.5032WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 75ms/step - loss: 0.8311 - accuracy: 0.5083 - val_loss: 0.6224 - val_accuracy: 0.6917\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.7359 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0150s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6949 - accuracy: 0.4967\n",
      "training acc:  0.5083333253860474 , training loss:  0.8311228156089783 , val acc:  0.6916666626930237 , val loss:  0.6224166750907898 , test acc:  0.4967491626739502 , test loss:  0.6949416399002075\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt3/250word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best create high function give second always & small means mm lot light reason add likely must lens maybe least simply form start large read etc level said though end world x bit page help based little non following trying often real \\mathcal rather person years already specific    life order usually water result option anything made url$ keep seems note come simple type makes us paper times information single place run b # similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line never particular open difference c whether position amount hard \\\\ target area online available server day design almost thus user\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.7762 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0095s vs `on_train_batch_end` time: 0.0262s). Check your callbacks.\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.7044 - accuracy: 0.4544WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 76ms/step - loss: 0.7042 - accuracy: 0.4531 - val_loss: 0.6831 - val_accuracy: 0.5625\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6894 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0203s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6903 - accuracy: 0.5000\n",
      "training acc:  0.453125 , training loss:  0.7042080760002136 , val acc:  0.5625 , val loss:  0.6831431984901428 , test acc:  0.5 , test loss:  0.6902651190757751\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt3/300word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best create high function give second always & small means mm lot light reason add likely must lens maybe least simply form start large read etc level said though end world x bit page help based little non following trying often real \\mathcal rather person years already specific    life order usually water result option anything made url$ keep seems note come simple type makes us paper times information single place run b # similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line never particular open difference c whether position amount hard \\\\ target area online available server day design almost thus user else character higher \\to energy range top full method game short next text object low several actual unless close easy useful wo needs others provide generally results call test got 10 seem 5 noise everything show write comes true values thought away wrong big access old left clear term words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.6930 - accuracy: 0.5169WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 76ms/step - loss: 0.6928 - accuracy: 0.5167 - val_loss: 0.6562 - val_accuracy: 0.5875\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6859 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_test_batch_end` time: 0.0134s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6852 - accuracy: 0.5078\n",
      "training acc:  0.5166666507720947 , training loss:  0.6927904486656189 , val acc:  0.5874999761581421 , val loss:  0.6561911702156067 , test acc:  0.5077600479125977 , test loss:  0.6852439641952515\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt3/350word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best create high function give second always & small means mm lot light reason add likely must lens maybe least simply form start large read etc level said though end world x bit page help based little non following trying often real \\mathcal rather person years already specific    life order usually water result option anything made url$ keep seems note come simple type makes us paper times information single place run b # similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line never particular open difference c whether position amount hard \\\\ target area online available server day design almost thus user else character higher \\to energy range top full method game short next text object low several actual unless close easy useful wo needs others provide generally results call test got 10 seem 5 noise everything show write comes true values thought away wrong big access old left clear term words fine free language current cases 100    along common multiple gives class certain ask although per believe 4 support feel quality book sort bad e.g. matter original map uses three story become within body effect exactly experience future easily avoid standard name price main yes step source money mind allow\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.6384 - accuracy: 0.5938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0139s vs `on_train_batch_end` time: 0.0518s). Check your callbacks.\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.6954 - accuracy: 0.5392WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 4s 75ms/step - loss: 0.6942 - accuracy: 0.5458 - val_loss: 0.6576 - val_accuracy: 0.7125\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.7230 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0151s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6955 - accuracy: 0.4893\n",
      "training acc:  0.5458333492279053 , training loss:  0.6942378282546997 , val acc:  0.7124999761581421 , val loss:  0.6576290726661682 , test acc:  0.48930367827415466 , test loss:  0.6955061554908752\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt3/400word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best create high function give second always & small means mm lot light reason add likely must lens maybe least simply form start large read etc level said though end world x bit page help based little non following trying often real \\mathcal rather person years already specific    life order usually water result option anything made url$ keep seems note come simple type makes us paper times information single place run b # similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line never particular open difference c whether position amount hard \\\\ target area online available server day design almost thus user else character higher \\to energy range top full method game short next text object low several actual unless close easy useful wo needs others provide generally results call test got 10 seem 5 noise everything show write comes true values thought away wrong big access old left clear term words fine free language current cases 100    along common multiple gives class certain ask although per believe 4 support feel quality book sort bad e.g. matter original map uses three story become within body effect exactly experience future easily avoid standard name price main yes step source money mind allow input post changes lenses weapon air nothing planet \\ magic error search numbers n year site cost related ability history tell speed car turn happen product hand remember difficult setting running english situation choose terms seen model necessary whatever problems computer company return layer children de taking approach stop explanation\n",
      "Epoch 1/30\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.7297 - accuracy: 0.4248WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 76ms/step - loss: 0.7293 - accuracy: 0.4229 - val_loss: 0.6865 - val_accuracy: 0.5417\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6747 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0155s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6880 - accuracy: 0.5877\n",
      "training acc:  0.4229166805744171 , training loss:  0.7293333411216736 , val acc:  0.5416666865348816 , val loss:  0.6865359544754028 , test acc:  0.5876677632331848 , test loss:  0.6880434155464172\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt3/450word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best create high function give second always & small means mm lot light reason add likely must lens maybe least simply form start large read etc level said though end world x bit page help based little non following trying often real \\mathcal rather person years already specific    life order usually water result option anything made url$ keep seems note come simple type makes us paper times information single place run b # similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line never particular open difference c whether position amount hard \\\\ target area online available server day design almost thus user else character higher \\to energy range top full method game short next text object low several actual unless close easy useful wo needs others provide generally results call test got 10 seem 5 noise everything show write comes true values thought away wrong big access old left clear term words fine free language current cases 100    along common multiple gives class certain ask although per believe 4 support feel quality book sort bad e.g. matter original map uses three story become within body effect exactly experience future easily avoid standard name price main yes step source money mind allow input post changes lenses weapon air nothing planet \\ magic error search numbers n year site cost related ability history tell speed car turn happen product hand remember difficult setting running english situation choose terms seen model necessary whatever problems computer company return layer children de taking approach stop explanation offer normal total sometimes risk personal cause lower needed smaller past build link view larger move focus examples knowledge job humans men black elements perhaps weapons depends later surface fairly characters context length says select _ options earth longer whole reach anyone points questions require produce    days inside ground\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.6845 - accuracy: 0.5466WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 75ms/step - loss: 0.6847 - accuracy: 0.5448 - val_loss: 0.6614 - val_accuracy: 0.7042\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.7083 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0147s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6940 - accuracy: 0.4926\n",
      "training acc:  0.5447916388511658 , training loss:  0.6847331523895264 , val acc:  0.7041666507720947 , val loss:  0.6613656282424927 , test acc:  0.49255451560020447 , test loss:  0.6939864158630371\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt3/500word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best create high function give second always & small means mm lot light reason add likely must lens maybe least simply form start large read etc level said though end world x bit page help based little non following trying often real \\mathcal rather person years already specific    life order usually water result option anything made url$ keep seems note come simple type makes us paper times information single place run b # similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line never particular open difference c whether position amount hard \\\\ target area online available server day design almost thus user else character higher \\to energy range top full method game short next text object low several actual unless close easy useful wo needs others provide generally results call test got 10 seem 5 noise everything show write comes true values thought away wrong big access old left clear term words fine free language current cases 100    along common multiple gives class certain ask although per believe 4 support feel quality book sort bad e.g. matter original map uses three story become within body effect exactly experience future easily avoid standard name price main yes step source money mind allow input post changes lenses weapon air nothing planet \\ magic error search numbers n year site cost related ability history tell speed car turn happen product hand remember difficult setting running english situation choose terms seen model necessary whatever problems computer company return layer children de taking approach stop explanation offer normal total sometimes risk personal cause lower needed smaller past build link view larger move focus examples knowledge job humans men black elements perhaps weapons depends later surface fairly characters context length says select _ options earth longer whole reach anyone points questions require produce    days inside ground food block functions expected takes reasons looks mentioned research half theory especially states background factor gets otherwise rules child sensor technology guess major subject photoshop 6 tried directly issues ' outside together meaning becomes sound zero yet distance exist slightly flash created assuming program modern wide ever play negative action\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6744 - accuracy: 0.5938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0369s). Check your callbacks.\n",
      " 60/100 [=================>............] - ETA: 2s - loss: 0.6862 - accuracy: 0.5385WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 75ms/step - loss: 0.6862 - accuracy: 0.5385 - val_loss: 0.6778 - val_accuracy: 0.7208\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.7149 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0148s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6980 - accuracy: 0.4573\n",
      "training acc:  0.5385416746139526 , training loss:  0.6861937642097473 , val acc:  0.7208333611488342 , val loss:  0.6778294444084167 , test acc:  0.45731961727142334 , test loss:  0.6980486512184143\n",
      "\n",
      "opt4\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt4/50word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however\n",
      "Epoch 1/30\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.6967 - accuracy: 0.5085WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 75ms/step - loss: 0.6966 - accuracy: 0.5094 - val_loss: 0.6876 - val_accuracy: 0.7625\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6966 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_test_batch_end` time: 0.0149s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6969 - accuracy: 0.4532\n",
      "training acc:  0.5093749761581421 , training loss:  0.6966085433959961 , val acc:  0.762499988079071 , val loss:  0.6876450181007385 , test acc:  0.4532298743724823 , test loss:  0.6969105005264282\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt4/100word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.7066 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0283s vs `on_train_batch_end` time: 0.0497s). Check your callbacks.\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.6941 - accuracy: 0.4820WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60/100 [=================>............] - 4s 75ms/step - loss: 0.6939 - accuracy: 0.4844 - val_loss: 0.7030 - val_accuracy: 0.4583\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7057 - accuracy: 0.4123\n",
      "training acc:  0.484375 , training loss:  0.6939076781272888 , val acc:  0.4583333432674408 , val loss:  0.7029797434806824 , test acc:  0.4123322069644928 , test loss:  0.70572429895401\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt4/150word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best create high function give second always & small means mm lot light reason add likely must lens maybe least simply form start large read etc level said though end world x bit page help based little non following trying often real \\mathcal rather person years already specific   \n",
      "Epoch 1/30\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.7157 - accuracy: 0.4756WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 4s 75ms/step - loss: 0.7147 - accuracy: 0.4823 - val_loss: 0.7126 - val_accuracy: 0.4458\n",
      "  1/596 [..............................] - ETA: 0s - loss: 0.7235 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0215s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7011 - accuracy: 0.4396\n",
      "training acc:  0.48229166865348816 , training loss:  0.7147087454795837 , val acc:  0.44583332538604736 , val loss:  0.7125842571258545 , test acc:  0.43959730863571167 , test loss:  0.7010924816131592\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt4/200word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best create high function give second always & small means mm lot light reason add likely must lens maybe least simply form start large read etc level said though end world x bit page help based little non following trying often real \\mathcal rather person years already specific    life order usually water result option anything made url$ keep seems note come simple type makes us paper times information single place run b # similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.7474 - accuracy: 0.3438WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0124s vs `on_train_batch_end` time: 0.0312s). Check your callbacks.\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.6967 - accuracy: 0.5286 ETA: 3s - loss: 0.6980 - accuracy:  - ETA: 2s - loss: 0.6974 - ac - ETA: 2s - loss: 0.6966 - accuracy: 0.53WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 75ms/step - loss: 0.6966 - accuracy: 0.5281 - val_loss: 0.6904 - val_accuracy: 0.4583\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6981 - accuracy: 0.4123\n",
      "training acc:  0.528124988079071 , training loss:  0.6965599060058594 , val acc:  0.4583333432674408 , val loss:  0.6904249787330627 , test acc:  0.4123322069644928 , test loss:  0.6980878114700317\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt4/250word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best create high function give second always & small means mm lot light reason add likely must lens maybe least simply form start large read etc level said though end world x bit page help based little non following trying often real \\mathcal rather person years already specific    life order usually water result option anything made url$ keep seems note come simple type makes us paper times information single place run b # similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line never particular open difference c whether position amount hard \\\\ target area online available server day design almost\n",
      "Epoch 1/30\n",
      " 60/100 [=================>............] - ETA: 2s - loss: 0.6958 - accuracy: 0.4969WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 76ms/step - loss: 0.6958 - accuracy: 0.4969 - val_loss: 0.7029 - val_accuracy: 0.4583\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.7227 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0008s vs `on_test_batch_end` time: 0.0156s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7032 - accuracy: 0.4123\n",
      "training acc:  0.49687498807907104 , training loss:  0.6957812905311584 , val acc:  0.4583333432674408 , val loss:  0.7029495239257812 , test acc:  0.4123322069644928 , test loss:  0.7032105326652527\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt4/300word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best create high function give second always & small means mm lot light reason add likely must lens maybe least simply form start large read etc level said though end world x bit page help based little non following trying often real \\mathcal rather person years already specific    life order usually water result option anything made url$ keep seems note come simple type makes us paper times information single place run b # similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line never particular open difference c whether position amount hard \\\\ target area online available server day design almost thus user else character higher \\to energy range top full method game short next text object low several actual unless close easy useful wo needs others provide generally results call test got 10 seem 5 noise everything show write comes true values thought away wrong big access old left clear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6695 - accuracy: 0.5938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0327s vs `on_train_batch_end` time: 0.0503s). Check your callbacks.\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.6953 - accuracy: 0.5413 ETA: 3s - loss: 0WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 76ms/step - loss: 0.6954 - accuracy: 0.5385 - val_loss: 0.6844 - val_accuracy: 0.6000\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6933 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0225s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6961 - accuracy: 0.4400\n",
      "training acc:  0.5385416746139526 , training loss:  0.695372462272644 , val acc:  0.6000000238418579 , val loss:  0.6844333410263062 , test acc:  0.4400167763233185 , test loss:  0.6961333155632019\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt4/350word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best create high function give second always & small means mm lot light reason add likely must lens maybe least simply form start large read etc level said though end world x bit page help based little non following trying often real \\mathcal rather person years already specific    life order usually water result option anything made url$ keep seems note come simple type makes us paper times information single place run b # similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line never particular open difference c whether position amount hard \\\\ target area online available server day design almost thus user else character higher \\to energy range top full method game short next text object low several actual unless close easy useful wo needs others provide generally results call test got 10 seem 5 noise everything show write comes true values thought away wrong big access old left clear term words fine free language current cases 100    along common multiple gives class certain ask although per believe 4 support feel quality book sort bad e.g. matter original map uses three story become within body effect exactly experience future easily avoid standard name price main yes step source money\n",
      "Epoch 1/30\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.6945 - accuracy: 0.5445WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 76ms/step - loss: 0.6944 - accuracy: 0.5469 - val_loss: 0.6910 - val_accuracy: 0.4583\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.7176 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0142s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7041 - accuracy: 0.4123\n",
      "training acc:  0.546875 , training loss:  0.6943641304969788 , val acc:  0.4583333432674408 , val loss:  0.6910374164581299 , test acc:  0.4123322069644928 , test loss:  0.7040586471557617\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt4/400word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best create high function give second always & small means mm lot light reason add likely must lens maybe least simply form start large read etc level said though end world x bit page help based little non following trying often real \\mathcal rather person years already specific    life order usually water result option anything made url$ keep seems note come simple type makes us paper times information single place run b # similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line never particular open difference c whether position amount hard \\\\ target area online available server day design almost thus user else character higher \\to energy range top full method game short next text object low several actual unless close easy useful wo needs others provide generally results call test got 10 seem 5 noise everything show write comes true values thought away wrong big access old left clear term words fine free language current cases 100    along common multiple gives class certain ask although per believe 4 support feel quality book sort bad e.g. matter original map uses three story become within body effect exactly experience future easily avoid standard name price main yes step source money mind allow input post changes lenses weapon air nothing planet \\ magic error search numbers n year site cost related ability history tell speed car turn happen product hand remember difficult setting running english situation choose terms seen model necessary whatever problems computer company return layer children de taking approach\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.7833 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0216s vs `on_train_batch_end` time: 0.0531s). Check your callbacks.\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.7212 - accuracy: 0.4958WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 4s 75ms/step - loss: 0.7208 - accuracy: 0.4958 - val_loss: 0.6935 - val_accuracy: 0.5417\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.6776 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_test_batch_end` time: 0.0204s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6874 - accuracy: 0.5877\n",
      "training acc:  0.4958333373069763 , training loss:  0.72075355052948 , val acc:  0.5416666865348816 , val loss:  0.6935133337974548 , test acc:  0.5876677632331848 , test loss:  0.6873879432678223\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt4/450word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best create high function give second always & small means mm lot light reason add likely must lens maybe least simply form start large read etc level said though end world x bit page help based little non following trying often real \\mathcal rather person years already specific    life order usually water result option anything made url$ keep seems note come simple type makes us paper times information single place run b # similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line never particular open difference c whether position amount hard \\\\ target area online available server day design almost thus user else character higher \\to energy range top full method game short next text object low several actual unless close easy useful wo needs others provide generally results call test got 10 seem 5 noise everything show write comes true values thought away wrong big access old left clear term words fine free language current cases 100    along common multiple gives class certain ask although per believe 4 support feel quality book sort bad e.g. matter original map uses three story become within body effect exactly experience future easily avoid standard name price main yes step source money mind allow input post changes lenses weapon air nothing planet \\ magic error search numbers n year site cost related ability history tell speed car turn happen product hand remember difficult setting running english situation choose terms seen model necessary whatever problems computer company return layer children de taking approach stop explanation offer normal total sometimes risk personal cause lower needed smaller past build link view larger move focus examples knowledge job humans men black elements perhaps weapons depends later surface fairly characters context length says select _ options earth longer whole reach anyone points questions require produce    days\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.7194 - accuracy: 0.5032WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 77ms/step - loss: 0.7186 - accuracy: 0.5083 - val_loss: 0.6805 - val_accuracy: 0.7417\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.7158 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_test_batch_end` time: 0.0224s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.7067 - accuracy: 0.4322\n",
      "training acc:  0.5083333253860474 , training loss:  0.7186223268508911 , val acc:  0.7416666746139526 , val loss:  0.6804588437080383 , test acc:  0.4321518540382385 , test loss:  0.7067349553108215\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1400/opt4/500word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even way make   two want might work people may first think much using know something see = used really good 'm different 1 well example question since case many probably however set 're things take try still say point 've problem possible enough actually new number find 2 better another answer go long without system power let value + 'd back right data 'll able given every look going code 3 image less either ca process change thing sure part around space best create high function give second always & small means mm lot light reason add likely must lens maybe least simply form start large read etc level said though end world x bit page help based little non following trying often real \\mathcal rather person years already specific    life order usually water result option anything made url$ keep seems note come simple type makes us paper times information single place run b # similar fact size control word file looking assume idea far put someone course 0 side color understand field found called pretty check group edit god state done human issue output important mean works instead key quite > sense < version general getting last working % due kind great i.e. correct making solution consider bytes list camera line never particular open difference c whether position amount hard \\\\ target area online available server day design almost thus user else character higher \\to energy range top full method game short next text object low several actual unless close easy useful wo needs others provide generally results call test got 10 seem 5 noise everything show write comes true values thought away wrong big access old left clear term words fine free language current cases 100    along common multiple gives class certain ask although per believe 4 support feel quality book sort bad e.g. matter original map uses three story become within body effect exactly experience future easily avoid standard name price main yes step source money mind allow input post changes lenses weapon air nothing planet \\ magic error search numbers n year site cost related ability history tell speed car turn happen product hand remember difficult setting running english situation choose terms seen model necessary whatever problems computer company return layer children de taking approach stop explanation offer normal total sometimes risk personal cause lower needed smaller past build link view larger move focus examples knowledge job humans men black elements perhaps weapons depends later surface fairly characters context length says select _ options earth longer whole reach anyone points questions require produce    days inside ground food block functions expected takes reasons looks mentioned research half theory especially states background factor gets otherwise rules child sensor technology guess major subject photoshop 6 tried directly issues ' outside together meaning becomes sound zero yet distance exist slightly flash created assuming program modern wide ever play\n",
      "Epoch 1/30\n",
      " 59/100 [================>.............] - ETA: 2s - loss: 0.6955 - accuracy: 0.4650WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 60/100 [=================>............] - 5s 76ms/step - loss: 0.6952 - accuracy: 0.4677 - val_loss: 0.7013 - val_accuracy: 0.4583\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.7264 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0063s vs `on_test_batch_end` time: 0.0137s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7055 - accuracy: 0.4123\n",
      "training acc:  0.4677083194255829 , training loss:  0.6952115297317505 , val acc:  0.4583333432674408 , val loss:  0.701257586479187 , test acc:  0.4123322069644928 , test loss:  0.7054883241653442\n",
      "\n",
      "Started evaluation for: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1600.csv\n",
      "Loading training file from path:  /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1600.csv\n",
      "(920, 6)\n",
      "Loading testing file from path:  /home/sukanya/PhD/Datasets/PAN SCD/pan19-style-change-detection/processed/test.csv\n",
      "(9538, 5)\n",
      "opt1\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt1/50word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when\n",
      "Epoch 1/30\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.9306 - accuracy: 0.5194 ETA: 5s - loss: - ETA: 4s - loss:WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 81ms/step - loss: 0.9257 - accuracy: 0.5190 - val_loss: 0.6684 - val_accuracy: 0.5625\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.6223 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0079s vs `on_test_batch_end` time: 0.0193s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6958 - accuracy: 0.5447\n",
      "training acc:  0.51902174949646 , training loss:  0.9257363080978394 , val acc:  0.5625 , val loss:  0.668365478515625 , test acc:  0.5446728467941284 , test loss:  0.6957716345787048\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt1/100word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 1.0370 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0286s vs `on_train_batch_end` time: 0.0611s). Check your callbacks.\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.7847 - accuracy: 0.4620 ETA: 4sWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 82ms/step - loss: 0.7847 - accuracy: 0.4620 - val_loss: 0.6831 - val_accuracy: 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6815 - accuracy: 0.5877\n",
      "training acc:  0.46195653080940247 , training loss:  0.7846843600273132 , val acc:  0.5625 , val loss:  0.6830618381500244 , test acc:  0.5876677632331848 , test loss:  0.6815452575683594\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt1/150word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know me different really each = something used see good 'm well its being 1 now example been probably question who since many try had case those set were however his point am things 've still 're between say take after problem enough find number another both system new possible too\n",
      "Epoch 1/30\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.7112 - accuracy: 0.5054WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 82ms/step - loss: 0.7112 - accuracy: 0.5054 - val_loss: 0.6799 - val_accuracy: 0.5625\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6388 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0146s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6832 - accuracy: 0.5877\n",
      "training acc:  0.5054348111152649 , training loss:  0.7111828923225403 , val acc:  0.5625 , val loss:  0.6798729300498962 , test acc:  0.5876677632331848 , test loss:  0.6832270622253418\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt1/200word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know me different really each = something used see good 'm well its being 1 now example been probably question who since many try had case those set were however his point am things 've still 're between say take after problem enough find number another both system new possible too actually 2 why better over answer power go did long without while through 'd code our right able back before look let value down 3 every data going either having + image high thing best off create change less given 'll own ca part sure small means give around \\mathcal\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 1.0096 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0641s). Check your callbacks.\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.7989 - accuracy: 0.4932WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 81ms/step - loss: 0.7989 - accuracy: 0.4932 - val_loss: 0.6832 - val_accuracy: 0.5625\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.6310 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0139s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6798 - accuracy: 0.5877\n",
      "training acc:  0.49320653080940247 , training loss:  0.7988969683647156 , val acc:  0.5625 , val loss:  0.6832428574562073 , test acc:  0.5876677632331848 , test loss:  0.6798408031463623\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt1/250word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know me different really each = something used see good 'm well its being 1 now example been probably question who since many try had case those set were however his point am things 've still 're between say take after problem enough find number another both system new possible too actually 2 why better over answer power go did long without while through 'd code our right able back before look let value down 3 every data going either having + image high thing best off create change less given 'll own ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function above mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though once keep already few again person single water\n",
      "Epoch 1/30\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.8201 - accuracy: 0.5190 ETA: 3s - loss: 0.8323 - accuracyWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 82ms/step - loss: 0.8201 - accuracy: 0.5190 - val_loss: 0.6663 - val_accuracy: 0.5852\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6554 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0163s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6994 - accuracy: 0.4663\n",
      "training acc:  0.51902174949646 , training loss:  0.8201160430908203 , val acc:  0.5852272510528564 , val loss:  0.6662544012069702 , test acc:  0.4663380980491638 , test loss:  0.699370801448822\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt1/300word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know me different really each = something used see good 'm well its being 1 now example been probably question who since many try had case those set were however his point am things 've still 're between say take after problem enough find number another both system new possible too actually 2 why better over answer power go did long without while through 'd code our right able back before look let value down 3 every data going either having + image high thing best off create change less given 'll own ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function above mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though once keep already few again person single water result said word come size note url$ life understand similar bytes under rather little trying makes control paper made works doing option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 4s - loss: 0.7058 - accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_end` time: 0.0640s). Check your callbacks.\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.6878 - accuracy: 0.5278WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 82ms/step - loss: 0.6889 - accuracy: 0.5190 - val_loss: 0.6808 - val_accuracy: 0.5852\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6760 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0152s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6942 - accuracy: 0.4863\n",
      "training acc:  0.51902174949646 , training loss:  0.6888502836227417 , val acc:  0.5852272510528564 , val loss:  0.6808022260665894 , test acc:  0.48626258969306946 , test loss:  0.6942383050918579\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt1/350word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know me different really each = something used see good 'm well its being 1 now example been probably question who since many try had case those set were however his point am things 've still 're between say take after problem enough find number another both system new possible too actually 2 why better over answer power go did long without while through 'd code our right able back before look let value down 3 every data going either having + image high thing best off create change less given 'll own ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function above mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though once keep already few again person single water result said word come size note url$ life understand similar bytes under rather little trying makes control paper made works doing option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. him course instead never consider last kind test great target almost working > open solution itself due general user called thus camera key getting character mean method day area whether particular amount hard\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6535 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0594s). Check your callbacks.\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.6902 - accuracy: 0.5167WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 83ms/step - loss: 0.6898 - accuracy: 0.5190 - val_loss: 0.6778 - val_accuracy: 0.5625\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.6655 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_test_batch_end` time: 0.0107s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6919 - accuracy: 0.5509\n",
      "training acc:  0.51902174949646 , training loss:  0.6898269057273865 , val acc:  0.5625 , val loss:  0.6778120398521423 , test acc:  0.5508599281311035 , test loss:  0.691871166229248\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt1/400word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know me different really each = something used see good 'm well its being 1 now example been probably question who since many try had case those set were however his point am things 've still 're between say take after problem enough find number another both system new possible too actually 2 why better over answer power go did long without while through 'd code our right able back before look let value down 3 every data going either having + image high thing best off create change less given 'll own ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function above mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though once keep already few again person single water result said word come size note url$ life understand similar bytes under rather little trying makes control paper made works doing option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. him course instead never consider last kind test great target almost working > open solution itself due general user called thus camera key getting character mean method day area whether particular amount hard she unless range position below field top energy else c text object full avoid against difference results easy true seem available big until low fine higher input values several explanation generally provide actual uses three needs within game access class map smaller clear short cases next language common easily close\n",
      "Epoch 1/30\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 1.4550 - accuracy: 0.4736WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 82ms/step - loss: 1.4353 - accuracy: 0.4810 - val_loss: 0.9313 - val_accuracy: 0.4375\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.9351 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0068s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7517 - accuracy: 0.4446\n",
      "training acc:  0.48097825050354004 , training loss:  1.4352920055389404 , val acc:  0.4375 , val loss:  0.9312584400177002 , test acc:  0.44463086128234863 , test loss:  0.7516772747039795\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt1/450word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know me different really each = something used see good 'm well its being 1 now example been probably question who since many try had case those set were however his point am things 've still 're between say take after problem enough find number another both system new possible too actually 2 why better over answer power go did long without while through 'd code our right able back before look let value down 3 every data going either having + image high thing best off create change less given 'll own ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function above mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though once keep already few again person single water result said word come size note url$ life understand similar bytes under rather little trying makes control paper made works doing option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. him course instead never consider last kind test great target almost working > open solution itself due general user called thus camera key getting character mean method day area whether particular amount hard she unless range position below field top energy else c text object full avoid against difference results easy true seem available big until low fine higher input values several explanation generally provide actual uses three needs within game access class map smaller clear short cases next language common easily close wo 10 4 believe multiple per away body free design magic old matter call words post useful book server left yes feel 5 along ask n ability others larger gives got weapon comes everything 100 support e.g. quality become bad write wrong history thought sort certain term standard source current\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7098 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0079s vs `on_train_batch_end` time: 0.0155s). Check your callbacks.\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.6925 - accuracy: 0.5153 ETA: 4s - loss: 0.6859 - accuracy:  - ETA: 4s - loss: 0.6902 - accuracy - ETA: 4s - loss: 0.6WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 83ms/step - loss: 0.6927 - accuracy: 0.5095 - val_loss: 0.6881 - val_accuracy: 0.6193\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6894 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0085s vs `on_test_batch_end` time: 0.0134s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6948 - accuracy: 0.4181\n",
      "training acc:  0.50951087474823 , training loss:  0.692681610584259 , val acc:  0.6193181872367859 , val loss:  0.688107430934906 , test acc:  0.4180998206138611 , test loss:  0.6948261857032776\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt1/500word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know me different really each = something used see good 'm well its being 1 now example been probably question who since many try had case those set were however his point am things 've still 're between say take after problem enough find number another both system new possible too actually 2 why better over answer power go did long without while through 'd code our right able back before look let value down 3 every data going either having + image high thing best off create change less given 'll own ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function above mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though once keep already few again person single water result said word come size note url$ life understand similar bytes under rather little trying makes control paper made works doing option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. him course instead never consider last kind test great target almost working > open solution itself due general user called thus camera key getting character mean method day area whether particular amount hard she unless range position below field top energy else c text object full avoid against difference results easy true seem available big until low fine higher input values several explanation generally provide actual uses three needs within game access class map smaller clear short cases next language common easily close wo 10 4 believe multiple per away body free design magic old matter call words post useful book server left yes feel 5 along ask n ability others larger gives got weapon comes everything 100 support e.g. quality become bad write wrong history thought sort certain term standard source current weapons year numbers changes show although her build nothing experience hand s story main tell turn mind de site model necessary noise further original step choose money error    happen air perhaps lenses price link allow situation planet men needed problems future related whole matl \\ personal characters children y\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 4s - loss: 0.7418 - accuracy: 0.5312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_end` time: 0.0601s). Check your callbacks.\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.6868 - accuracy: 0.5258 ETA: 5s - l - ETA: 4s - loss: 0.6871 WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 82ms/step - loss: 0.6868 - accuracy: 0.5258 - val_loss: 0.6825 - val_accuracy: 0.6364\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6924 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0239s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7005 - accuracy: 0.3970\n",
      "training acc:  0.5258151888847351 , training loss:  0.6868138313293457 , val acc:  0.6363636255264282 , val loss:  0.6825143098831177 , test acc:  0.3970218002796173 , test loss:  0.7004868388175964\n",
      "\n",
      "opt2\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt2/50word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7467 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0170s vs `on_train_batch_end` time: 0.0262s). Check your callbacks.\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.6904 - accuracy: 0.5204WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 80ms/step - loss: 0.6904 - accuracy: 0.5204 - val_loss: 0.6805 - val_accuracy: 0.5682\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6701 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0154s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6916 - accuracy: 0.5459\n",
      "training acc:  0.520380437374115 , training loss:  0.6903819441795349 , val acc:  0.5681818127632141 , val loss:  0.6804525256156921 , test acc:  0.5459312200546265 , test loss:  0.6916140913963318\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt2/100word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much\n",
      "Epoch 1/30\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.7046 - accuracy: 0.4972WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 79ms/step - loss: 0.7041 - accuracy: 0.4986 - val_loss: 0.6835 - val_accuracy: 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/596 [..............................] - ETA: 7s - loss: 0.6634 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0124s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6867 - accuracy: 0.5877\n",
      "training acc:  0.49864131212234497 , training loss:  0.7041293978691101 , val acc:  0.5625 , val loss:  0.6835365891456604 , test acc:  0.5876677632331848 , test loss:  0.6866763234138489\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt2/150word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know me different really each = something used see good 'm well its being 1 now example been probably question who since many try had case those set were however his point am things 've still 're between say take after problem enough find number another both system new\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7201 - accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0028s vs `on_train_batch_end` time: 0.0214s). Check your callbacks.\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.6996 - accuracy: 0.4810WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 82ms/step - loss: 0.6996 - accuracy: 0.4810 - val_loss: 0.6793 - val_accuracy: 0.5625\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.6563 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0132s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6874 - accuracy: 0.5877\n",
      "training acc:  0.48097825050354004 , training loss:  0.699637770652771 , val acc:  0.5625 , val loss:  0.6793494820594788 , test acc:  0.5876677632331848 , test loss:  0.6874290704727173\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt2/200word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know me different really each = something used see good 'm well its being 1 now example been probably question who since many try had case those set were however his point am things 've still 're between say take after problem enough find number another both system new possible too actually 2 why better over answer power go did long without while through 'd code our right able back before look let value down 3 every data going either having + image high thing best off create change less given 'll own ca part sure small means give\n",
      "Epoch 1/30\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.7195 - accuracy: 0.5204 ETA: 4s - loss: 0.7283 WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 82ms/step - loss: 0.7195 - accuracy: 0.5204 - val_loss: 0.6754 - val_accuracy: 0.5852\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6962 - accuracy: 0.4669\n",
      "training acc:  0.520380437374115 , training loss:  0.7194933891296387 , val acc:  0.5852272510528564 , val loss:  0.6754209995269775 , test acc:  0.46686241030693054 , test loss:  0.696200966835022\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt2/250word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know me different really each = something used see good 'm well its being 1 now example been probably question who since many try had case those set were however his point am things 've still 're between say take after problem enough find number another both system new possible too actually 2 why better over answer power go did long without while through 'd code our right able back before look let value down 3 every data going either having + image high thing best off create change less given 'll own ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function above mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though once keep already few again person\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.8607 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0172s vs `on_train_batch_end` time: 0.0481s). Check your callbacks.\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.7352 - accuracy: 0.4905 ETA: 5s - ETA: 4s - losWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 83ms/step - loss: 0.7352 - accuracy: 0.4905 - val_loss: 0.6805 - val_accuracy: 0.5625\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6372 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0136s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6822 - accuracy: 0.5877\n",
      "training acc:  0.49048912525177 , training loss:  0.7352055311203003 , val acc:  0.5625 , val loss:  0.6804956793785095 , test acc:  0.5876677632331848 , test loss:  0.6821647882461548\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt2/300word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know me different really each = something used see good 'm well its being 1 now example been probably question who since many try had case those set were however his point am things 've still 're between say take after problem enough find number another both system new possible too actually 2 why better over answer power go did long without while through 'd code our right able back before look let value down 3 every data going either having + image high thing best off create change less given 'll own ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function above mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though once keep already few again person single water result said word come size note url$ life understand similar bytes under rather little trying makes control paper made works doing option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 1s - loss: 0.9251 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0159s). Check your callbacks.\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.6956 - accuracy: 0.5204WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 82ms/step - loss: 0.6956 - accuracy: 0.5204 - val_loss: 0.6741 - val_accuracy: 0.5625\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6542 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_test_batch_end` time: 0.0148s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6908 - accuracy: 0.5633\n",
      "training acc:  0.520380437374115 , training loss:  0.6955745816230774 , val acc:  0.5625 , val loss:  0.6740519404411316 , test acc:  0.5633389353752136 , test loss:  0.6907840371131897\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt2/350word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know me different really each = something used see good 'm well its being 1 now example been probably question who since many try had case those set were however his point am things 've still 're between say take after problem enough find number another both system new possible too actually 2 why better over answer power go did long without while through 'd code our right able back before look let value down 3 every data going either having + image high thing best off create change less given 'll own ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function above mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though once keep already few again person single water result said word come size note url$ life understand similar bytes under rather little trying makes control paper made works doing option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. him course instead never consider last kind test great target almost working > open solution itself due general user called thus camera key getting character mean method day area whether particular\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7007 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0048s vs `on_train_batch_end` time: 0.0523s). Check your callbacks.\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.6904 - accuracy: 0.5136WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 84ms/step - loss: 0.6904 - accuracy: 0.5136 - val_loss: 0.6749 - val_accuracy: 0.5625\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.6440 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_test_batch_end` time: 0.0137s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6873 - accuracy: 0.5853\n",
      "training acc:  0.5135869383811951 , training loss:  0.6904325485229492 , val acc:  0.5625 , val loss:  0.6748602390289307 , test acc:  0.5852558612823486 , test loss:  0.6872636675834656\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt2/400word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know me different really each = something used see good 'm well its being 1 now example been probably question who since many try had case those set were however his point am things 've still 're between say take after problem enough find number another both system new possible too actually 2 why better over answer power go did long without while through 'd code our right able back before look let value down 3 every data going either having + image high thing best off create change less given 'll own ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function above mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though once keep already few again person single water result said word come size note url$ life understand similar bytes under rather little trying makes control paper made works doing option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. him course instead never consider last kind test great target almost working > open solution itself due general user called thus camera key getting character mean method day area whether particular amount hard she unless range position below field top energy else c text object full avoid against difference results easy true seem available big until low fine higher input values several explanation generally provide actual uses three needs within game access class map smaller clear short cases next language common\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6397 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0222s vs `on_train_batch_end` time: 0.0566s). Check your callbacks.\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.7008 - accuracy: 0.5222WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 83ms/step - loss: 0.7004 - accuracy: 0.5204 - val_loss: 0.6781 - val_accuracy: 0.6080\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.6847 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0158s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6993 - accuracy: 0.4081\n",
      "training acc:  0.520380437374115 , training loss:  0.7004050612449646 , val acc:  0.6079545617103577 , val loss:  0.6781023740768433 , test acc:  0.40813758969306946 , test loss:  0.6992998123168945\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt2/450word_list.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know me different really each = something used see good 'm well its being 1 now example been probably question who since many try had case those set were however his point am things 've still 're between say take after problem enough find number another both system new possible too actually 2 why better over answer power go did long without while through 'd code our right able back before look let value down 3 every data going either having + image high thing best off create change less given 'll own ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function above mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though once keep already few again person single water result said word come size note url$ life understand similar bytes under rather little trying makes control paper made works doing option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. him course instead never consider last kind test great target almost working > open solution itself due general user called thus camera key getting character mean method day area whether particular amount hard she unless range position below field top energy else c text object full avoid against difference results easy true seem available big until low fine higher input values several explanation generally provide actual uses three needs within game access class map smaller clear short cases next language common easily close wo 10 4 believe multiple per away body free design magic old matter call words post useful book server left yes feel 5 along ask n ability others larger gives got weapon comes everything 100 support e.g. quality become bad write wrong history thought sort certain term standard\n",
      "Epoch 1/30\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.9668 - accuracy: 0.4819WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 82ms/step - loss: 0.9616 - accuracy: 0.4810 - val_loss: 0.7271 - val_accuracy: 0.4091\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.7286 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0150s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6933 - accuracy: 0.5133\n",
      "training acc:  0.48097825050354004 , training loss:  0.961635410785675 , val acc:  0.40909090638160706 , val loss:  0.727145254611969 , test acc:  0.5133179426193237 , test loss:  0.6932713389396667\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt2/500word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on have or not can if would but your 's an so n't from do they at will there by one which more my all some what was we when use like then has / also about just could no time any other only them get their than need how out up does should - where even make way might very same people because two these into want may   work first here he such most using much think know me different really each = something used see good 'm well its being 1 now example been probably question who since many try had case those set were however his point am things 've still 're between say take after problem enough find number another both system new possible too actually 2 why better over answer power go did long without while through 'd code our right able back before look let value down 3 every data going either having + image high thing best off create change less given 'll own ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function above mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though once keep already few again person single water result said word come size note url$ life understand similar bytes under rather little trying makes control paper made works doing option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. him course instead never consider last kind test great target almost working > open solution itself due general user called thus camera key getting character mean method day area whether particular amount hard she unless range position below field top energy else c text object full avoid against difference results easy true seem available big until low fine higher input values several explanation generally provide actual uses three needs within game access class map smaller clear short cases next language common easily close wo 10 4 believe multiple per away body free design magic old matter call words post useful book server left yes feel 5 along ask n ability others larger gives got weapon comes everything 100 support e.g. quality become bad write wrong history thought sort certain term standard source current weapons year numbers changes show although her build nothing experience hand s story main tell turn mind de site model necessary noise further original step choose money error    happen air perhaps lenses price link allow situation planet men needed problems future related whole matl \\ personal characters\n",
      "Epoch 1/30\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.6997 - accuracy: 0.5222WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 84ms/step - loss: 0.6999 - accuracy: 0.5190 - val_loss: 0.6752 - val_accuracy: 0.5909\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6666 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0139s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6936 - accuracy: 0.5160\n",
      "training acc:  0.51902174949646 , training loss:  0.6999400854110718 , val acc:  0.5909090638160706 , val loss:  0.6752121448516846 , test acc:  0.5160444378852844 , test loss:  0.6936418414115906\n",
      "\n",
      "opt3\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt3/50word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.7548 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0212s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45/100 [============>.................] - ETA: 3s - loss: 0.7012 - accuracy: 0.4986WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 77ms/step - loss: 0.6995 - accuracy: 0.5014 - val_loss: 0.6621 - val_accuracy: 0.5625\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6653 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0118s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6838 - accuracy: 0.58770s - loss: 0.6842 - ac\n",
      "training acc:  0.501358687877655 , training loss:  0.6995235681533813 , val acc:  0.5625 , val loss:  0.6620909571647644 , test acc:  0.5876677632331848 , test loss:  0.6838310956954956\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt3/100word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.6884 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_train_batch_end` time: 0.0448s). Check your callbacks.\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.6806 - accuracy: 0.5312WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 78ms/step - loss: 0.6806 - accuracy: 0.5312 - val_loss: 0.6482 - val_accuracy: 0.6136\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.6909 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0135s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6857 - accuracy: 0.4952\n",
      "training acc:  0.53125 , training loss:  0.6805957555770874 , val acc:  0.6136363744735718 , val loss:  0.6482047438621521 , test acc:  0.49517616629600525 , test loss:  0.685696542263031\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt3/150word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though keep already person single water\n",
      "Epoch 1/30\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.6753 - accuracy: 0.5380WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 79ms/step - loss: 0.6753 - accuracy: 0.5380 - val_loss: 0.6438 - val_accuracy: 0.6023\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6837 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0143s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6847 - accuracy: 0.5002\n",
      "training acc:  0.5380434989929199 , training loss:  0.6753303408622742 , val acc:  0.6022727489471436 , val loss:  0.6438419222831726 , test acc:  0.5002097487449646 , test loss:  0.684720516204834\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt3/200word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though keep already person single water result said word come size note url$ life understand similar bytes rather little trying makes control paper made works option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & #\n",
      "Epoch 1/30\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.6829 - accuracy: 0.5097WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 81ms/step - loss: 0.6817 - accuracy: 0.5163 - val_loss: 0.6510 - val_accuracy: 0.5966\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6778 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0206s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6845 - accuracy: 0.5010\n",
      "training acc:  0.5163043737411499 , training loss:  0.6817103028297424 , val acc:  0.5965909361839294 , val loss:  0.651019811630249 , test acc:  0.5010486841201782 , test loss:  0.6845104694366455\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt3/250word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though keep already person single water result said word come size note url$ life understand similar bytes rather little trying makes control paper made works option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. course instead never consider last kind test great target almost working > open solution due general user called thus camera key getting character mean method day area whether particular amount hard unless range position field\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.6736 - accuracy: 0.5542WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 80ms/step - loss: 0.6720 - accuracy: 0.5571 - val_loss: 0.6448 - val_accuracy: 0.6477\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6956 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_test_batch_end` time: 0.0144s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6865 - accuracy: 0.4950\n",
      "training acc:  0.5570651888847351 , training loss:  0.672046422958374 , val acc:  0.6477272510528564 , val loss:  0.6447562575340271 , test acc:  0.49496644735336304 , test loss:  0.6865171194076538\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt3/300word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though keep already person single water result said word come size note url$ life understand similar bytes rather little trying makes control paper made works option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. course instead never consider last kind test great target almost working > open solution due general user called thus camera key getting character mean method day area whether particular amount hard unless range position field top energy else c text object full avoid difference results easy true seem available big low fine higher input values several explanation generally provide actual uses three needs within game access class map smaller clear short cases next language common easily close wo 10 4 believe multiple per away body\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7640 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_train_batch_end` time: 0.0262s). Check your callbacks.\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.6997 - accuracy: 0.4769WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 80ms/step - loss: 0.6997 - accuracy: 0.4769 - val_loss: 0.6627 - val_accuracy: 0.5625\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.6692 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_test_batch_end` time: 0.0160s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6838 - accuracy: 0.5877\n",
      "training acc:  0.47690218687057495 , training loss:  0.6997203826904297 , val acc:  0.5625 , val loss:  0.6626665592193604 , test acc:  0.5876677632331848 , test loss:  0.6837600469589233\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt3/350word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though keep already person single water result said word come size note url$ life understand similar bytes rather little trying makes control paper made works option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. course instead never consider last kind test great target almost working > open solution due general user called thus camera key getting character mean method day area whether particular amount hard unless range position field top energy else c text object full avoid difference results easy true seem available big low fine higher input values several explanation generally provide actual uses three needs within game access class map smaller clear short cases next language common easily close wo 10 4 believe multiple per away body free design magic old matter call words post useful book server left yes feel 5 along ask n ability others larger gives got weapon comes everything 100 support e.g. quality become bad write wrong history thought sort certain term standard source current weapons year numbers changes show although build nothing\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7167 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0221s vs `on_train_batch_end` time: 0.0562s). Check your callbacks.\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.6815 - accuracy: 0.5042WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 80ms/step - loss: 0.6840 - accuracy: 0.5027 - val_loss: 0.6527 - val_accuracy: 0.5682\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.6757 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0155s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6839 - accuracy: 0.4910\n",
      "training acc:  0.5027173757553101 , training loss:  0.6840063333511353 , val acc:  0.5681818127632141 , val loss:  0.652739942073822 , test acc:  0.4909815490245819 , test loss:  0.6838887929916382\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt3/400word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though keep already person single water result said word come size note url$ life understand similar bytes rather little trying makes control paper made works option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. course instead never consider last kind test great target almost working > open solution due general user called thus camera key getting character mean method day area whether particular amount hard unless range position field top energy else c text object full avoid difference results easy true seem available big low fine higher input values several explanation generally provide actual uses three needs within game access class map smaller clear short cases next language common easily close wo 10 4 believe multiple per away body free design magic old matter call words post useful book server left yes feel 5 along ask n ability others larger gives got weapon comes everything 100 support e.g. quality become bad write wrong history thought sort certain term standard source current weapons year numbers changes show although build nothing experience hand story main tell turn mind de site model necessary noise original step choose money error    happen air perhaps lenses price link allow situation planet men needed problems future related whole matl \\ personal characters children cost anyone select earth created setting exactly past move layer examples fire\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.9773 - accuracy: 0.5938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0477s). Check your callbacks.\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.8783 - accuracy: 0.4181WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 79ms/step - loss: 0.8752 - accuracy: 0.4130 - val_loss: 0.7211 - val_accuracy: 0.3239\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6465 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0137s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6890 - accuracy: 0.5539\n",
      "training acc:  0.41304346919059753 , training loss:  0.8752343654632568 , val acc:  0.3238636255264282 , val loss:  0.7210940718650818 , test acc:  0.5539010167121887 , test loss:  0.6890277862548828\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt3/450word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though keep already person single water result said word come size note url$ life understand similar bytes rather little trying makes control paper made works option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. course instead never consider last kind test great target almost working > open solution due general user called thus camera key getting character mean method day area whether particular amount hard unless range position field top energy else c text object full avoid difference results easy true seem available big low fine higher input values several explanation generally provide actual uses three needs within game access class map smaller clear short cases next language common easily close wo 10 4 believe multiple per away body free design magic old matter call words post useful book server left yes feel 5 along ask n ability others larger gives got weapon comes everything 100 support e.g. quality become bad write wrong history thought sort certain term standard source current weapons year numbers changes show although build nothing experience hand story main tell turn mind de site model necessary noise original step choose money error    happen air perhaps lenses price link allow situation planet men needed problems future related whole matl \\ personal characters children cost anyone select earth created setting exactly past move layer examples fire company functions points normal running taking name focus effect search says surface food block seen product english return lower whatever black require otherwise sensor depends cause view remember elements subject reasons mentioned humans background sometimes difficult context flash technology terms length stop speed computer fairly meaning fuel known women gets\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.7017 - accuracy: 0.5312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0327s vs `on_train_batch_end` time: 0.0535s). Check your callbacks.\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.6805 - accuracy: 0.5222WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 80ms/step - loss: 0.6841 - accuracy: 0.5177 - val_loss: 0.6464 - val_accuracy: 0.5795\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6746 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0146s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6831 - accuracy: 0.5059\n",
      "training acc:  0.5176630616188049 , training loss:  0.6841213703155518 , val acc:  0.5795454382896423 , val loss:  0.6463517546653748 , test acc:  0.5058724880218506 , test loss:  0.6830539703369141\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt3/500word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though keep already person single water result said word come size note url$ life understand similar bytes rather little trying makes control paper made works option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. course instead never consider last kind test great target almost working > open solution due general user called thus camera key getting character mean method day area whether particular amount hard unless range position field top energy else c text object full avoid difference results easy true seem available big low fine higher input values several explanation generally provide actual uses three needs within game access class map smaller clear short cases next language common easily close wo 10 4 believe multiple per away body free design magic old matter call words post useful book server left yes feel 5 along ask n ability others larger gives got weapon comes everything 100 support e.g. quality become bad write wrong history thought sort certain term standard source current weapons year numbers changes show although build nothing experience hand story main tell turn mind de site model necessary noise original step choose money error    happen air perhaps lenses price link allow situation planet men needed problems future related whole matl \\ personal characters children cost anyone select earth created setting exactly past move layer examples fire company functions points normal running taking name focus effect search says surface food block seen product english return lower whatever black require otherwise sensor depends cause view remember elements subject reasons mentioned humans background sometimes difficult context flash technology terms length stop speed computer fairly meaning fuel known women gets longer ground outside sequence knowledge distance society questions research sound produce issues areas software car exist guess approach 6 que modern takes hit looks mostly total yet later exact expected front direct rules ' zero reach structure systems update extra becomes degree goes basically factor theory assuming play action base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7054 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0262s vs `on_train_batch_end` time: 0.0483s). Check your callbacks.\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.6707 - accuracy: 0.5778WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 80ms/step - loss: 0.6712 - accuracy: 0.5761 - val_loss: 0.6366 - val_accuracy: 0.6534\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6875 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0138s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6871 - accuracy: 0.5057\n",
      "training acc:  0.5760869383811951 , training loss:  0.6712009310722351 , val acc:  0.6534090638160706 , val loss:  0.6366087794303894 , test acc:  0.505662739276886 , test loss:  0.6871038675308228\n",
      "\n",
      "opt4\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt4/50word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 1.3921 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_end` time: 0.0488s). Check your callbacks.\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.8686 - accuracy: 0.5204WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 80ms/step - loss: 0.8686 - accuracy: 0.5204 - val_loss: 0.6652 - val_accuracy: 0.5966\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6588 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_test_batch_end` time: 0.0173s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6931 - accuracy: 0.5134\n",
      "training acc:  0.520380437374115 , training loss:  0.868558406829834 , val acc:  0.5965909361839294 , val loss:  0.6652345061302185 , test acc:  0.5134228467941284 , test loss:  0.6931074261665344\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt4/100word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part\n",
      "Epoch 1/30\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.6992 - accuracy: 0.5204WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 81ms/step - loss: 0.6992 - accuracy: 0.5204 - val_loss: 0.6762 - val_accuracy: 0.5966\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6789 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_test_batch_end` time: 0.0115s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6932 - accuracy: 0.5012\n",
      "training acc:  0.520380437374115 , training loss:  0.699202835559845 , val acc:  0.5965909361839294 , val loss:  0.6762329936027527 , test acc:  0.5011535286903381 , test loss:  0.693221390247345\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt4/150word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though keep already person\n",
      "Epoch 1/30\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.6958 - accuracy: 0.5476 ETA: 3s - loss: 0.7004 - accuracy: 0.WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 81ms/step - loss: 0.6958 - accuracy: 0.5476 - val_loss: 0.6713 - val_accuracy: 0.5625\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6174 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0160s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6853 - accuracy: 0.5877\n",
      "training acc:  0.5475543737411499 , training loss:  0.6957578063011169 , val acc:  0.5625 , val loss:  0.671273410320282 , test acc:  0.5876677632331848 , test loss:  0.6853180527687073\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt4/200word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though keep already person single water result said word come size note url$ life understand similar bytes rather little trying makes control paper made works option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 1.0683 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0263s vs `on_train_batch_end` time: 0.0517s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45/100 [============>.................] - ETA: 3s - loss: 0.9768 - accuracy: 0.4750WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 82ms/step - loss: 0.9709 - accuracy: 0.4783 - val_loss: 0.7230 - val_accuracy: 0.3750\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6868 - accuracy: 0.5576\n",
      "training acc:  0.47826087474823 , training loss:  0.9708531498908997 , val acc:  0.375 , val loss:  0.7229830622673035 , test acc:  0.557571291923523 , test loss:  0.686782956123352\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt4/250word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though keep already person single water result said word come size note url$ life understand similar bytes rather little trying makes control paper made works option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. course instead never consider last kind test great target almost working > open solution due general user called thus camera key getting character mean method day area whether particular amount hard unless range\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 1.0954 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0324s). Check your callbacks.\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 0.8561 - accuracy: 0.5204WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 81ms/step - loss: 0.8561 - accuracy: 0.5204 - val_loss: 0.6543 - val_accuracy: 0.5966\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6446 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0138s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6959 - accuracy: 0.5071\n",
      "training acc:  0.520380437374115 , training loss:  0.8561221361160278 , val acc:  0.5965909361839294 , val loss:  0.6543352007865906 , test acc:  0.5071308612823486 , test loss:  0.6958903670310974\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt4/300word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though keep already person single water result said word come size note url$ life understand similar bytes rather little trying makes control paper made works option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. course instead never consider last kind test great target almost working > open solution due general user called thus camera key getting character mean method day area whether particular amount hard unless range position field top energy else c text object full avoid difference results easy true seem available big low fine higher input values several explanation generally provide actual uses three needs within game access class map smaller clear short cases next language common easily close wo 10 4 believe multiple per\n",
      "Epoch 1/30\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.7069 - accuracy: 0.4972 ETA: 4s -WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 80ms/step - loss: 0.7057 - accuracy: 0.5000 - val_loss: 0.6723 - val_accuracy: 0.5625\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6375 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0160s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6847 - accuracy: 0.5877\n",
      "training acc:  0.5 , training loss:  0.7057454586029053 , val acc:  0.5625 , val loss:  0.6722626090049744 , test acc:  0.5876677632331848 , test loss:  0.6846529841423035\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt4/350word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though keep already person single water result said word come size note url$ life understand similar bytes rather little trying makes control paper made works option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. course instead never consider last kind test great target almost working > open solution due general user called thus camera key getting character mean method day area whether particular amount hard unless range position field top energy else c text object full avoid difference results easy true seem available big low fine higher input values several explanation generally provide actual uses three needs within game access class map smaller clear short cases next language common easily close wo 10 4 believe multiple per away body free design magic old matter call words post useful book server left yes feel 5 along ask n ability others larger gives got weapon comes everything 100 support e.g. quality become bad write wrong history thought sort certain term standard source current weapons year numbers changes show although\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.8973 - accuracy: 0.4681 ETA: 4s - loss: 1.0860 - accu - ETA: 3s - loss: 0.9102 - accuracy: WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 82ms/step - loss: 0.8932 - accuracy: 0.4592 - val_loss: 0.7041 - val_accuracy: 0.4205\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6696 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0165s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6821 - accuracy: 0.5785\n",
      "training acc:  0.45923912525177 , training loss:  0.893221378326416 , val acc:  0.4204545319080353 , val loss:  0.7040589451789856 , test acc:  0.5785444378852844 , test loss:  0.6821188926696777\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt4/400word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though keep already person single water result said word come size note url$ life understand similar bytes rather little trying makes control paper made works option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. course instead never consider last kind test great target almost working > open solution due general user called thus camera key getting character mean method day area whether particular amount hard unless range position field top energy else c text object full avoid difference results easy true seem available big low fine higher input values several explanation generally provide actual uses three needs within game access class map smaller clear short cases next language common easily close wo 10 4 believe multiple per away body free design magic old matter call words post useful book server left yes feel 5 along ask n ability others larger gives got weapon comes everything 100 support e.g. quality become bad write wrong history thought sort certain term standard source current weapons year numbers changes show although build nothing experience hand story main tell turn mind de site model necessary noise original step choose money error    happen air perhaps lenses price link allow situation planet men needed problems future related whole matl \\ personal characters children cost anyone select earth created setting exactly past move layer\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 1.1121 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0533s). Check your callbacks.\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.7407 - accuracy: 0.5250WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 81ms/step - loss: 0.7391 - accuracy: 0.5258 - val_loss: 0.6656 - val_accuracy: 0.5966\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6757 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_test_batch_end` time: 0.0147s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6962 - accuracy: 0.4766\n",
      "training acc:  0.5258151888847351 , training loss:  0.7391389012336731 , val acc:  0.5965909361839294 , val loss:  0.6655640006065369 , test acc:  0.47661492228507996 , test loss:  0.6961756944656372\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt4/450word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though keep already person single water result said word come size note url$ life understand similar bytes rather little trying makes control paper made works option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. course instead never consider last kind test great target almost working > open solution due general user called thus camera key getting character mean method day area whether particular amount hard unless range position field top energy else c text object full avoid difference results easy true seem available big low fine higher input values several explanation generally provide actual uses three needs within game access class map smaller clear short cases next language common easily close wo 10 4 believe multiple per away body free design magic old matter call words post useful book server left yes feel 5 along ask n ability others larger gives got weapon comes everything 100 support e.g. quality become bad write wrong history thought sort certain term standard source current weapons year numbers changes show although build nothing experience hand story main tell turn mind de site model necessary noise original step choose money error    happen air perhaps lenses price link allow situation planet men needed problems future related whole matl \\ personal characters children cost anyone select earth created setting exactly past move layer examples fire company functions points normal running taking name focus effect search says surface food block seen product english return lower whatever black require otherwise sensor depends cause view remember elements subject reasons mentioned humans background sometimes difficult context flash technology terms length stop speed computer fairly meaning fuel known\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 2.3815 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0321s). Check your callbacks.\n",
      " 46/100 [============>.................] - ETA: 3s - loss: 1.3690 - accuracy: 0.4796WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46/100 [============>.................] - 4s 80ms/step - loss: 1.3690 - accuracy: 0.4796 - val_loss: 0.9107 - val_accuracy: 0.4091\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.8358 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0134s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7486 - accuracy: 0.4688\n",
      "training acc:  0.479619562625885 , training loss:  1.369023323059082 , val acc:  0.40909090638160706 , val loss:  0.9106927514076233 , test acc:  0.46875 , test loss:  0.7486161589622498\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1600/opt4/500word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even make way might people two want may   work first using much think know different really = something used see good 'm well 1 example probably question since many try case set however point things 've still 're say take problem enough find number another system new possible actually 2 better answer power go long without 'd code right able back look let value 3 every data going either + image high thing best create change less given 'll ca part sure small means give around \\mathcal must likely second large form always lot space process reason lens page least < function mm read non end start add maybe help real bit based often world etc level light seems following simply anything x specific years order usually though keep already person single water result said word come size note url$ life understand similar bytes rather little trying makes control paper made works option human 0 simple output    type looking times check us state put god found edit idea place group online list quite % color fact run done someone information & # important version pretty assume issue correct \\to sense line making side b file far i.e. course instead never consider last kind test great target almost working > open solution due general user called thus camera key getting character mean method day area whether particular amount hard unless range position field top energy else c text object full avoid difference results easy true seem available big low fine higher input values several explanation generally provide actual uses three needs within game access class map smaller clear short cases next language common easily close wo 10 4 believe multiple per away body free design magic old matter call words post useful book server left yes feel 5 along ask n ability others larger gives got weapon comes everything 100 support e.g. quality become bad write wrong history thought sort certain term standard source current weapons year numbers changes show although build nothing experience hand story main tell turn mind de site model necessary noise original step choose money error    happen air perhaps lenses price link allow situation planet men needed problems future related whole matl \\ personal characters children cost anyone select earth created setting exactly past move layer examples fire company functions points normal running taking name focus effect search says surface food block seen product english return lower whatever black require otherwise sensor depends cause view remember elements subject reasons mentioned humans background sometimes difficult context flash technology terms length stop speed computer fairly meaning fuel known women gets longer ground outside sequence knowledge distance society questions research sound produce issues areas software car exist guess approach 6 que modern takes hit looks mostly total yet later exact expected front direct rules ' zero reach structure systems update extra becomes degree goes basically factor theory assuming play\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 1s - loss: 0.7665 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0115s). Check your callbacks.\n",
      " 45/100 [============>.................] - ETA: 3s - loss: 0.6992 - accuracy: 0.4931 ETA: 5s - loss: 0.7062 - WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 46/100 [============>.................] - 4s 80ms/step - loss: 0.6991 - accuracy: 0.4918 - val_loss: 0.6787 - val_accuracy: 0.5625\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6705 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_test_batch_end` time: 0.0135s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6892 - accuracy: 0.5690\n",
      "training acc:  0.49184781312942505 , training loss:  0.6991327404975891 , val acc:  0.5625 , val loss:  0.6786845922470093 , test acc:  0.5690016746520996 , test loss:  0.689155638217926\n",
      "\n",
      "Started evaluation for: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1800.csv\n",
      "Loading training file from path:  /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_1800.csv\n",
      "(726, 6)\n",
      "Loading testing file from path:  /home/sukanya/PhD/Datasets/PAN SCD/pan19-style-change-detection/processed/test.csv\n",
      "(9538, 5)\n",
      "opt1\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt1/50word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when\n",
      "Epoch 1/30\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.6751 - accuracy: 0.5712 EWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 86ms/step - loss: 0.6751 - accuracy: 0.5712 - val_loss: 0.6667 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6407 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0143s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6899 - accuracy: 0.5666\n",
      "training acc:  0.5711805820465088 , training loss:  0.6751355528831482 , val acc:  0.5902777910232544 , val loss:  0.6666842103004456 , test acc:  0.5665897727012634 , test loss:  0.6898723840713501\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt1/100word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 1.4253 - accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.0263s). Check your callbacks.\n",
      " 35/100 [=========>....................] - ETA: 4s - loss: 0.9113 - accuracy: 0.5679WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 87ms/step - loss: 0.9036 - accuracy: 0.5712 - val_loss: 0.6785 - val_accuracy: 0.5903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/596 [..............................] - ETA: 7s - loss: 0.5646 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0166s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.7116 - accuracy: 0.5743\n",
      "training acc:  0.5711805820465088 , training loss:  0.9036102294921875 , val acc:  0.5902777910232544 , val loss:  0.6785147786140442 , test acc:  0.5743498206138611 , test loss:  0.7116012573242188\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt1/150word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me   different know each see really something good used 1 'm its example well being question probably been case since now many his set had try 've point who however things those were still take between 're 2 am say number after another system enough problem actually over why both\n",
      "Epoch 1/30\n",
      " 35/100 [=========>....................] - ETA: 4s - loss: 0.6791 - accuracy: 0.5679 ETA: 5s - loss: 0.677WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 87ms/step - loss: 0.6774 - accuracy: 0.5712 - val_loss: 0.6623 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.6116 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0162s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6890 - accuracy: 0.5834\n",
      "training acc:  0.5711805820465088 , training loss:  0.6773806214332581 , val acc:  0.5902777910232544 , val loss:  0.662341833114624 , test acc:  0.5833683013916016 , test loss:  0.689023494720459\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt1/200word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me   different know each see really something good used 1 'm its example well being question probably been case since now many his set had try 've point who however things those were still take between 're 2 am say number after another system enough problem actually over why both better find new possible too without answer while did power through our long go code 'd look down + let back data able right 3 value going high before either best having ca sure own given form less image thing 'll & change part space means every always give lens\n",
      "Epoch 1/30\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.7243 - accuracy: 0.5087WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 89ms/step - loss: 0.7243 - accuracy: 0.5087 - val_loss: 0.6667 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.5982 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0063s vs `on_test_batch_end` time: 0.0163s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6876 - accuracy: 0.5877\n",
      "training acc:  0.5086805820465088 , training loss:  0.7243421077728271 , val acc:  0.5902777910232544 , val loss:  0.6666829586029053 , test acc:  0.5876677632331848 , test loss:  0.6875579357147217\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt1/250word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me   different know each see really something good used 1 'm its example well being question probably been case since now many his set had try 've point who however things those were still take between 're 2 am say number after another system enough problem actually over why both better find new possible too without answer while did power through our long go code 'd look down + let back data able right 3 value going high before either best having ca sure own given form less image thing 'll & change part space means every always give lens around small off page \\mathcal process create function second large must x reason non add end above likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.8699 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0205s vs `on_train_batch_end` time: 0.0451s). Check your callbacks.\n",
      " 35/100 [=========>....................] - ETA: 4s - loss: 0.7912 - accuracy: 0.4661WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 88ms/step - loss: 0.7887 - accuracy: 0.4670 - val_loss: 0.6736 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.6318 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0114s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6810 - accuracy: 0.5877\n",
      "training acc:  0.4670138955116272 , training loss:  0.7886784076690674 , val acc:  0.5902777910232544 , val loss:  0.67355877161026 , test acc:  0.5876677632331848 , test loss:  0.6809670329093933\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt1/300word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me   different know each see really something good used 1 'm its example well being question probably been case since now many his set had try 've point who however things those were still take between 're 2 am say number after another system enough problem actually over why both better find new possible too without answer while did power through our long go code 'd look down + let back data able right 3 value going high before either best having ca sure own given form less image thing 'll & change part space means every always give lens around small off page \\mathcal process create function second large must x reason non add end above likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size again little us 0 following few years place output maybe url$ result world quite someone information option trying human check once doing makes \\to him times done life line keep looking made far found control works online open simple color list run idea\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.7009 - accuracy: 0.5712WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 88ms/step - loss: 0.7009 - accuracy: 0.5712 - val_loss: 0.6572 - val_accuracy: 0.5972\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6347 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0129s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6950 - accuracy: 0.5338\n",
      "training acc:  0.5711805820465088 , training loss:  0.7008896470069885 , val acc:  0.5972222089767456 , val loss:  0.6572324633598328 , test acc:  0.5337668061256409 , test loss:  0.6949602961540222\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt1/350word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me   different know each see really something good used 1 'm its example well being question probably been case since now many his set had try 've point who however things those were still take between 're 2 am say number after another system enough problem actually over why both better find new possible too without answer while did power through our long go code 'd look down + let back data able right 3 value going high before either best having ca sure own given form less image thing 'll & change part space means every always give lens around small off page \\mathcal process create function second large must x reason non add end above likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size again little us 0 following few years place output maybe url$ result world quite someone information option trying human check once doing makes \\to him times done life line keep looking made far found control works online open simple color list run idea under pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working she hard test never making user text kind particular itself side field last camera range great getting whether generally thus full mean energy below position\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 1.0202 - accuracy: 0.3438WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0089s vs `on_train_batch_end` time: 0.0363s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.7432 - accuracy: 0.4931 ETA: 4s - loss: 0.7630 - acWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 89ms/step - loss: 0.7432 - accuracy: 0.4931 - val_loss: 0.6667 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 0s - loss: 0.6019 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_test_batch_end` time: 0.0132s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6859 - accuracy: 0.5877\n",
      "training acc:  0.4930555522441864 , training loss:  0.7432210445404053 , val acc:  0.5902777910232544 , val loss:  0.6667285561561584 , test acc:  0.5876677632331848 , test loss:  0.6858610510826111\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt1/400word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me   different know each see really something good used 1 'm its example well being question probably been case since now many his set had try 've point who however things those were still take between 're 2 am say number after another system enough problem actually over why both better find new possible too without answer while did power through our long go code 'd look down + let back data able right 3 value going high before either best having ca sure own given form less image thing 'll & change part space means every always give lens around small off page \\mathcal process create function second large must x reason non add end above likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size again little us 0 following few years place output maybe url$ result world quite someone information option trying human check once doing makes \\to him times done life line keep looking made far found control works online open simple color list run idea under pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working she hard test never making user text kind particular itself side field last camera range great getting whether generally thus full mean energy below position several due input called unless > day difference big results clear against true method body words else object matter provide free c map fine target until amount multiple believe higher area n left server 4 uses class available cases top design per access avoid 5 common values gives three easily\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7968 - accuracy: 0.1562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0322s vs `on_train_batch_end` time: 0.0521s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.6884 - accuracy: 0.5226 ETA: 5s - loss: 0.713 - ETA: 5s - loss: 0WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 90ms/step - loss: 0.6884 - accuracy: 0.5226 - val_loss: 0.6630 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6160 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0114s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6870 - accuracy: 0.5877\n",
      "training acc:  0.5225694179534912 , training loss:  0.6884157061576843 , val acc:  0.5902777910232544 , val loss:  0.6629970073699951 , test acc:  0.5876677632331848 , test loss:  0.6869508624076843\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt1/450word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me   different know each see really something good used 1 'm its example well being question probably been case since now many his set had try 've point who however things those were still take between 're 2 am say number after another system enough problem actually over why both better find new possible too without answer while did power through our long go code 'd look down + let back data able right 3 value going high before either best having ca sure own given form less image thing 'll & change part space means every always give lens around small off page \\mathcal process create function second large must x reason non add end above likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size again little us 0 following few years place output maybe url$ result world quite someone information option trying human check once doing makes \\to him times done life line keep looking made far found control works online open simple color list run idea under pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working she hard test never making user text kind particular itself side field last camera range great getting whether generally thus full mean energy below position several due input called unless > day difference big results clear against true method body words else object matter provide free c map fine target until amount multiple believe higher area n left server 4 uses class available cases top design per access avoid 5 common values gives three easily explanation feel post e.g. ask easy old character \\ short 10 low larger close smaller actual seem yes standard become write within de next 100 support language main current away wo price link needs quality comes others her earth hand along useful step related company call cost s everything product\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7084 - accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0210s vs `on_train_batch_end` time: 0.0334s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.6790 - accuracy: 0.5573WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 89ms/step - loss: 0.6790 - accuracy: 0.5573 - val_loss: 0.6615 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6044 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0141s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6891 - accuracy: 0.5877\n",
      "training acc:  0.5572916865348816 , training loss:  0.6789953112602234 , val acc:  0.5902777910232544 , val loss:  0.6615264415740967 , test acc:  0.5876677632331848 , test loss:  0.6890838146209717\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt1/500word_list.txt\n",
      "WORD INDEX STR: the to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me   different know each see really something good used 1 'm its example well being question probably been case since now many his set had try 've point who however things those were still take between 're 2 am say number after another system enough problem actually over why both better find new possible too without answer while did power through our long go code 'd look down + let back data able right 3 value going high before either best having ca sure own given form less image thing 'll & change part space means every always give lens around small off page \\mathcal process create function second large must x reason non add end above likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size again little us 0 following few years place output maybe url$ result world quite someone information option trying human check once doing makes \\to him times done life line keep looking made far found control works online open simple color list run idea under pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working she hard test never making user text kind particular itself side field last camera range great getting whether generally thus full mean energy below position several due input called unless > day difference big results clear against true method body words else object matter provide free c map fine target until amount multiple believe higher area n left server 4 uses class available cases top design per access avoid 5 common values gives three easily explanation feel post e.g. ask easy old character \\ short 10 low larger close smaller actual seem yes standard become write within de next 100 support language main current away wo price link needs quality comes others her earth hand along useful step related company call cost s everything product wrong original show past bad nothing lenses term    changes although sort book english points necessary build needed got weapons thought perhaps choose taking numbers error experience meaning whole tell men ability characters source surface job certain game noise exactly site history created \\\\ model story view anyone examples d\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.9583 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_train_batch_end` time: 0.0241s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.7006 - accuracy: 0.5712 ETA: 5s - loss:WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 89ms/step - loss: 0.7006 - accuracy: 0.5712 - val_loss: 0.6540 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6124 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0132s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6949 - accuracy: 0.5633\n",
      "training acc:  0.5711805820465088 , training loss:  0.7005922794342041 , val acc:  0.5902777910232544 , val loss:  0.6540181040763855 , test acc:  0.5633389353752136 , test loss:  0.694922924041748\n",
      "\n",
      "opt2\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt2/50word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was\n",
      "Epoch 1/30\n",
      " 35/100 [=========>....................] - ETA: 4s - loss: 0.7265 - accuracy: 0.5696 EWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 86ms/step - loss: 0.7248 - accuracy: 0.5712 - val_loss: 0.6669 - val_accuracy: 0.6111\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6586 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0069s vs `on_test_batch_end` time: 0.0146s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6942 - accuracy: 0.5126\n",
      "training acc:  0.5711805820465088 , training loss:  0.724799394607544 , val acc:  0.6111111044883728 , val loss:  0.6668733954429626 , test acc:  0.5125839114189148 , test loss:  0.6942297220230103\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt2/100word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most\n",
      "Epoch 1/30\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.7093 - accuracy: 0.5052 ETA: 5s - loss: 0.7436 - accuracy:  - ETA: 5s -WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 85ms/step - loss: 0.7093 - accuracy: 0.5052 - val_loss: 0.6674 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 0s - loss: 0.6103 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0136s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6838 - accuracy: 0.5877\n",
      "training acc:  0.5052083134651184 , training loss:  0.7093149423599243 , val acc:  0.5902777910232544 , val loss:  0.6673569679260254 , test acc:  0.5876677632331848 , test loss:  0.6838344931602478\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt2/150word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me   different know each see really something good used 1 'm its example well being question probably been case since now many his set had try 've point who however things those were still take between 're 2 am say number after another system enough problem actually over\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 4s - loss: 0.7865 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0258s vs `on_train_batch_end` time: 0.0429s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.8005 - accuracy: 0.5712WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 86ms/step - loss: 0.8005 - accuracy: 0.5712 - val_loss: 0.6571 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.5970 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0141s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6955 - accuracy: 0.5707\n",
      "training acc:  0.5711805820465088 , training loss:  0.8004777431488037 , val acc:  0.5902777910232544 , val loss:  0.6571465134620667 , test acc:  0.5706795454025269 , test loss:  0.695515513420105\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt2/200word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me   different know each see really something good used 1 'm its example well being question probably been case since now many his set had try 've point who however things those were still take between 're 2 am say number after another system enough problem actually over why both better find new possible too without answer while did power through our long go code 'd look down + let back data able right 3 value going high before either best having ca sure own given form less image thing 'll & change part space means every always\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 4s - loss: 0.7950 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0318s vs `on_train_batch_end` time: 0.0523s). Check your callbacks.\n",
      " 35/100 [=========>....................] - ETA: 4s - loss: 0.7020 - accuracy: 0.5125WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 85ms/step - loss: 0.7021 - accuracy: 0.5122 - val_loss: 0.6653 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6129 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0134s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6847 - accuracy: 0.5877\n",
      "training acc:  0.5121527910232544 , training loss:  0.7021390795707703 , val acc:  0.5902777910232544 , val loss:  0.6653305292129517 , test acc:  0.5876677632331848 , test loss:  0.6846692562103271\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt2/250word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me   different know each see really something good used 1 'm its example well being question probably been case since now many his set had try 've point who however things those were still take between 're 2 am say number after another system enough problem actually over why both better find new possible too without answer while did power through our long go code 'd look down + let back data able right 3 value going high before either best having ca sure own given form less image thing 'll & change part space means every always give lens around small off page \\mathcal process create function second large must x reason non add end above likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type\n",
      "Epoch 1/30\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 1.1619 - accuracy: 0.5712WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 88ms/step - loss: 1.1619 - accuracy: 0.5712 - val_loss: 0.8472 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.5300 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0129s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.8220 - accuracy: 0.5838\n",
      "training acc:  0.5711805820465088 , training loss:  1.1618765592575073 , val acc:  0.5902777910232544 , val loss:  0.8472464084625244 , test acc:  0.583787739276886 , test loss:  0.8220260739326477\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt2/300word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me   different know each see really something good used 1 'm its example well being question probably been case since now many his set had try 've point who however things those were still take between 're 2 am say number after another system enough problem actually over why both better find new possible too without answer while did power through our long go code 'd look down + let back data able right 3 value going high before either best having ca sure own given form less image thing 'll & change part space means every always give lens around small off page \\mathcal process create function second large must x reason non add end above likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size again little us 0 following few years place output maybe url$ result world quite someone information option trying human check once doing makes \\to him times done life line keep looking made far found control works online open simple color list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 4s - loss: 1.4122 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0344s vs `on_train_batch_end` time: 0.0551s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.9818 - accuracy: 0.4288 ETA: 5s - loss: 1.0WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 89ms/step - loss: 0.9818 - accuracy: 0.4288 - val_loss: 0.7443 - val_accuracy: 0.3750\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.7413 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_test_batch_end` time: 0.0141s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6959 - accuracy: 0.4930\n",
      "training acc:  0.4288194477558136 , training loss:  0.9818105697631836 , val acc:  0.375 , val loss:  0.744304895401001 , test acc:  0.4929739832878113 , test loss:  0.6958773136138916\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt2/350word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me   different know each see really something good used 1 'm its example well being question probably been case since now many his set had try 've point who however things those were still take between 're 2 am say number after another system enough problem actually over why both better find new possible too without answer while did power through our long go code 'd look down + let back data able right 3 value going high before either best having ca sure own given form less image thing 'll & change part space means every always give lens around small off page \\mathcal process create function second large must x reason non add end above likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size again little us 0 following few years place output maybe url$ result world quite someone information option trying human check once doing makes \\to him times done life line keep looking made far found control works online open simple color list run idea under pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working she hard test never making user text kind particular itself side field last camera range great getting whether generally thus full mean energy\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6601 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0415s vs `on_train_batch_end` time: 0.0629s). Check your callbacks.\n",
      " 35/100 [=========>....................] - ETA: 4s - loss: 0.6955 - accuracy: 0.5786WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 88ms/step - loss: 0.6971 - accuracy: 0.5712 - val_loss: 0.6572 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.6349 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0167s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6927 - accuracy: 0.5564\n",
      "training acc:  0.5711805820465088 , training loss:  0.6970527172088623 , val acc:  0.5902777910232544 , val loss:  0.6571934819221497 , test acc:  0.5564177632331848 , test loss:  0.6927276849746704\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt2/400word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me   different know each see really something good used 1 'm its example well being question probably been case since now many his set had try 've point who however things those were still take between 're 2 am say number after another system enough problem actually over why both better find new possible too without answer while did power through our long go code 'd look down + let back data able right 3 value going high before either best having ca sure own given form less image thing 'll & change part space means every always give lens around small off page \\mathcal process create function second large must x reason non add end above likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size again little us 0 following few years place output maybe url$ result world quite someone information option trying human check once doing makes \\to him times done life line keep looking made far found control works online open simple color list run idea under pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working she hard test never making user text kind particular itself side field last camera range great getting whether generally thus full mean energy below position several due input called unless > day difference big results clear against true method body words else object matter provide free c map fine target until amount multiple believe higher area n left server 4 uses class available cases top design per access avoid 5 common values gives\n",
      "Epoch 1/30\n",
      " 35/100 [=========>....................] - ETA: 4s - loss: 0.7915 - accuracy: 0.4786 ETA: 4s - loss: 0.7985 - accuracy: 0.WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 89ms/step - loss: 0.7882 - accuracy: 0.4826 - val_loss: 0.6775 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6414 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0130s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6803 - accuracy: 0.5877\n",
      "training acc:  0.4826388955116272 , training loss:  0.7881984710693359 , val acc:  0.5902777910232544 , val loss:  0.677489697933197 , test acc:  0.5876677632331848 , test loss:  0.680340588092804\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt2/450word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me   different know each see really something good used 1 'm its example well being question probably been case since now many his set had try 've point who however things those were still take between 're 2 am say number after another system enough problem actually over why both better find new possible too without answer while did power through our long go code 'd look down + let back data able right 3 value going high before either best having ca sure own given form less image thing 'll & change part space means every always give lens around small off page \\mathcal process create function second large must x reason non add end above likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size again little us 0 following few years place output maybe url$ result world quite someone information option trying human check once doing makes \\to him times done life line keep looking made far found control works online open simple color list run idea under pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working she hard test never making user text kind particular itself side field last camera range great getting whether generally thus full mean energy below position several due input called unless > day difference big results clear against true method body words else object matter provide free c map fine target until amount multiple believe higher area n left server 4 uses class available cases top design per access avoid 5 common values gives three easily explanation feel post e.g. ask easy old character \\ short 10 low larger close smaller actual seem yes standard become write within de next 100 support language main current away wo price link needs quality comes others her earth hand along useful step related company call cost s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 4s - loss: 0.6529 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0220s vs `on_train_batch_end` time: 0.0363s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.6751 - accuracy: 0.5712WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 89ms/step - loss: 0.6751 - accuracy: 0.5712 - val_loss: 0.6599 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6220 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0140s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6885 - accuracy: 0.5843\n",
      "training acc:  0.5711805820465088 , training loss:  0.6751031875610352 , val acc:  0.5902777910232544 , val loss:  0.6598649024963379 , test acc:  0.5843120813369751 , test loss:  0.6885349154472351\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt2/500word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is in that you it i $ for be this with as   are on or have not can if would but your 's an n't so from do at they will by there my one which all what more some was we when use like then has / also no about any could time other just only them get need their than how does out up should - even where might make way very same into because people these two want work he such first may here = using think most much me   different know each see really something good used 1 'm its example well being question probably been case since now many his set had try 've point who however things those were still take between 're 2 am say number after another system enough problem actually over why both better find new possible too without answer while did power through our long go code 'd look down + let back data able right 3 value going high before either best having ca sure own given form less image thing 'll & change part space means every always give lens around small off page \\mathcal process create function second large must x reason non add end above likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size again little us 0 following few years place output maybe url$ result world quite someone information option trying human check once doing makes \\to him times done life line keep looking made far found control works online open simple color list run idea under pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working she hard test never making user text kind particular itself side field last camera range great getting whether generally thus full mean energy below position several due input called unless > day difference big results clear against true method body words else object matter provide free c map fine target until amount multiple believe higher area n left server 4 uses class available cases top design per access avoid 5 common values gives three easily explanation feel post e.g. ask easy old character \\ short 10 low larger close smaller actual seem yes standard become write within de next 100 support language main current away wo price link needs quality comes others her earth hand along useful step related company call cost s everything product wrong original show past bad nothing lenses term    changes although sort book english points necessary build needed got weapons thought perhaps choose taking numbers error experience meaning whole tell men ability characters source surface job certain game noise exactly site history created \\\\ model story view anyone\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6482 - accuracy: 0.6562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0330s vs `on_train_batch_end` time: 0.0675s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.6757 - accuracy: 0.5712WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 90ms/step - loss: 0.6757 - accuracy: 0.5712 - val_loss: 0.6662 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 9s - loss: 0.6493 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0145s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6897 - accuracy: 0.5748\n",
      "training acc:  0.5711805820465088 , training loss:  0.675667941570282 , val acc:  0.5902777910232544 , val loss:  0.6661803722381592 , test acc:  0.5747693181037903 , test loss:  0.6897153854370117\n",
      "\n",
      "opt3\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt3/50word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7009 - accuracy: 0.5938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0169s vs `on_train_batch_end` time: 0.0364s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.7042 - accuracy: 0.5330WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 84ms/step - loss: 0.7042 - accuracy: 0.5330 - val_loss: 0.6450 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6317 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0125s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6813 - accuracy: 0.5877\n",
      "training acc:  0.5329861044883728 , training loss:  0.7041707038879395 , val acc:  0.5902777910232544 , val loss:  0.6450208425521851 , test acc:  0.5876677632331848 , test loss:  0.6812578439712524\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt3/100word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.5152 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0165s vs `on_train_batch_end` time: 0.0266s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.7339 - accuracy: 0.5851WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36/100 [=========>....................] - 3s 84ms/step - loss: 0.7339 - accuracy: 0.5851 - val_loss: 0.6077 - val_accuracy: 0.6458\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6896 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0142s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6883 - accuracy: 0.4963\n",
      "training acc:  0.5850694179534912 , training loss:  0.7338844537734985 , val acc:  0.6458333134651184 , val loss:  0.6076834797859192 , test acc:  0.4963296949863434 , test loss:  0.6883012652397156\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt3/150word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space means every always give lens around small page \\mathcal process create function second large must x reason non add end likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7496 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0170s vs `on_train_batch_end` time: 0.0268s). Check your callbacks.\n",
      " 35/100 [=========>....................] - ETA: 4s - loss: 0.7024 - accuracy: 0.4982 ETA:  - ETA: 4s - loss: 0.7081 - accuraWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 86ms/step - loss: 0.7007 - accuracy: 0.5017 - val_loss: 0.6534 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.6512 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0148s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6812 - accuracy: 0.5877\n",
      "training acc:  0.5017361044883728 , training loss:  0.7007314562797546 , val acc:  0.5902777910232544 , val loss:  0.653358519077301 , test acc:  0.5876677632331848 , test loss:  0.6811965107917786\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt3/200word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space means every always give lens around small page \\mathcal process create function second large must x reason non add end likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size little us 0 following years place output maybe url$ result world quite someone information option trying human check makes \\to times done life line keep looking made far found control works online open simple color list run idea pretty #\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6964 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0079s vs `on_train_batch_end` time: 0.0522s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.6675 - accuracy: 0.5608WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 87ms/step - loss: 0.6675 - accuracy: 0.5608 - val_loss: 0.6317 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6588 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0190s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6818 - accuracy: 0.4788\n",
      "training acc:  0.5607638955116272 , training loss:  0.6674929261207581 , val acc:  0.5902777910232544 , val loss:  0.6317130327224731 , test acc:  0.4788171052932739 , test loss:  0.6818108558654785\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt3/250word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space means every always give lens around small page \\mathcal process create function second large must x reason non add end likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size little us 0 following years place output maybe url$ result world quite someone information option trying human check makes \\to times done life line keep looking made far found control works online open simple color list run idea pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working hard test never making user text kind particular side field last camera range great getting whether generally thus full mean energy position several due input called unless >\n",
      "Epoch 1/30\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.7064 - accuracy: 0.4826WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 86ms/step - loss: 0.7064 - accuracy: 0.4826 - val_loss: 0.6483 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6503 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0152s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6808 - accuracy: 0.5877\n",
      "training acc:  0.4826388955116272 , training loss:  0.7063726782798767 , val acc:  0.5902777910232544 , val loss:  0.6483440399169922 , test acc:  0.5876677632331848 , test loss:  0.6808208227157593\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt3/300word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space means every always give lens around small page \\mathcal process create function second large must x reason non add end likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size little us 0 following years place output maybe url$ result world quite someone information option trying human check makes \\to times done life line keep looking made far found control works online open simple color list run idea pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working hard test never making user text kind particular side field last camera range great getting whether generally thus full mean energy position several due input called unless > day difference big results clear true method body words else object matter provide free c map fine target amount multiple believe higher area n left server 4 uses class available cases top design per access avoid 5 common values gives three easily explanation feel post e.g. ask easy old character\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 1.1018 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 0.0590s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.9146 - accuracy: 0.4080WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 86ms/step - loss: 0.9146 - accuracy: 0.4080 - val_loss: 0.7817 - val_accuracy: 0.3264\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.6878 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0151s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.7064 - accuracy: 0.5019\n",
      "training acc:  0.4079861044883728 , training loss:  0.9145962595939636 , val acc:  0.3263888955116272 , val loss:  0.7817164063453674 , test acc:  0.5018875598907471 , test loss:  0.7063741683959961\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt3/350word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space means every always give lens around small page \\mathcal process create function second large must x reason non add end likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size little us 0 following years place output maybe url$ result world quite someone information option trying human check makes \\to times done life line keep looking made far found control works online open simple color list run idea pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working hard test never making user text kind particular side field last camera range great getting whether generally thus full mean energy position several due input called unless > day difference big results clear true method body words else object matter provide free c map fine target amount multiple believe higher area n left server 4 uses class available cases top design per access avoid 5 common values gives three easily explanation feel post e.g. ask easy old character \\ short 10 low larger close smaller actual seem yes standard become write within de next 100 support language main current away wo price link needs quality comes others earth hand along useful step related company call cost everything product wrong original show past bad nothing lenses term    changes\n",
      "Epoch 1/30\n",
      " 35/100 [=========>....................] - ETA: 4s - loss: 0.9646 - accuracy: 0.3964WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 86ms/step - loss: 0.9593 - accuracy: 0.4010 - val_loss: 0.7703 - val_accuracy: 0.2361\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6685 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0162s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7011 - accuracy: 0.5043\n",
      "training acc:  0.4010416567325592 , training loss:  0.9593048095703125 , val acc:  0.2361111044883728 , val loss:  0.7702615261077881 , test acc:  0.504299521446228 , test loss:  0.7011375427246094\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt3/400word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space means every always give lens around small page \\mathcal process create function second large must x reason non add end likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size little us 0 following years place output maybe url$ result world quite someone information option trying human check makes \\to times done life line keep looking made far found control works online open simple color list run idea pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working hard test never making user text kind particular side field last camera range great getting whether generally thus full mean energy position several due input called unless > day difference big results clear true method body words else object matter provide free c map fine target amount multiple believe higher area n left server 4 uses class available cases top design per access avoid 5 common values gives three easily explanation feel post e.g. ask easy old character \\ short 10 low larger close smaller actual seem yes standard become write within de next 100 support language main current away wo price link needs quality comes others earth hand along useful step related company call cost everything product wrong original show past bad nothing lenses term    changes although sort book english points necessary build needed got weapons thought perhaps choose taking numbers error experience meaning whole tell men ability characters source surface job certain game noise exactly site history created \\\\ model story view anyone examples car magic return personal weapon money mentioned effect children situation turn\n",
      "Epoch 1/30\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.8078 - accuracy: 0.3767WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 84ms/step - loss: 0.8078 - accuracy: 0.3767 - val_loss: 0.6976 - val_accuracy: 0.5833\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.6544 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0143s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6864 - accuracy: 0.5865\n",
      "training acc:  0.3767361044883728 , training loss:  0.8077951669692993 , val acc:  0.5833333134651184 , val loss:  0.6975935101509094 , test acc:  0.5865142345428467 , test loss:  0.686410129070282\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt3/450word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space means every always give lens around small page \\mathcal process create function second large must x reason non add end likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size little us 0 following years place output maybe url$ result world quite someone information option trying human check makes \\to times done life line keep looking made far found control works online open simple color list run idea pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working hard test never making user text kind particular side field last camera range great getting whether generally thus full mean energy position several due input called unless > day difference big results clear true method body words else object matter provide free c map fine target amount multiple believe higher area n left server 4 uses class available cases top design per access avoid 5 common values gives three easily explanation feel post e.g. ask easy old character \\ short 10 low larger close smaller actual seem yes standard become write within de next 100 support language main current away wo price link needs quality comes others earth hand along useful step related company call cost everything product wrong original show past bad nothing lenses term    changes although sort book english points necessary build needed got weapons thought perhaps choose taking numbers error experience meaning whole tell men ability characters source surface job certain game noise exactly site history created \\\\ model story view anyone examples car magic return personal weapon money mentioned effect children situation turn running allow focus search functions normal year length stop require lower ground sensor computer context directly yet says aperture mind fairly name black sometimes food fire depends fuel theory seen exact matl cause whatever speed elements outside constant extra questions humans que self terms happen research block future move subject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.8621 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0096s vs `on_train_batch_end` time: 0.0365s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.7118 - accuracy: 0.5851WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 86ms/step - loss: 0.7118 - accuracy: 0.5851 - val_loss: 0.6063 - val_accuracy: 0.6319\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6585 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0152s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6868 - accuracy: 0.5093\n",
      "training acc:  0.5850694179534912 , training loss:  0.7117776274681091 , val acc:  0.6319444179534912 , val loss:  0.6063346862792969 , test acc:  0.509333074092865 , test loss:  0.6868283152580261\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt3/500word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space means every always give lens around small page \\mathcal process create function second large must x reason non add end likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size little us 0 following years place output maybe url$ result world quite someone information option trying human check makes \\to times done life line keep looking made far found control works online open simple color list run idea pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working hard test never making user text kind particular side field last camera range great getting whether generally thus full mean energy position several due input called unless > day difference big results clear true method body words else object matter provide free c map fine target amount multiple believe higher area n left server 4 uses class available cases top design per access avoid 5 common values gives three easily explanation feel post e.g. ask easy old character \\ short 10 low larger close smaller actual seem yes standard become write within de next 100 support language main current away wo price link needs quality comes others earth hand along useful step related company call cost everything product wrong original show past bad nothing lenses term    changes although sort book english points necessary build needed got weapons thought perhaps choose taking numbers error experience meaning whole tell men ability characters source surface job certain game noise exactly site history created \\\\ model story view anyone examples car magic return personal weapon money mentioned effect children situation turn running allow focus search functions normal year length stop require lower ground sensor computer context directly yet says aperture mind fairly name black sometimes food fire depends fuel theory seen exact matl cause whatever speed elements outside constant extra questions humans que self terms happen research block future move subject approach gets software problems women society h otherwise known sequence distance content basically structure issues attack systems reasons especially factor direct expected technology defined later types options wide canon zero risk pay print looks head difficult assuming random hit offer guess requires slightly air engine takes update front strong applied\n",
      "Epoch 1/30\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.6642 - accuracy: 0.5712WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 87ms/step - loss: 0.6642 - accuracy: 0.5712 - val_loss: 0.6298 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6472 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0067s vs `on_test_batch_end` time: 0.0151s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6806 - accuracy: 0.5877\n",
      "training acc:  0.5711805820465088 , training loss:  0.664182722568512 , val acc:  0.5902777910232544 , val loss:  0.6298491954803467 , test acc:  0.5876677632331848 , test loss:  0.6805832982063293\n",
      "\n",
      "opt4\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt4/50word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 4s - loss: 1.1290 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0660s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.8703 - accuracy: 0.4618WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 84ms/step - loss: 0.8703 - accuracy: 0.4618 - val_loss: 0.6754 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6384 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_test_batch_end` time: 0.0056s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6807 - accuracy: 0.5877\n",
      "training acc:  0.4618055522441864 , training loss:  0.870315432548523 , val acc:  0.5902777910232544 , val loss:  0.6753698587417603 , test acc:  0.5876677632331848 , test loss:  0.6807133555412292\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt4/100word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.7635 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0015s vs `on_train_batch_end` time: 0.0105s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.6710 - accuracy: 0.5712WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36/100 [=========>....................] - 3s 84ms/step - loss: 0.6710 - accuracy: 0.5712 - val_loss: 0.6544 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.6295 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0147s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6867 - accuracy: 0.5631\n",
      "training acc:  0.5711805820465088 , training loss:  0.6710155606269836 , val acc:  0.5902777910232544 , val loss:  0.6543595194816589 , test acc:  0.563129186630249 , test loss:  0.686728835105896\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt4/150word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space means every always give lens around small page \\mathcal process create function second large must x reason non add end likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.8676 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_end` time: 0.0260s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.7229 - accuracy: 0.4774WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 85ms/step - loss: 0.7229 - accuracy: 0.4774 - val_loss: 0.6598 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6085 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0161s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6849 - accuracy: 0.5877\n",
      "training acc:  0.4774305522441864 , training loss:  0.7228875756263733 , val acc:  0.5902777910232544 , val loss:  0.6597572565078735 , test acc:  0.5876677632331848 , test loss:  0.684945285320282\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt4/200word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space means every always give lens around small page \\mathcal process create function second large must x reason non add end likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size little us 0 following years place output maybe url$ result world quite someone information option trying human check makes \\to times done life line keep looking made far found control works online open simple color list run idea\n",
      "Epoch 1/30\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.7277 - accuracy: 0.4913WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 88ms/step - loss: 0.7277 - accuracy: 0.4913 - val_loss: 0.6645 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6295 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_test_batch_end` time: 0.0207s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6830 - accuracy: 0.5877\n",
      "training acc:  0.4913194477558136 , training loss:  0.7276725769042969 , val acc:  0.5902777910232544 , val loss:  0.6644656658172607 , test acc:  0.5876677632331848 , test loss:  0.6829802989959717\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt4/250word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space means every always give lens around small page \\mathcal process create function second large must x reason non add end likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size little us 0 following years place output maybe url$ result world quite someone information option trying human check makes \\to times done life line keep looking made far found control works online open simple color list run idea pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working hard test never making user text kind particular side field last camera range great getting whether generally thus full mean energy position several due input called\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7335 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0782s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.6826 - accuracy: 0.5399 ETA: 5s - - ETA: 4s - loss: 0.6793 - accuracy: WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 87ms/step - loss: 0.6826 - accuracy: 0.5399 - val_loss: 0.6548 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.6103 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_test_batch_end` time: 0.0239s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6875 - accuracy: 0.5877\n",
      "training acc:  0.5399305820465088 , training loss:  0.682645857334137 , val acc:  0.5902777910232544 , val loss:  0.6547839045524597 , test acc:  0.5876677632331848 , test loss:  0.6874803304672241\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt4/300word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space means every always give lens around small page \\mathcal process create function second large must x reason non add end likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size little us 0 following years place output maybe url$ result world quite someone information option trying human check makes \\to times done life line keep looking made far found control works online open simple color list run idea pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working hard test never making user text kind particular side field last camera range great getting whether generally thus full mean energy position several due input called unless > day difference big results clear true method body words else object matter provide free c map fine target amount multiple believe higher area n left server 4 uses class available cases top design per access avoid 5 common values gives three easily explanation feel post e.g. ask easy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 1.2869 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0211s vs `on_train_batch_end` time: 0.0363s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.8925 - accuracy: 0.4444WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 86ms/step - loss: 0.8925 - accuracy: 0.4444 - val_loss: 0.6869 - val_accuracy: 0.5903\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6522 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0140s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6806 - accuracy: 0.5877\n",
      "training acc:  0.4444444477558136 , training loss:  0.892547070980072 , val acc:  0.5902777910232544 , val loss:  0.6869304180145264 , test acc:  0.5876677632331848 , test loss:  0.6806114315986633\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt4/350word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space means every always give lens around small page \\mathcal process create function second large must x reason non add end likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size little us 0 following years place output maybe url$ result world quite someone information option trying human check makes \\to times done life line keep looking made far found control works online open simple color list run idea pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working hard test never making user text kind particular side field last camera range great getting whether generally thus full mean energy position several due input called unless > day difference big results clear true method body words else object matter provide free c map fine target amount multiple believe higher area n left server 4 uses class available cases top design per access avoid 5 common values gives three easily explanation feel post e.g. ask easy old character \\ short 10 low larger close smaller actual seem yes standard become write within de next 100 support language main current away wo price link needs quality comes others earth hand along useful step related company call cost everything product wrong original show past bad nothing lenses term\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 1.3029 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0026s vs `on_train_batch_end` time: 0.0438s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 1.4553 - accuracy: 0.4271WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 85ms/step - loss: 1.4553 - accuracy: 0.4271 - val_loss: 1.0342 - val_accuracy: 0.3889\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.9236 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0154s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7963 - accuracy: 0.4433\n",
      "training acc:  0.4270833432674408 , training loss:  1.4552655220031738 , val acc:  0.3888888955116272 , val loss:  1.0341901779174805 , test acc:  0.4432676136493683 , test loss:  0.7963205575942993\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt4/400word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space means every always give lens around small page \\mathcal process create function second large must x reason non add end likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size little us 0 following years place output maybe url$ result world quite someone information option trying human check makes \\to times done life line keep looking made far found control works online open simple color list run idea pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working hard test never making user text kind particular side field last camera range great getting whether generally thus full mean energy position several due input called unless > day difference big results clear true method body words else object matter provide free c map fine target amount multiple believe higher area n left server 4 uses class available cases top design per access avoid 5 common values gives three easily explanation feel post e.g. ask easy old character \\ short 10 low larger close smaller actual seem yes standard become write within de next 100 support language main current away wo price link needs quality comes others earth hand along useful step related company call cost everything product wrong original show past bad nothing lenses term    changes although sort book english points necessary build needed got weapons thought perhaps choose taking numbers error experience meaning whole tell men ability characters source surface job certain game noise exactly site history created \\\\ model story view anyone examples car magic return personal weapon money mentioned effect children\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.7534 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0315s vs `on_train_batch_end` time: 0.0578s). Check your callbacks.\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 0.6676 - accuracy: 0.5747 ETA: 5s - loss: 0.6679 - accuracy: 0. - ETA: 4s - loss: 0.665WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 86ms/step - loss: 0.6676 - accuracy: 0.5747 - val_loss: 0.6530 - val_accuracy: 0.6250\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6520 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0126s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6912 - accuracy: 0.5300\n",
      "training acc:  0.5746527910232544 , training loss:  0.6675941944122314 , val acc:  0.625 , val loss:  0.6529677510261536 , test acc:  0.529991626739502 , test loss:  0.6911580562591553\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt4/450word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space means every always give lens around small page \\mathcal process create function second large must x reason non add end likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size little us 0 following years place output maybe url$ result world quite someone information option trying human check makes \\to times done life line keep looking made far found control works online open simple color list run idea pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working hard test never making user text kind particular side field last camera range great getting whether generally thus full mean energy position several due input called unless > day difference big results clear true method body words else object matter provide free c map fine target amount multiple believe higher area n left server 4 uses class available cases top design per access avoid 5 common values gives three easily explanation feel post e.g. ask easy old character \\ short 10 low larger close smaller actual seem yes standard become write within de next 100 support language main current away wo price link needs quality comes others earth hand along useful step related company call cost everything product wrong original show past bad nothing lenses term    changes although sort book english points necessary build needed got weapons thought perhaps choose taking numbers error experience meaning whole tell men ability characters source surface job certain game noise exactly site history created \\\\ model story view anyone examples car magic return personal weapon money mentioned effect children situation turn running allow focus search functions normal year length stop require lower ground sensor computer context directly yet says aperture mind fairly name black sometimes food fire depends fuel theory seen exact matl cause whatever speed elements outside constant extra questions humans que self terms happen research block future\n",
      "Epoch 1/30\n",
      " 36/100 [=========>....................] - ETA: 4s - loss: 1.3060 - accuracy: 0.4288 ETA: 5s - los - ETA: 4s - loss: 1.3797 - acWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 87ms/step - loss: 1.3060 - accuracy: 0.4288 - val_loss: 0.9250 - val_accuracy: 0.3889\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.8493 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0079s vs `on_test_batch_end` time: 0.0136s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.7540 - accuracy: 0.4486\n",
      "training acc:  0.4288194477558136 , training loss:  1.305997371673584 , val acc:  0.3888888955116272 , val loss:  0.9250485301017761 , test acc:  0.44861575961112976 , test loss:  0.7540118098258972\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_1800/opt4/500word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like / also could time get need - even might make way people two want work first may = using think much   different know see really something good used 1 'm example well question probably case since many set try 've point however things still take 're 2 say number another system enough problem actually better find new possible without answer power long go code 'd look + let back data able right 3 value going high either best ca sure given form less image thing 'll & change part space means every always give lens around small page \\mathcal process create function second large must x reason non add end likely lot often mm word usually seems < least though water bit real paper already    specific based god simply read light order help note anything start person come similar type level said rather understand single instead etc version size little us 0 following years place output maybe url$ result world quite someone information option trying human check makes \\to times done life line keep looking made far found control works online open simple color list run idea pretty # consider % important b issue key i.e. sense edit almost assume group put solution file course state fact general correct bytes working hard test never making user text kind particular side field last camera range great getting whether generally thus full mean energy position several due input called unless > day difference big results clear true method body words else object matter provide free c map fine target amount multiple believe higher area n left server 4 uses class available cases top design per access avoid 5 common values gives three easily explanation feel post e.g. ask easy old character \\ short 10 low larger close smaller actual seem yes standard become write within de next 100 support language main current away wo price link needs quality comes others earth hand along useful step related company call cost everything product wrong original show past bad nothing lenses term    changes although sort book english points necessary build needed got weapons thought perhaps choose taking numbers error experience meaning whole tell men ability characters source surface job certain game noise exactly site history created \\\\ model story view anyone examples car magic return personal weapon money mentioned effect children situation turn running allow focus search functions normal year length stop require lower ground sensor computer context directly yet says aperture mind fairly name black sometimes food fire depends fuel theory seen exact matl cause whatever speed elements outside constant extra questions humans que self terms happen research block future move subject approach gets software problems women society h otherwise known sequence distance content basically structure issues attack systems reasons especially factor direct expected technology defined later types options wide canon zero risk pay print looks head difficult assuming random hit offer guess requires slightly air engine takes update front\n",
      "Epoch 1/30\n",
      " 35/100 [=========>....................] - ETA: 4s - loss: 0.7445 - accuracy: 0.5768WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 36/100 [=========>....................] - 3s 86ms/step - loss: 0.7440 - accuracy: 0.5729 - val_loss: 0.6374 - val_accuracy: 0.6250\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6308 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0151s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6961 - accuracy: 0.5260\n",
      "training acc:  0.5729166865348816 , training loss:  0.7440049648284912 , val acc:  0.625 , val loss:  0.6374008655548096 , test acc:  0.5260066986083984 , test loss:  0.6961092948913574\n",
      "\n",
      "Started evaluation for: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_2000.csv\n",
      "Loading training file from path:  /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/datasets/training_2000.csv\n",
      "(586, 6)\n",
      "Loading testing file from path:  /home/sukanya/PhD/Datasets/PAN SCD/pan19-style-change-detection/processed/test.csv\n",
      "(9538, 5)\n",
      "opt1\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt1/50word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 4s - loss: 0.7562 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0549s). Check your callbacks.\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.6796 - accuracy: 0.5647WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 91ms/step - loss: 0.6796 - accuracy: 0.5647 - val_loss: 0.6621 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.5896 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_test_batch_end` time: 0.0107s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6907 - accuracy: 0.5877\n",
      "training acc:  0.5646551847457886 , training loss:  0.6796069741249084 , val acc:  0.6071428656578064 , val loss:  0.6621322631835938 , test acc:  0.5876677632331848 , test loss:  0.6907185316085815\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt1/100word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me\n",
      "Epoch 1/30\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.7323 - accuracy: 0.4763WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 90ms/step - loss: 0.7323 - accuracy: 0.4763 - val_loss: 0.6594 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6019 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0156s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6866 - accuracy: 0.5877\n",
      "training acc:  0.4762931168079376 , training loss:  0.7323004007339478 , val acc:  0.6071428656578064 , val loss:  0.6593688130378723 , test acc:  0.5876677632331848 , test loss:  0.6865898370742798\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt1/150word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me something see different really = good know used 'm example its 1 being well each probably been set many case since were point question his now things however those who try am 're had still enough take say number between 've 2 system over after both better find actually another\n",
      "Epoch 1/30\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.8871 - accuracy: 0.3556WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 89ms/step - loss: 0.8871 - accuracy: 0.3556 - val_loss: 0.6848 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6663 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0153s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6834 - accuracy: 0.5877\n",
      "training acc:  0.3556034564971924 , training loss:  0.8871446847915649 , val acc:  0.6071428656578064 , val loss:  0.6848115921020508 , test acc:  0.5876677632331848 , test loss:  0.6834452152252197\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt1/200word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me something see different really = good know used 'm example its 1 being well each probably been set many case since were point question his now things however those who try am 're had still enough take say number between 've 2 system over after both better find actually another while possible without answer why new power too look did data go code long let problem right through 'd down back \\mathcal able value before image our going high either less 3 part lens sure having form ca own change best means + & thing always off 'll around space\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6888 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0096s vs `on_train_batch_end` time: 0.0475s). Check your callbacks.\n",
      " 28/100 [=======>......................] - ETA: 5s - loss: 0.6315 - accuracy: 0.6473WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 90ms/step - loss: 0.6359 - accuracy: 0.6422 - val_loss: 0.6706 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.5618 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0146s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7096 - accuracy: 0.5877\n",
      "training acc:  0.642241358757019 , training loss:  0.6358574628829956 , val acc:  0.6071428656578064 , val loss:  0.6706430315971375 , test acc:  0.5876677632331848 , test loss:  0.7095727920532227\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt1/250word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me something see different really = good know used 'm example its 1 being well each probably been set many case since were point question his now things however those who try am 're had still enough take say number between 've 2 system over after both better find actually another while possible without answer why new power too look did data go code long let problem right through 'd down back \\mathcal able value before image our going high either less 3 part lens sure having form ca own change best means + & thing always off 'll around space give page given small create likely mm large process often every second add end function above usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 1.8609 - accuracy: 0.3438WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0103s vs `on_train_batch_end` time: 0.0598s). Check your callbacks.\n",
      " 29/100 [=======>......................] - ETA: 5s - loss: 1.3655 - accuracy: 0.3578WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 94ms/step - loss: 1.3655 - accuracy: 0.3578 - val_loss: 0.8965 - val_accuracy: 0.3929\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.8843 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0143s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7390 - accuracy: 0.4359\n",
      "training acc:  0.35775861144065857 , training loss:  1.3655341863632202 , val acc:  0.3928571343421936 , val loss:  0.8964751362800598 , test acc:  0.43592700362205505 , test loss:  0.739039421081543\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt1/300word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me something see different really = good know used 'm example its 1 being well each probably been set many case since were point question his now things however those who try am 're had still enough take say number between 've 2 system over after both better find actually another while possible without answer why new power too look did data go code long let problem right through 'd down back \\mathcal able value before image our going high either less 3 part lens sure having form ca own change best means + & thing always off 'll around space give page given small create likely mm large process often every second add end function above usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe again come check few quite someone size anything information trying place made instead control 0 us understand looking makes person once issue real said option following keep him consider simple works under idea state output open world % far itself i.e. map doing pretty found line\n",
      "Epoch 1/30\n",
      " 29/100 [=======>......................] - ETA: 5s - loss: 0.7183 - accuracy: 0.6422WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 93ms/step - loss: 0.7183 - accuracy: 0.6422 - val_loss: 0.6489 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.5828 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0164s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7027 - accuracy: 0.5666\n",
      "training acc:  0.642241358757019 , training loss:  0.7183137536048889 , val acc:  0.6071428656578064 , val loss:  0.6489266157150269 , test acc:  0.5665897727012634 , test loss:  0.7027475237846375\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt1/350word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me something see different really = good know used 'm example its 1 being well each probably been set many case since were point question his now things however those who try am 're had still enough take say number between 've 2 system over after both better find actually another while possible without answer why new power too look did data go code long let problem right through 'd down back \\mathcal able value before image our going high either less 3 part lens sure having form ca own change best means + & thing always off 'll around space give page given small create likely mm large process often every second add end function above usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe again come check few quite someone size anything information trying place made instead control 0 us understand looking makes person once issue real said option following keep him consider simple works under idea state output open world % far itself i.e. map doing pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b against text group never particular below several key position provide energy left input\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 1.3935 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0419s vs `on_train_batch_end` time: 0.0677s). Check your callbacks.\n",
      " 29/100 [=======>......................] - ETA: 5s - loss: 1.1899 - accuracy: 0.3578WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 93ms/step - loss: 1.1899 - accuracy: 0.3578 - val_loss: 0.9104 - val_accuracy: 0.3929\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.8967 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0138s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7465 - accuracy: 0.4262\n",
      "training acc:  0.35775861144065857 , training loss:  1.1899268627166748 , val acc:  0.3928571343421936 , val loss:  0.9103947281837463 , test acc:  0.42617449164390564 , test loss:  0.7464566826820374\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt1/400word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me something see different really = good know used 'm example its 1 being well each probably been set many case since were point question his now things however those who try am 're had still enough take say number between 've 2 system over after both better find actually another while possible without answer why new power too look did data go code long let problem right through 'd down back \\mathcal able value before image our going high either less 3 part lens sure having form ca own change best means + & thing always off 'll around space give page given small create likely mm large process often every second add end function above usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe again come check few quite someone size anything information trying place made instead control 0 us understand looking makes person once issue real said option following keep him consider simple works under idea state output open world % far itself i.e. map doing pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b against text group never particular below several key position provide energy left input bytes fine clear until she true object mean > method difference day easy else unless per side smaller avoid class easily available cases last support multiple larger c whether access kind believe top common called amount results close old uses post words n 4 area free matter \\ target higher\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.6924 - accuracy: 0.5194WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 93ms/step - loss: 0.6924 - accuracy: 0.5194 - val_loss: 0.6553 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.5943 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0148s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6896 - accuracy: 0.5877\n",
      "training acc:  0.5193965435028076 , training loss:  0.6924108266830444 , val acc:  0.6071428656578064 , val loss:  0.6552930474281311 , test acc:  0.5876677632331848 , test loss:  0.6895654797554016\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt1/450word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me something see different really = good know used 'm example its 1 being well each probably been set many case since were point question his now things however those who try am 're had still enough take say number between 've 2 system over after both better find actually another while possible without answer why new power too look did data go code long let problem right through 'd down back \\mathcal able value before image our going high either less 3 part lens sure having form ca own change best means + & thing always off 'll around space give page given small create likely mm large process often every second add end function above usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe again come check few quite someone size anything information trying place made instead control 0 us understand looking makes person once issue real said option following keep him consider simple works under idea state output open world % far itself i.e. map doing pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b against text group never particular below several key position provide energy left input bytes fine clear until she true object mean > method difference day easy else unless per side smaller avoid class easily available cases last support multiple larger c whether access kind believe top common called amount results close old uses post words n 4 area free matter \\ target higher weapons within current become 100 everything e.g. 10 useful design values actual lenses her gives 5 comes subject running explanation d ask low language seem feel quality character de    big game perhaps surface model link short s yes view english ability children situation step source call needs tell away\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6291 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0398s vs `on_train_batch_end` time: 0.0609s). Check your callbacks.\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.6412 - accuracy: 0.6422 ETA: 5s - loss: 0.6504 - accuraWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 92ms/step - loss: 0.6412 - accuracy: 0.6422 - val_loss: 0.6476 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.5922 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_test_batch_end` time: 0.0114s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6975 - accuracy: 0.5764\n",
      "training acc:  0.642241358757019 , training loss:  0.6411617994308472 , val acc:  0.6071428656578064 , val loss:  0.6476441025733948 , test acc:  0.5764471292495728 , test loss:  0.697544515132904\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt1/500word_list.txt\n",
      "WORD INDEX STR: the to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me something see different really = good know used 'm example its 1 being well each probably been set many case since were point question his now things however those who try am 're had still enough take say number between 've 2 system over after both better find actually another while possible without answer why new power too look did data go code long let problem right through 'd down back \\mathcal able value before image our going high either less 3 part lens sure having form ca own change best means + & thing always off 'll around space give page given small create likely mm large process often every second add end function above usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe again come check few quite someone size anything information trying place made instead control 0 us understand looking makes person once issue real said option following keep him consider simple works under idea state output open world % far itself i.e. map doing pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b against text group never particular below several key position provide energy left input bytes fine clear until she true object mean > method difference day easy else unless per side smaller avoid class easily available cases last support multiple larger c whether access kind believe top common called amount results close old uses post words n 4 area free matter \\ target higher weapons within current become 100 everything e.g. 10 useful design values actual lenses her gives 5 comes subject running explanation d ask low language seem feel quality character de    big game perhaps surface model link short s yes view english ability children situation step source call needs tell away bad related wo along original fire main three y write others error server car next choose although price needed product show points body elements thought examples meaning fuel characters book necessary hand wrong standard aperture past normal got sort certain numbers length sensor exactly changes return men taking magic que\n",
      "Epoch 1/30\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.6453 - accuracy: 0.6422WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29/100 [=======>......................] - 3s 93ms/step - loss: 0.6453 - accuracy: 0.6422 - val_loss: 0.6532 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.5887 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0152s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6937 - accuracy: 0.5877\n",
      "training acc:  0.642241358757019 , training loss:  0.6453143358230591 , val acc:  0.6071428656578064 , val loss:  0.6531606316566467 , test acc:  0.5876677632331848 , test loss:  0.6937457323074341\n",
      "\n",
      "opt2\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt2/50word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6831 - accuracy: 0.5625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0264s vs `on_train_batch_end` time: 0.0649s). Check your callbacks.\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.6381 - accuracy: 0.6422WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 92ms/step - loss: 0.6381 - accuracy: 0.6422 - val_loss: 0.6571 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.5934 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_test_batch_end` time: 0.0136s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6909 - accuracy: 0.5849\n",
      "training acc:  0.642241358757019 , training loss:  0.6381403207778931 , val acc:  0.6071428656578064 , val loss:  0.6570706963539124 , test acc:  0.5849412679672241 , test loss:  0.6908725500106812\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt2/100word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 4s - loss: 0.4240 - accuracy: 0.8438WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_end` time: 0.0250s). Check your callbacks.\n",
      " 28/100 [=======>......................] - ETA: 5s - loss: 0.6771 - accuracy: 0.6317 - ETA: 5s - loss: 0.6783 - accuracy: 0.63WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 91ms/step - loss: 0.6709 - accuracy: 0.6422 - val_loss: 0.6514 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6111 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0162s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6908 - accuracy: 0.5783\n",
      "training acc:  0.642241358757019 , training loss:  0.6708651781082153 , val acc:  0.6071428656578064 , val loss:  0.651395320892334 , test acc:  0.5783347487449646 , test loss:  0.6908156275749207\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt2/150word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me something see different really = good know used 'm example its 1 being well each probably been set many case since were point question his now things however those who try am 're had still enough take say number between 've 2 system over after both better find\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 4s - loss: 0.7317 - accuracy: 0.3438WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0329s vs `on_train_batch_end` time: 0.0516s). Check your callbacks.\n",
      " 28/100 [=======>......................] - ETA: 5s - loss: 0.6722 - accuracy: 0.5737WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 92ms/step - loss: 0.6644 - accuracy: 0.5862 - val_loss: 0.6584 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.5836 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0141s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6926 - accuracy: 0.5877\n",
      "training acc:  0.5862069129943848 , training loss:  0.6643549799919128 , val acc:  0.6071428656578064 , val loss:  0.6583896279335022 , test acc:  0.5876677632331848 , test loss:  0.6925607323646545\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt2/200word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me something see different really = good know used 'm example its 1 being well each probably been set many case since were point question his now things however those who try am 're had still enough take say number between 've 2 system over after both better find actually another while possible without answer why new power too look did data go code long let problem right through 'd down back \\mathcal able value before image our going high either less 3 part lens sure having form ca own change best means + & thing always off 'll\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6074 - accuracy: 0.7188WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0168s vs `on_train_batch_end` time: 0.0475s). Check your callbacks.\n",
      " 28/100 [=======>......................] - ETA: 5s - loss: 0.6384 - accuracy: 0.6451WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 91ms/step - loss: 0.6401 - accuracy: 0.6422 - val_loss: 0.6474 - val_accuracy: 0.6071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/596 [..............................] - ETA: 5s - loss: 0.6063 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0063s vs `on_test_batch_end` time: 0.0155s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6930 - accuracy: 0.5751\n",
      "training acc:  0.642241358757019 , training loss:  0.6401029825210571 , val acc:  0.6071428656578064 , val loss:  0.6473627090454102 , test acc:  0.5750839114189148 , test loss:  0.6929933428764343\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt2/250word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me something see different really = good know used 'm example its 1 being well each probably been set many case since were point question his now things however those who try am 're had still enough take say number between 've 2 system over after both better find actually another while possible without answer why new power too look did data go code long let problem right through 'd down back \\mathcal able value before image our going high either less 3 part lens sure having form ca own change best means + & thing always off 'll around space give page given small create likely mm large process often every second add end function above usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start\n",
      "Epoch 1/30\n",
      " 29/100 [=======>......................] - ETA: 5s - loss: 0.8146 - accuracy: 0.3534 ETA: 5s - loss: 0.8WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 93ms/step - loss: 0.8146 - accuracy: 0.3534 - val_loss: 0.6877 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6739 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0144s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6856 - accuracy: 0.5877\n",
      "training acc:  0.3534482717514038 , training loss:  0.8145802021026611 , val acc:  0.6071428656578064 , val loss:  0.6877191662788391 , test acc:  0.5876677632331848 , test loss:  0.6856173276901245\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt2/300word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me something see different really = good know used 'm example its 1 being well each probably been set many case since were point question his now things however those who try am 're had still enough take say number between 've 2 system over after both better find actually another while possible without answer why new power too look did data go code long let problem right through 'd down back \\mathcal able value before image our going high either less 3 part lens sure having form ca own change best means + & thing always off 'll around space give page given small create likely mm large process often every second add end function above usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe again come check few quite someone size anything information trying place made instead control 0 us understand looking makes person once issue real said option following keep him consider simple works under idea state output open world % far itself i.e. map doing pretty\n",
      "Epoch 1/30\n",
      " 28/100 [=======>......................] - ETA: 5s - loss: 0.9100 - accuracy: 0.3862WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 92ms/step - loss: 0.9027 - accuracy: 0.3901 - val_loss: 0.6733 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6439 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0159s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6812 - accuracy: 0.5877\n",
      "training acc:  0.39008620381355286 , training loss:  0.9026885032653809 , val acc:  0.6071428656578064 , val loss:  0.6733253598213196 , test acc:  0.5876677632331848 , test loss:  0.6811633110046387\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt2/350word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me something see different really = good know used 'm example its 1 being well each probably been set many case since were point question his now things however those who try am 're had still enough take say number between 've 2 system over after both better find actually another while possible without answer why new power too look did data go code long let problem right through 'd down back \\mathcal able value before image our going high either less 3 part lens sure having form ca own change best means + & thing always off 'll around space give page given small create likely mm large process often every second add end function above usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe again come check few quite someone size anything information trying place made instead control 0 us understand looking makes person once issue real said option following keep him consider simple works under idea state output open world % far itself i.e. map doing pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b against text group never particular below several key position provide energy\n",
      "Epoch 1/30\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.6497 - accuracy: 0.6121 ETA: 5s - loss: 0.6475 - accuraWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 92ms/step - loss: 0.6497 - accuracy: 0.6121 - val_loss: 0.6611 - val_accuracy: 0.6071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/596 [..............................] - ETA: 3s - loss: 0.5700 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_test_batch_end` time: 0.0190s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7008 - accuracy: 0.5877\n",
      "training acc:  0.6120689511299133 , training loss:  0.6496830582618713 , val acc:  0.6071428656578064 , val loss:  0.6610950827598572 , test acc:  0.5876677632331848 , test loss:  0.700849175453186\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt2/400word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me something see different really = good know used 'm example its 1 being well each probably been set many case since were point question his now things however those who try am 're had still enough take say number between 've 2 system over after both better find actually another while possible without answer why new power too look did data go code long let problem right through 'd down back \\mathcal able value before image our going high either less 3 part lens sure having form ca own change best means + & thing always off 'll around space give page given small create likely mm large process often every second add end function above usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe again come check few quite someone size anything information trying place made instead control 0 us understand looking makes person once issue real said option following keep him consider simple works under idea state output open world % far itself i.e. map doing pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b against text group never particular below several key position provide energy left input bytes fine clear until she true object mean > method difference day easy else unless per side smaller avoid class easily available cases last support multiple larger c whether access kind believe top common called amount results close old uses post words n 4 area free matter \\\n",
      "Epoch 1/30\n",
      " 28/100 [=======>......................] - ETA: 5s - loss: 0.7232 - accuracy: 0.6451WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 92ms/step - loss: 0.7244 - accuracy: 0.6422 - val_loss: 0.6722 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.5537 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0168s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.7194 - accuracy: 0.5823\n",
      "training acc:  0.642241358757019 , training loss:  0.7244403958320618 , val acc:  0.6071428656578064 , val loss:  0.6722148656845093 , test acc:  0.5823196172714233 , test loss:  0.7194417119026184\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt2/450word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me something see different really = good know used 'm example its 1 being well each probably been set many case since were point question his now things however those who try am 're had still enough take say number between 've 2 system over after both better find actually another while possible without answer why new power too look did data go code long let problem right through 'd down back \\mathcal able value before image our going high either less 3 part lens sure having form ca own change best means + & thing always off 'll around space give page given small create likely mm large process often every second add end function above usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe again come check few quite someone size anything information trying place made instead control 0 us understand looking makes person once issue real said option following keep him consider simple works under idea state output open world % far itself i.e. map doing pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b against text group never particular below several key position provide energy left input bytes fine clear until she true object mean > method difference day easy else unless per side smaller avoid class easily available cases last support multiple larger c whether access kind believe top common called amount results close old uses post words n 4 area free matter \\ target higher weapons within current become 100 everything e.g. 10 useful design values actual lenses her gives 5 comes subject running explanation d ask low language seem feel quality character de    big game perhaps surface model link short s yes view english ability children situation step source call needs\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.4758 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0281s vs `on_train_batch_end` time: 0.0828s). Check your callbacks.\n",
      " 28/100 [=======>......................] - ETA: 5s - loss: 0.6536 - accuracy: 0.6473WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 93ms/step - loss: 0.6562 - accuracy: 0.6422 - val_loss: 0.6504 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.5811 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_test_batch_end` time: 0.0142s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6999 - accuracy: 0.5819\n",
      "training acc:  0.642241358757019 , training loss:  0.6562434434890747 , val acc:  0.6071428656578064 , val loss:  0.6503938436508179 , test acc:  0.5819001793861389 , test loss:  0.6999394297599792\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt2/500word_list.txt\n",
      "WORD INDEX STR: the . , to a  of and is that in you it i $ for be this with as   are on have or can not if would but your 's an from n't so do they at will by there one my which all what more some was we when use like then also has / no time could about other any only just them need than get their does should out how up - even where might make very same way because these two people want such into work he may here using first most much think   me something see different really = good know used 'm example its 1 being well each probably been set many case since were point question his now things however those who try am 're had still enough take say number between 've 2 system over after both better find actually another while possible without answer why new power too look did data go code long let problem right through 'd down back \\mathcal able value before image our going high either less 3 part lens sure having form ca own change best means + & thing always off 'll around space give page given small create likely mm large process often every second add end function above usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe again come check few quite someone size anything information trying place made instead control 0 us understand looking makes person once issue real said option following keep him consider simple works under idea state output open world % far itself i.e. map doing pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b against text group never particular below several key position provide energy left input bytes fine clear until she true object mean > method difference day easy else unless per side smaller avoid class easily available cases last support multiple larger c whether access kind believe top common called amount results close old uses post words n 4 area free matter \\ target higher weapons within current become 100 everything e.g. 10 useful design values actual lenses her gives 5 comes subject running explanation d ask low language seem feel quality character de    big game perhaps surface model link short s yes view english ability children situation step source call needs tell away bad related wo along original fire main three y write others error server car next choose although price needed product show points body elements thought examples meaning fuel characters book necessary hand wrong standard aperture past normal got sort certain numbers length sensor exactly changes return men taking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 29/100 [=======>......................] - ETA: 5s - loss: 0.8832 - accuracy: 0.3793 ETA: 5s - loss: 1.0245 - accura - ETA: 5s - loss: 0.9632 WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 94ms/step - loss: 0.8832 - accuracy: 0.3793 - val_loss: 0.6743 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6446 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0122s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.6808 - accuracy: 0.5877\n",
      "training acc:  0.37931033968925476 , training loss:  0.8831775188446045 , val acc:  0.6071428656578064 , val loss:  0.6743038892745972 , test acc:  0.5876677632331848 , test loss:  0.6807627081871033\n",
      "\n",
      "opt3\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt3/50word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.5971 - accuracy: 0.6875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0150s vs `on_train_batch_end` time: 0.0608s). Check your callbacks.\n",
      " 28/100 [=======>......................] - ETA: 4s - loss: 0.7597 - accuracy: 0.6384 ETA: 5s -WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 89ms/step - loss: 0.7484 - accuracy: 0.6444 - val_loss: 0.6709 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6270 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_test_batch_end` time: 0.0133s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.7244 - accuracy: 0.4716\n",
      "training acc:  0.6443965435028076 , training loss:  0.7484291195869446 , val acc:  0.6071428656578064 , val loss:  0.6709481477737427 , test acc:  0.471581369638443 , test loss:  0.7244176864624023\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt3/100word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing\n",
      "Epoch 1/30\n",
      " 28/100 [=======>......................] - ETA: 5s - loss: 0.6812 - accuracy: 0.5201 ETA: 5s - loss: 0WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 92ms/step - loss: 0.6842 - accuracy: 0.5172 - val_loss: 0.6351 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6426 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_test_batch_end` time: 0.0143s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6806 - accuracy: 0.5877\n",
      "training acc:  0.517241358757019 , training loss:  0.6842447519302368 , val acc:  0.6071428656578064 , val loss:  0.6351054310798645 , test acc:  0.5876677632331848 , test loss:  0.6806224584579468\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt3/150word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing always 'll around space give page given small create likely mm large process often every second add end function usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version\n",
      "Epoch 1/30\n",
      "  1/100 [..............................] - ETA: 0s - loss: 0.5600 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0108s). Check your callbacks.\n",
      " 28/100 [=======>......................] - ETA: 4s - loss: 0.6181 - accuracy: 0.6429WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 90ms/step - loss: 0.6168 - accuracy: 0.6422 - val_loss: 0.6130 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6555 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0131s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6844 - accuracy: 0.4813\n",
      "training acc:  0.642241358757019 , training loss:  0.6167568564414978 , val acc:  0.6071428656578064 , val loss:  0.6129869818687439 , test acc:  0.4813338816165924 , test loss:  0.6843858957290649\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt3/200word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing always 'll around space give page given small create likely mm large process often every second add end function usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe come check quite someone size anything information trying place made instead control 0 us understand looking makes person issue real said option following keep consider simple works idea state output open world % far i.e. map pretty found line almost correct file camera\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 4s - loss: 0.6646 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0437s vs `on_train_batch_end` time: 0.0723s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28/100 [=======>......................] - ETA: 4s - loss: 0.6292 - accuracy: 0.6473WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 89ms/step - loss: 0.6326 - accuracy: 0.6422 - val_loss: 0.6212 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6305 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0073s vs `on_test_batch_end` time: 0.0128s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6820 - accuracy: 0.5877\n",
      "training acc:  0.642241358757019 , training loss:  0.6326271295547485 , val acc:  0.6071428656578064 , val loss:  0.6212285757064819 , test acc:  0.5876677632331848 , test loss:  0.681999921798706\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt3/250word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing always 'll around space give page given small create likely mm large process often every second add end function usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe come check quite someone size anything information trying place made instead control 0 us understand looking makes person issue real said option following keep consider simple works idea state output open world % far i.e. map pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b text group never particular several key position provide energy left input bytes fine clear true object mean\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 1.4177 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_train_batch_end` time: 0.0210s). Check your callbacks.\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 1.3778 - accuracy: 0.3534WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 90ms/step - loss: 1.3778 - accuracy: 0.3534 - val_loss: 1.1117 - val_accuracy: 0.3750\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.7841 - accuracy: 0.4977\n",
      "training acc:  0.3534482717514038 , training loss:  1.3778396844863892 , val acc:  0.375 , val loss:  1.1116749048233032 , test acc:  0.49769294261932373 , test loss:  0.7840958833694458\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt3/300word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing always 'll around space give page given small create likely mm large process often every second add end function usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe come check quite someone size anything information trying place made instead control 0 us understand looking makes person issue real said option following keep consider simple works idea state output open world % far i.e. map pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b text group never particular several key position provide energy left input bytes fine clear true object mean > method difference day easy else unless per side smaller avoid class easily available cases last support multiple larger c whether access kind believe top common called amount results close old uses post words n 4 area free matter \\ target higher weapons within current become 100 everything e.g. 10\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 4s - loss: 0.7722 - accuracy: 0.3438WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0218s vs `on_train_batch_end` time: 0.0525s). Check your callbacks.\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.6691 - accuracy: 0.5668WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 90ms/step - loss: 0.6691 - accuracy: 0.5668 - val_loss: 0.6286 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6280 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0074s vs `on_test_batch_end` time: 0.0144s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6799 - accuracy: 0.5877\n",
      "training acc:  0.5668103694915771 , training loss:  0.669066309928894 , val acc:  0.6071428656578064 , val loss:  0.6285597681999207 , test acc:  0.5876677632331848 , test loss:  0.6798917055130005\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt3/350word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing always 'll around space give page given small create likely mm large process often every second add end function usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe come check quite someone size anything information trying place made instead control 0 us understand looking makes person issue real said option following keep consider simple works idea state output open world % far i.e. map pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b text group never particular several key position provide energy left input bytes fine clear true object mean > method difference day easy else unless per side smaller avoid class easily available cases last support multiple larger c whether access kind believe top common called amount results close old uses post words n 4 area free matter \\ target higher weapons within current become 100 everything e.g. 10 useful design values actual lenses gives 5 comes subject running explanation ask low language seem feel quality character de    big game perhaps surface model link short yes view english ability children situation step source call needs tell away bad related wo along original fire main three write others error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.9725 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0310s vs `on_train_batch_end` time: 0.0652s). Check your callbacks.\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.9165 - accuracy: 0.3405 ETAWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 89ms/step - loss: 0.9165 - accuracy: 0.3405 - val_loss: 0.7603 - val_accuracy: 0.2679\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6769 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_test_batch_end` time: 0.0161s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6997 - accuracy: 0.5024\n",
      "training acc:  0.3405172526836395 , training loss:  0.9165400266647339 , val acc:  0.2678571343421936 , val loss:  0.760252833366394 , test acc:  0.5024119019508362 , test loss:  0.6996743083000183\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt3/400word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing always 'll around space give page given small create likely mm large process often every second add end function usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe come check quite someone size anything information trying place made instead control 0 us understand looking makes person issue real said option following keep consider simple works idea state output open world % far i.e. map pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b text group never particular several key position provide energy left input bytes fine clear true object mean > method difference day easy else unless per side smaller avoid class easily available cases last support multiple larger c whether access kind believe top common called amount results close old uses post words n 4 area free matter \\ target higher weapons within current become 100 everything e.g. 10 useful design values actual lenses gives 5 comes subject running explanation ask low language seem feel quality character de    big game perhaps surface model link short yes view english ability children situation step source call needs tell away bad related wo along original fire main three write others error server car next choose although price needed product show points body elements thought examples meaning fuel characters book necessary hand wrong standard aperture past normal got sort certain numbers length sensor exactly changes return men taking magic que personal cost lower term sequence focus black require sometimes experience exact mentioned\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7917 - accuracy: 0.2188WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.0262s). Check your callbacks.\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.7034 - accuracy: 0.4806 ETA: 5s - loss: 0.7100 - accuracyWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 91ms/step - loss: 0.7034 - accuracy: 0.4806 - val_loss: 0.6411 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.6479 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0075s vs `on_test_batch_end` time: 0.0148s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6803 - accuracy: 0.5877\n",
      "training acc:  0.4806034564971924 , training loss:  0.7034064531326294 , val acc:  0.6071428656578064 , val loss:  0.6411208510398865 , test acc:  0.5876677632331848 , test loss:  0.6802677512168884\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt3/450word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing always 'll around space give page given small create likely mm large process often every second add end function usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe come check quite someone size anything information trying place made instead control 0 us understand looking makes person issue real said option following keep consider simple works idea state output open world % far i.e. map pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b text group never particular several key position provide energy left input bytes fine clear true object mean > method difference day easy else unless per side smaller avoid class easily available cases last support multiple larger c whether access kind believe top common called amount results close old uses post words n 4 area free matter \\ target higher weapons within current become 100 everything e.g. 10 useful design values actual lenses gives 5 comes subject running explanation ask low language seem feel quality character de    big game perhaps surface model link short yes view english ability children situation step source call needs tell away bad related wo along original fire main three write others error server car next choose although price needed product show points body elements thought examples meaning fuel characters book necessary hand wrong standard aperture past normal got sort certain numbers length sensor exactly changes return men taking magic que personal cost lower term sequence focus black require sometimes experience exact mentioned noise company effect weapon nothing outside context computer gets speed money build ground cause allow seen whatever turn matl block functions front extra fairly whole issues distance update stop anyone self ships name factor created problems attack job directly food move approach says depends expected defined technology humans h basically\n",
      "Epoch 1/30\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 1.5160 - accuracy: 0.3556 ETA: 5s - loss: 1.8079  - ETA: 5s - loss: 1.6397 - WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29/100 [=======>......................] - 3s 91ms/step - loss: 1.5160 - accuracy: 0.3556 - val_loss: 1.2198 - val_accuracy: 0.3750\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.8061 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_test_batch_end` time: 0.0114s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 25ms/step - loss: 0.8062 - accuracy: 0.4915\n",
      "training acc:  0.3556034564971924 , training loss:  1.5159751176834106 , val acc:  0.375 , val loss:  1.219765543937683 , test acc:  0.49150586128234863 , test loss:  0.8061934113502502\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt3/500word_list.txt\n",
      "WORD INDEX STR:  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing always 'll around space give page given small create likely mm large process often every second add end function usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe come check quite someone size anything information trying place made instead control 0 us understand looking makes person issue real said option following keep consider simple works idea state output open world % far i.e. map pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b text group never particular several key position provide energy left input bytes fine clear true object mean > method difference day easy else unless per side smaller avoid class easily available cases last support multiple larger c whether access kind believe top common called amount results close old uses post words n 4 area free matter \\ target higher weapons within current become 100 everything e.g. 10 useful design values actual lenses gives 5 comes subject running explanation ask low language seem feel quality character de    big game perhaps surface model link short yes view english ability children situation step source call needs tell away bad related wo along original fire main three write others error server car next choose although price needed product show points body elements thought examples meaning fuel characters book necessary hand wrong standard aperture past normal got sort certain numbers length sensor exactly changes return men taking magic que personal cost lower term sequence focus black require sometimes experience exact mentioned noise company effect weapon nothing outside context computer gets speed money build ground cause allow seen whatever turn matl block functions front extra fairly whole issues distance update stop anyone self ships name factor created problems attack job directly food move approach says depends expected defined technology humans h basically hit earth direct yet later ship wide requires terms default engine research theory ways site offer known difficult content options history canon structure states systems year reasons happens zero random verb happen play ones especially print women strong action air future society longer otherwise everyone mind detail looks questions head\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.7362 - accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0112s vs `on_train_batch_end` time: 0.0506s). Check your callbacks.\n",
      " 28/100 [=======>......................] - ETA: 5s - loss: 0.6198 - accuracy: 0.6473WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 93ms/step - loss: 0.6184 - accuracy: 0.6466 - val_loss: 0.6016 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 6s - loss: 0.6442 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0194s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6848 - accuracy: 0.5096\n",
      "training acc:  0.6465517282485962 , training loss:  0.6184397339820862 , val acc:  0.6071428656578064 , val loss:  0.6016480326652527 , test acc:  0.5096476674079895 , test loss:  0.6848328113555908\n",
      "\n",
      "opt4\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt4/50word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 3s - loss: 0.6826 - accuracy: 0.5938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0113s vs `on_train_batch_end` time: 0.0646s). Check your callbacks.\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.6459 - accuracy: 0.6422 ETA: 5s - loss: 0WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 91ms/step - loss: 0.6459 - accuracy: 0.6422 - val_loss: 0.6496 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.5770 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0158s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6946 - accuracy: 0.5877\n",
      "training acc:  0.642241358757019 , training loss:  0.6458536982536316 , val acc:  0.6071428656578064 , val loss:  0.6496483683586121 , test acc:  0.5876677632331848 , test loss:  0.6945503354072571\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt4/100word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means +\n",
      "Epoch 1/30\n",
      " 29/100 [=======>......................] - ETA: 5s - loss: 0.9331 - accuracy: 0.4009 ETA: 5s - loss: 0.9970 - accuraWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 92ms/step - loss: 0.9331 - accuracy: 0.4009 - val_loss: 0.6715 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6353 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_test_batch_end` time: 0.0150s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6801 - accuracy: 0.5877\n",
      "training acc:  0.40086206793785095 , training loss:  0.9331123232841492 , val acc:  0.6071428656578064 , val loss:  0.6714800596237183 , test acc:  0.5876677632331848 , test loss:  0.6801491975784302\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt4/150word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing always 'll around space give page given small create likely mm large process often every second add end function usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather\n",
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 2s - loss: 0.5283 - accuracy: 0.8125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0677s). Check your callbacks.\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.6326 - accuracy: 0.6422WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 90ms/step - loss: 0.6326 - accuracy: 0.6422 - val_loss: 0.6418 - val_accuracy: 0.6071\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6902 - accuracy: 0.5661\n",
      "training acc:  0.642241358757019 , training loss:  0.6326374411582947 , val acc:  0.6071428656578064 , val loss:  0.6417688131332397 , test acc:  0.5660654306411743 , test loss:  0.6901853680610657\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt4/200word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing always 'll around space give page given small create likely mm large process often every second add end function usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe come check quite someone size anything information trying place made instead control 0 us understand looking makes person issue real said option following keep consider simple works idea state output open world % far i.e. map pretty found line almost correct\n",
      "Epoch 1/30\n",
      " 28/100 [=======>......................] - ETA: 4s - loss: 0.6788 - accuracy: 0.5491WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 88ms/step - loss: 0.6812 - accuracy: 0.5474 - val_loss: 0.6498 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.5798 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0066s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6964 - accuracy: 0.5877\n",
      "training acc:  0.5474137663841248 , training loss:  0.6812482476234436 , val acc:  0.6071428656578064 , val loss:  0.6497626304626465 , test acc:  0.5876677632331848 , test loss:  0.6963742971420288\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt4/250word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing always 'll around space give page given small create likely mm large process often every second add end function usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe come check quite someone size anything information trying place made instead control 0 us understand looking makes person issue real said option following keep consider simple works idea state output open world % far i.e. map pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b text group never particular several key position provide energy left input bytes fine clear true\n",
      "Epoch 1/30\n",
      " 28/100 [=======>......................] - ETA: 5s - loss: 0.9773 - accuracy: 0.3527WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 91ms/step - loss: 0.9684 - accuracy: 0.3578 - val_loss: 0.7394 - val_accuracy: 0.3571\n",
      "  1/596 [..............................] - ETA: 7s - loss: 0.7167 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_test_batch_end` time: 0.0145s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6956 - accuracy: 0.5029\n",
      "training acc:  0.35775861144065857 , training loss:  0.9684122800827026 , val acc:  0.3571428656578064 , val loss:  0.7394115328788757 , test acc:  0.5029362440109253 , test loss:  0.6956095099449158\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt4/300word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing always 'll around space give page given small create likely mm large process often every second add end function usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe come check quite someone size anything information trying place made instead control 0 us understand looking makes person issue real said option following keep consider simple works idea state output open world % far i.e. map pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b text group never particular several key position provide energy left input bytes fine clear true object mean > method difference day easy else unless per side smaller avoid class easily available cases last support multiple larger c whether access kind believe top common called amount results close old uses post words n 4 area free matter \\ target higher weapons within current become 100 everything\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/100 [..............................] - ETA: 4s - loss: 0.7536 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0181s vs `on_train_batch_end` time: 0.0439s). Check your callbacks.\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.6353 - accuracy: 0.6422 ETA: 5s - loss: 0.6407 - accuraWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 90ms/step - loss: 0.6353 - accuracy: 0.6422 - val_loss: 0.6342 - val_accuracy: 0.6250\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.6067 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_test_batch_end` time: 0.0233s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6949 - accuracy: 0.5559\n",
      "training acc:  0.642241358757019 , training loss:  0.6352719068527222 , val acc:  0.625 , val loss:  0.6342214941978455 , test acc:  0.5558934807777405 , test loss:  0.6949007511138916\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt4/350word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing always 'll around space give page given small create likely mm large process often every second add end function usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe come check quite someone size anything information trying place made instead control 0 us understand looking makes person issue real said option following keep consider simple works idea state output open world % far i.e. map pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b text group never particular several key position provide energy left input bytes fine clear true object mean > method difference day easy else unless per side smaller avoid class easily available cases last support multiple larger c whether access kind believe top common called amount results close old uses post words n 4 area free matter \\ target higher weapons within current become 100 everything e.g. 10 useful design values actual lenses gives 5 comes subject running explanation ask low language seem feel quality character de    big game perhaps surface model link short yes view english ability children situation step source call needs tell away bad related wo along original fire main three write\n",
      "Epoch 1/30\n",
      " 28/100 [=======>......................] - ETA: 5s - loss: 0.6255 - accuracy: 0.6451WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 90ms/step - loss: 0.6281 - accuracy: 0.6422 - val_loss: 0.6354 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 5s - loss: 0.6073 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0158s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6944 - accuracy: 0.5618\n",
      "training acc:  0.642241358757019 , training loss:  0.6281297206878662 , val acc:  0.6071428656578064 , val loss:  0.6354211568832397 , test acc:  0.5617659687995911 , test loss:  0.6944422125816345\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt4/400word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing always 'll around space give page given small create likely mm large process often every second add end function usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe come check quite someone size anything information trying place made instead control 0 us understand looking makes person issue real said option following keep consider simple works idea state output open world % far i.e. map pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b text group never particular several key position provide energy left input bytes fine clear true object mean > method difference day easy else unless per side smaller avoid class easily available cases last support multiple larger c whether access kind believe top common called amount results close old uses post words n 4 area free matter \\ target higher weapons within current become 100 everything e.g. 10 useful design values actual lenses gives 5 comes subject running explanation ask low language seem feel quality character de    big game perhaps surface model link short yes view english ability children situation step source call needs tell away bad related wo along original fire main three write others error server car next choose although price needed product show points body elements thought examples meaning fuel characters book necessary hand wrong standard aperture past normal got sort certain numbers length sensor exactly changes return men taking magic que personal cost lower term sequence focus black require sometimes experience\n",
      "Epoch 1/30\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.6295 - accuracy: 0.6422 ETA: 5s - loss: 0.6354 WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 90ms/step - loss: 0.6295 - accuracy: 0.6422 - val_loss: 0.6411 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 4s - loss: 0.5928 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0068s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6967 - accuracy: 0.5877\n",
      "training acc:  0.642241358757019 , training loss:  0.6294537782669067 , val acc:  0.6071428656578064 , val loss:  0.6410702466964722 , test acc:  0.5876677632331848 , test loss:  0.6966627240180969\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt4/450word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing always 'll around space give page given small create likely mm large process often every second add end function usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe come check quite someone size anything information trying place made instead control 0 us understand looking makes person issue real said option following keep consider simple works idea state output open world % far i.e. map pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b text group never particular several key position provide energy left input bytes fine clear true object mean > method difference day easy else unless per side smaller avoid class easily available cases last support multiple larger c whether access kind believe top common called amount results close old uses post words n 4 area free matter \\ target higher weapons within current become 100 everything e.g. 10 useful design values actual lenses gives 5 comes subject running explanation ask low language seem feel quality character de    big game perhaps surface model link short yes view english ability children situation step source call needs tell away bad related wo along original fire main three write others error server car next choose although price needed product show points body elements thought examples meaning fuel characters book necessary hand wrong standard aperture past normal got sort certain numbers length sensor exactly changes return men taking magic que personal cost lower term sequence focus black require sometimes experience exact mentioned noise company effect weapon nothing outside context computer gets speed money build ground cause allow seen whatever turn matl block functions front extra fairly whole issues distance update stop anyone self ships name factor created problems attack job directly food move approach says depends expected defined technology humans\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.7418 - accuracy: 0.4828WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 91ms/step - loss: 0.7418 - accuracy: 0.4828 - val_loss: 0.6501 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 8s - loss: 0.6074 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0064s vs `on_test_batch_end` time: 0.0149s). Check your callbacks.\n",
      "596/596 [==============================] - 14s 24ms/step - loss: 0.6860 - accuracy: 0.5877\n",
      "training acc:  0.48275861144065857 , training loss:  0.7417845726013184 , val acc:  0.6071428656578064 , val loss:  0.65009605884552 , test acc:  0.5876677632331848 , test loss:  0.685966432094574\n",
      "\n",
      "Evaluating: /home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/vocab/MPL_2000/opt4/500word_list.txt\n",
      "WORD INDEX STR: . ,  $   would 's n't one use like also / time could need get - even might make way two people want work may using first much think   something see different really = good know used 'm example 1 well probably set many case since point question things however try 're still enough take say number 've 2 system better find actually another possible without answer new power look data go code long let problem right 'd back \\mathcal able value image going high either less 3 part lens sure form ca change best means + & thing always 'll around space give page given small create likely mm large process often every second add end function usually < lot bit non reason specific seems # \\to must word    simply order single least though help light note x paper level based etc similar already rather read version start type result little url$ run maybe come check quite someone size anything information trying place made instead control 0 us understand looking makes person issue real said option following keep consider simple works idea state output open world % far i.e. map pretty found line almost correct file camera making color test edit done years working field general assume put thus sense user range online human solution god list important fact great generally times due water full getting course life hard b text group never particular several key position provide energy left input bytes fine clear true object mean > method difference day easy else unless per side smaller avoid class easily available cases last support multiple larger c whether access kind believe top common called amount results close old uses post words n 4 area free matter \\ target higher weapons within current become 100 everything e.g. 10 useful design values actual lenses gives 5 comes subject running explanation ask low language seem feel quality character de    big game perhaps surface model link short yes view english ability children situation step source call needs tell away bad related wo along original fire main three write others error server car next choose although price needed product show points body elements thought examples meaning fuel characters book necessary hand wrong standard aperture past normal got sort certain numbers length sensor exactly changes return men taking magic que personal cost lower term sequence focus black require sometimes experience exact mentioned noise company effect weapon nothing outside context computer gets speed money build ground cause allow seen whatever turn matl block functions front extra fairly whole issues distance update stop anyone self ships name factor created problems attack job directly food move approach says depends expected defined technology humans h basically hit earth direct yet later ship wide requires terms default engine research theory ways site offer known difficult content options history canon structure states systems year reasons happens zero random verb happen play ones especially print women strong action air future society longer otherwise everyone mind detail looks\n",
      "Epoch 1/30\n",
      " 29/100 [=======>......................] - ETA: 4s - loss: 0.6333 - accuracy: 0.6422WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3000 batches). You may need to use the repeat() function when building your dataset.\n",
      " 29/100 [=======>......................] - 3s 90ms/step - loss: 0.6333 - accuracy: 0.6422 - val_loss: 0.6384 - val_accuracy: 0.6071\n",
      "  1/596 [..............................] - ETA: 3s - loss: 0.5976 - accuracy: 0.7500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0065s vs `on_test_batch_end` time: 0.0180s). Check your callbacks.\n",
      "596/596 [==============================] - 15s 24ms/step - loss: 0.6953 - accuracy: 0.5702\n",
      "training acc:  0.642241358757019 , training loss:  0.6332914233207703 , val acc:  0.6071428656578064 , val loss:  0.6383984684944153 , test acc:  0.5701552033424377 , test loss:  0.6952690482139587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for training_file_path,vocab_folder,results_folder in zip(training_files,vocab_base_folder_list,results_folder_list):\n",
    "    print(\"Started evaluation for:\", training_file_path)\n",
    "    evaluate(training_file_path, testing_file_path, vocab_folder, results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''print(\"Loading training file from path: \", config.config_io.get('pan_19_processed_train'))\n",
    "training_data = pd.read_csv(config.config_io.get('pan_19_processed_train'))\n",
    "training_data.shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''training_data.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''training_data.columns'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''print(\"Loading testing file from path: \", config.config_io.get('pan_19_processed_test'))\n",
    "testing_data = pd.read_csv(config.config_io.get('pan_19_processed_test'))\n",
    "testing_data.shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''testing_data.head()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the feature set files\n",
    "We are doing it manually now for greater control. In the future, we must automate to read the files directly form the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_set_files (base_vocab_folder, vocab_size = range(50, 550,50)):\n",
    "    opt1 = [base_vocab_folder+\"/opt1/\"+str(i)+\"word_list.txt\" for i in vocab_size]\n",
    "    opt2 = [base_vocab_folder+\"/opt2/\"+str(i)+\"word_list.txt\" for i in vocab_size]\n",
    "    opt3 = [base_vocab_folder+\"/opt3/\"+str(i)+\"word_list.txt\" for i in vocab_size]\n",
    "    opt4 = [base_vocab_folder+\"/opt4/\"+str(i)+\"word_list.txt\" for i in vocab_size]\n",
    "    return {'opt1': opt1, 'opt2': opt2, 'opt3': opt3, 'opt4': opt4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index files\n",
    "'''\n",
    "feature_set_files = { 'opt1':\n",
    "['/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt1/50word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt1/100word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt1/150word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt1/200word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt1/250word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt1/300word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt1/350word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt1/400word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt1/450word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt1/500word_list.txt'],\n",
    "                    'opt2':\n",
    "['/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt2/50word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt2/100word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt2/150word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt2/200word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt2/250word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt2/300word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt2/350word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt2/400word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt2/450word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt2/500word_list.txt'],\n",
    "                    'opt3':\n",
    "['/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt3/50word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt3/100word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt3/150word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt3/200word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt3/250word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt3/300word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt3/350word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt3/400word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt3/450word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt3/500word_list.txt'],\n",
    "                    'opt4':\n",
    "['/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt4/50word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt4/100word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt4/150word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt4/200word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt4/250word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt4/300word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt4/350word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt4/400word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt4/450word_list.txt',\n",
    "'/home/sukanya/PycharmProjects/SiameseNNPAN/src/word_index/opt4/500word_list.txt']}'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''nlp.pipeline'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'very', 'large', 'line', 'this', 'is', 'i', 'do', \"n't\", 'know', 'how', 'this', 'works', 'yet', '!', 'god', 'save', 'me', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "#nlp = English()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# takes micro seconds 90.7 s  962 ns per loop (mean  std. dev. of 7 runs, 10000 loops each), 100 times faster than below loop\n",
    "def custom_analyzer(text):\n",
    "    #doc = nlp(text.lower())\n",
    "    doc = list(nlp.pipe([text.lower()], disable=[\"tagger\", \"parser\",\"ner\"]))[0]\n",
    "    return [t.text for t in doc]\n",
    "\n",
    "print(custom_analyzer(\"A very large line this is I don't know how this works yet! God Save me!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes mili seconds 4.23 ms  124 s per loop (mean  std. dev. of 7 runs, 100 loops each)\n",
    "'''def custom_analyzer(text):\n",
    "    doc = nlp(text.lower())\n",
    "    #doc = list(nlp.pipe([text.lower()], disable=[\"tagger\", \"parser\",\"ner\"]))[0]\n",
    "    return [t.text for t in doc]\n",
    "\n",
    "print(custom_analyzer(\"A very large line this is I don't know how this works yet! God Save me!!\"))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''feature_set_files'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''metrics = {}\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "for key in list(feature_set_files.keys()):\n",
    "    print(key)\n",
    "    temp_metrics_dict={}\n",
    "    vocab_files = []\n",
    "    training_acc = []\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    validation_acc = []\n",
    "    testing_acc= []\n",
    "    testing_loss = []\n",
    "    vocab_size_list = []\n",
    "    print(key, feature_set_files.get(key))\n",
    "    for feature_file in feature_set_files.get(key):\n",
    "        word_tokenizer = Tokenizer(analyzer=custom_analyzer)\n",
    "        with open(feature_file, \"r\") as f:\n",
    "            word_index_str = f.read().replace('\\n', '')\n",
    "        print(word_index_str)\n",
    "        word_tokenizer.fit_on_texts([word_index_str])\n",
    "        len_train = len(training_data)\n",
    "        # define the generators\n",
    "        from src.DataGenerator import DataGenerator\n",
    "        training_generator = DataGenerator(training_data.iloc[0:int(0.8*len_train)], tokenizer=word_tokenizer, batch_size=16)\n",
    "        validation_generator = DataGenerator(training_data.iloc[int(0.8*len_train):], tokenizer=word_tokenizer, batch_size=16)\n",
    "        testing_generator = DataGenerator(testing_data, tokenizer=word_tokenizer, batch_size=16)\n",
    "\n",
    "        # this is a hack for \"'DataGenerator' object has no attribute 'index'\". It turns out that on_epoch_end creates the index that is used\n",
    "        training_generator.on_epoch_end()\n",
    "        validation_generator.on_epoch_end()\n",
    "        testing_generator.on_epoch_end()\n",
    "        # parameters\n",
    "        num_classes =2\n",
    "        num_features = 1\n",
    "\n",
    "        # define model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(50, input_dim=num_features, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        # compile model\n",
    "        opt = SGD(lr=0.01, momentum=0.9)\n",
    "        #model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        # fit model\n",
    "        history = model.fit(training_generator, validation_data=validation_generator, verbose=1, batch_size=16, \n",
    "                             epochs=30, steps_per_epoch=100, callbacks=[callback]) #validation_steps=100,\n",
    "\n",
    "        history_dict = history.history\n",
    "        vocab_files.append(feature_file.split('/')[-1])\n",
    "        json.dump(history_dict, open(\"history_\" + key + \"_\" +feature_file.split('/')[-1], 'w'))\n",
    "        loss = history_dict['loss'][-1]\n",
    "        training_loss.append(loss)\n",
    "        acc = history_dict['accuracy'][-1]\n",
    "        training_acc.append(acc)\n",
    "        val_loss = history_dict['val_loss'][-1]\n",
    "        validation_loss.append(val_loss)\n",
    "        val_acc = history_dict['val_accuracy'][-1]\n",
    "        validation_acc.append(val_acc)\n",
    "        test_loss, test_acc = model.evaluate(testing_generator)\n",
    "        testing_acc.append(test_acc)\n",
    "        testing_loss.append(test_loss)\n",
    "        vocab_size_list.append(feature_file.split('/')[-1])\n",
    "        print(\"training acc: \", acc, \", training loss: \", loss, \", val acc: \", val_acc, \", val loss: \", val_loss,\", test acc: \", test_acc, \", test loss: \", test_loss)\n",
    "        print()\n",
    "    temp_metrics_dict={'vocab_size':vocab_size_list,\n",
    "        'training_loss' : training_loss,\n",
    "         'training_acc': training_acc,\n",
    "         'validation_loss': validation_loss,\n",
    "         'validation_acc': validation_acc,\n",
    "         'testing_loss':testing_loss,\n",
    "         'testing_acc':testing_acc}\n",
    "    json.dump(temp_metrics_dict, open(\"total_history_\" + key, 'w'))\n",
    "    metrics[key] = temp_metrics_dict\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''json.dump(metrics, open(\"history/total_history_combined\", 'w'))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df_opt1 = pd.DataFrame(metrics.get('opt1'))\n",
    "df_opt2 = pd.DataFrame(metrics.get('opt2'))\n",
    "df_opt3 = pd.DataFrame(metrics.get('opt3'))\n",
    "df_opt4 = pd.DataFrame(metrics.get('opt4'))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df_opt1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''temp_df_1 = df_opt1.loc[df_opt1['vocab_size'] == '100word_list.txt'].copy()\n",
    "temp_df_1.index= [\"opt1\"]\n",
    "temp_df_1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''temp_df_2 = df_opt2.loc[df_opt2['vocab_size'] == '100word_list.txt'].copy()\n",
    "temp_df_2.index= [\"opt2\"]\n",
    "temp_df_2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''temp_df_3 = df_opt3.loc[df_opt3['vocab_size'] == '100word_list.txt'].copy()\n",
    "temp_df_3.index= [\"opt3\"]\n",
    "temp_df_3'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''temp_df_4 = df_opt4.loc[df_opt4['vocab_size'] == '100word_list.txt'].copy()\n",
    "temp_df_4.index= [\"opt4\"]\n",
    "temp_df_4'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''result = pd.concat([temp_df_1,temp_df_2,temp_df_3, temp_df_4])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''result'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''result[['training_acc', 'validation_acc', 'testing_acc']].plot.bar(rot=0).legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''result[['training_acc', 'validation_acc', 'testing_acc']].plot.bar(rot=0).legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-eec63030ae03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/result.xlsx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'opt1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'opt2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'opt3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'opt4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0moptdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.8/site-packages/pandas/io/excel/_openpyxl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, engine, mode, **engine_kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mengine_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Use the openpyxl module as the Excel writer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mopenpyxl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkbook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWorkbook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mengine_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "for folder in results_folder_list[0:1]:\n",
    "    file_path = folder + \"/total_history_combined\"\n",
    "    with open(file_path) as json_file: \n",
    "        data = json.load(json_file) \n",
    "    with pd.ExcelWriter(folder+\"/result.xlsx\", mode='a') as writer:  \n",
    "        for opt in ['opt1', 'opt2', 'opt3', 'opt4']:\n",
    "            optdf = pd.DataFrame(data[opt])\n",
    "            optdf.columns = ['Vocabulary Size', 'Training Accuracy', 'Validation Accuracy', \n",
    "                 'Testing Accuracy', 'Training Loss','Validation Loss' ,'Testing Loss']     \n",
    "            print(optdf)#optdf.to_csv(folder+opt+'.csv')\n",
    "            optdf.to_excel(writer, sheet_name=opt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sukanya/PhD/Results/010_22_Feb_PAN_dataset_MPL_based/results/MPL_0/total_history_combined\n",
      "{'vocab_size': ['50word_list.txt', '100word_list.txt', '150word_list.txt', '200word_list.txt', '250word_list.txt', '300word_list.txt', '350word_list.txt', '400word_list.txt', '450word_list.txt', '500word_list.txt'], 'training_acc': [0.5687500238418579, 0.59375, 0.5787500143051147, 0.5693749785423279, 0.5849999785423279, 0.5543749928474426, 0.5893750190734863, 0.5706250071525574, 0.5893750190734863, 0.5699999928474426], 'validation_acc': [0.5783227682113647, 0.5667194128036499, 0.5791139006614685, 0.5838607549667358, 0.5719936490058899, 0.5638185739517212, 0.565664529800415, 0.5896624326705933, 0.5741033554077148, 0.5825421810150146], 'testing_acc': [0.5863045454025269, 0.5766568779945374, 0.5923867225646973, 0.588716447353363, 0.5785444378852844, 0.5817952752113342, 0.5762374401092529, 0.6033976674079895, 0.5822147727012634, 0.5833683013916016], 'training_loss': [0.6754927635192871, 0.6661174297332764, 0.6685338020324707, 0.6791483163833618, 0.6670557260513306, 0.67670077085495, 0.6721302270889282, 0.6743893623352051, 0.6642787456512451, 0.6801482439041138], 'validation_loss': [0.6705930829048157, 0.6684046983718872, 0.6713536977767944, 0.6744591593742371, 0.6705494523048401, 0.6674444675445557, 0.6699225306510925, 0.6722056269645691, 0.6713194251060486, 0.6773322224617004], 'testing_loss': [0.6672631502151489, 0.6646770238876343, 0.669553816318512, 0.6732144951820374, 0.6654130816459656, 0.6624096632003784, 0.6644439697265625, 0.670284628868103, 0.6659209728240967, 0.6770035624504089]}\n",
      "    Vocabulary Size  Training Accuracy  Validation Accuracy  Testing Accuracy  \\\n",
      "0   50word_list.txt           0.568750             0.578323          0.586305   \n",
      "1  100word_list.txt           0.593750             0.566719          0.576657   \n",
      "2  150word_list.txt           0.578750             0.579114          0.592387   \n",
      "3  200word_list.txt           0.569375             0.583861          0.588716   \n",
      "4  250word_list.txt           0.585000             0.571994          0.578544   \n",
      "5  300word_list.txt           0.554375             0.563819          0.581795   \n",
      "6  350word_list.txt           0.589375             0.565665          0.576237   \n",
      "7  400word_list.txt           0.570625             0.589662          0.603398   \n",
      "8  450word_list.txt           0.589375             0.574103          0.582215   \n",
      "9  500word_list.txt           0.570000             0.582542          0.583368   \n",
      "\n",
      "   Training Loss  Validation Loss  Testing Loss  \n",
      "0       0.675493         0.670593      0.667263  \n",
      "1       0.666117         0.668405      0.664677  \n",
      "2       0.668534         0.671354      0.669554  \n",
      "3       0.679148         0.674459      0.673214  \n",
      "4       0.667056         0.670549      0.665413  \n",
      "5       0.676701         0.667444      0.662410  \n",
      "6       0.672130         0.669923      0.664444  \n",
      "7       0.674389         0.672206      0.670285  \n",
      "8       0.664279         0.671319      0.665921  \n",
      "9       0.680148         0.677332      0.677004  \n",
      "['Vocabulary Size', 'Training Accuracy', 'Validation Accuracy', 'Testing Accuracy', 'Training Loss', 'Validation Loss', 'Testing Loss']\n"
     ]
    }
   ],
   "source": [
    "with open(file_path) as json_file: \n",
    "    data = json.load(json_file) \n",
    "print(file_path)\n",
    "print(data['opt1'])\n",
    "optdf = pd.DataFrame(data['opt1'])\n",
    "optdf.columns = ['Vocabulary Size', 'Training Accuracy', 'Validation Accuracy', \n",
    "                 'Testing Accuracy', 'Training Loss','Validation Loss' ,'Testing Loss']            \n",
    "print(optdf)\n",
    "print(list(optdf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (SiameseNetworkTensorflow)",
   "language": "python",
   "name": "pycharm-edf9727a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
